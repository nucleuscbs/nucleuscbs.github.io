<!DOCTYPE html> <html lang="en"> <style> :root { --primary-color: rgba(10, 10, 72, 0.55);; --overlay-color: rgba(24, 39, 51 , 0.85); --menu-speed: 0.75s; } .container-show { max-width: 105rem; margin: auto; overflow: hidden; padding: 0 3rem; } .showcase { background: #fff; height: 90vh; position: relative; } .showcase:before { content: ''; background: #fff; position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: -1; } .showcase .showcase-inner { display: flex; flex-direction: row; align-items: center; justify-content: center; height: 100%; } .showcase h1 { font-size: 8rem; line-height: 1; } .showcase p { font-size: 2rem; } .logo{ font-family: Gotham XNarrow; text-transform: uppercase; } </style> <head> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <meta name="p:domain_verify" content="24f4001fe32c84cdbe8a07fc24fdeecd"/> <title>Logistic Regression And Surviving The Titanic | The Analytics Bay</title> <!-- Begin Jekyll SEO tag v2.6.1 --> <title>Logistic Regression And Surviving The Titanic | The Analytics Bay</title> <meta name="generator" content="Jekyll v4.1.1" /> <meta property="og:title" content="Logistic Regression And Surviving The Titanic" /> <meta name="author" content="Shivansh Mehendiratta" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Logistic Regression is the go to method for binary classification in machine learning. In the article, we’ll be learning about the logit function, and using it to solve the one of the most popular problems on Kaggle, the Titanic Dataset." /> <meta property="og:description" content="Logistic Regression is the go to method for binary classification in machine learning. In the article, we’ll be learning about the logit function, and using it to solve the one of the most popular problems on Kaggle, the Titanic Dataset." /> <link rel="canonical" href="http://localhost:4000/blog/Logistic-Regression/" /> <meta property="og:url" content="http://localhost:4000/blog/Logistic-Regression/" /> <meta property="og:site_name" content="The Analytics Bay" /> <meta property="og:image" content="http://localhost:4000/bg.jpeg" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2021-07-22T00:00:00+05:30" /> <meta name="twitter:card" content="summary_large_image" /> <meta property="twitter:image" content="http://localhost:4000/bg.jpeg" /> <meta property="twitter:title" content="Logistic Regression And Surviving The Titanic" /> <meta name="twitter:site" content="@shivansh3121" /> <meta name="twitter:creator" content="@shivansh3121" /> <script type="application/ld+json"> {"@type":"BlogPosting","headline":"Logistic Regression And Surviving The Titanic","dateModified":"2021-07-22T00:00:00+05:30","datePublished":"2021-07-22T00:00:00+05:30","url":"http://localhost:4000/blog/Logistic-Regression/","image":"http://localhost:4000/bg.jpeg","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/Logistic-Regression/"},"author":{"@type":"Person","name":"Shivansh Mehendiratta"},"description":"Logistic Regression is the go to method for binary classification in machine learning. In the article, we’ll be learning about the logit function, and using it to solve the one of the most popular problems on Kaggle, the Titanic Dataset.","@context":"https://schema.org"}</script> <!-- End Jekyll SEO tag --> <link rel="shortcut icon" href="/img/favicon.png"> <link rel="stylesheet" href="/main.css"> <link href='https://olifro.st/feed.xml' rel='alternate' type='application/atom+xml'> <meta name="theme-color" content="#020cfa"> </head> <body> <div class="menu-wrap"> <input type="checkbox" class="toggler"> <div class="hamburger"><div></div></div> <div class="menu"> <div> <div> <ul> <li><a href="/blog">Blog</a></li> <li><a href="/about">About</a></li> <li><a href="/contact">Contact Us</a></li> </ul> </div> </div> </div> </div> <div class="bar container" style="top:0; border-bottom: 5px solid #bb71f8;"><a href="/"><div class="logo" style=" padding: 30px 0;">The Analytics Bay</div></a><ul class="specialul"> <li><a href="/contact">Contact Us</a></li> <li><a href="/about">About</a></li> <li><a href="/blog">Blog</a></li> </ul></div> <main class="content"> <div class="container" style="margin-top: 80px"> <a href="/blog/">← All Posts</a> / <time datetime="2021-07-22T00:00:00+05:30" class="post-date">22 Jul 2021</time> <article class="post"> <h1 class="post-title">Logistic Regression And Surviving The Titanic</h1> <p></p> <p class="post-description"><p>Logistic Regression is the go to method for binary classification in machine learning. In the article, we’ll be learning about the logit function, and using it to solve the one of the most popular problems on Kaggle, the Titanic Dataset.</p> </p> <p><p>Logistic Regression, or the Logit Model, is one of the fundamental blocks of statistics and machine learning. It is used when the dependent variable is categorical, and is the go to method for binary classification (two class values). For example, the probability of win/lose or pass/fail an exam. Most popular use of logistic regression today is in determining if an email is spam or not.</p> <p><img src="/blog/LogisticRegression/Logistic1.jpg" /></p> <h1>How does it work?</h1> <p>At the core of the logistic regression is the logit function, also called the sigmoid function and was developed by statisticians to describe properties of population growth in ecology, biology and environment sciences. It’s an S-shaped curve that maps any real-valued number into a value between 0 and 1, not necessary at only those limits. The equation for Logistic Regression is:</p> <p><img src="/blog/LogisticRegression/Logistic2.jpg" /></p> <p>Where y is the predicted output, B0 is the intercept and B1 is the coefficient for (x). It can be said that Logistic regression is a linear function. However, the predictions are morphed into classification using the logit function. Example of Logistic Regression</p> <p>We can use an example to learn Logistic Regression better. Let’s say we have data that can be used to predict a person’s gender based on their height. Given a height of 150cm is the person male or female.</p> <p>Let’s say that the coefficients are b0 = -100 and b1 = 0.6. The above equation can be utilized to predict if a person is male given a height of 150cm.</p> <p><b>y = e^(b0 + b1<em>x) / (1 + e^(b0 + b1</em>x))</b></p> <p><b>y = e^(-100 + 0.6<em>150) / (1 + e^(-100 + 0.6</em>x))</b></p> <p><b>y = 0.00004539</b></p> <p>The probability is so low that it can be used as 0, and certainly this person is not male. Since, this is classification and we want a crisp answer, we can create bins for a complete classification of the values, for example:</p> <p><b>0 if p(male) &lt; 0.5</b></p> <p><b>1 if p(male) &gt;= 0.5</b></p> <p>Logistic regression models are models that have a certain fixed number of parameters that depend on the number of input features, and they output categorical predictions, like for example if a cancer is malignant or not.</p> <h1>Types of Logistic Regression</h1> <p><b>Binary Logistic Regression:</b> The final response has only two possible outcomes. For example, either a student passes an exam or not.</p> <p><b>Multinomial Logistic Regression:</b> More than two possible outcomes, without any ordering. For example, predicting which of the election candidates wins among many.</p> <p><b>Ordinal Logistic Regression:</b> An ordered possibility of outcomes. For example, figuring out the movie rating from 1 to 5.</p> <p>However, in this article we’ll be focusing solely on the binary classification type as it is the most popular among the three. ‘</p> <h1>Surviving a Disaster and The Titanic Dataset</h1> <p>The most popular dataset on Kaggle, undoubtedly, is the Titanic Dataset. It can also be considered a rite of passage for aspiring data scientists learning classification models. And why not, the data is structured in a way that helps people learn the fundamentals of classification and logistic regression.</p> <p><img src="/blog/LogisticRegression/Logistic3.jpg" /></p> <p>The dataset has the following variables (attributes) which are explained very well on Kaggle.</p> <p><img src="/blog/LogisticRegression/Logistic4.jpg" /></p> <p>Here, we shall use the train.csv provided to train the model and predict the survival of a passenger based on the given variables.</p> <h1>Code </h1> <h3>Importing the dataset, and understanding the data: </h3> <p>import pandas as pd</p> <p>titanic = pd.read_csv(“train.csv”)</p> <p>titanic.shape</p> <p><img src="/blog/LogisticRegression/Logistic5.jpg" /> <img src="/blog/LogisticRegression/Logistic6.jpg" /> <img src="/blog/LogisticRegression/Logistic7.jpg" /></p> <h3>Data Preprocessing </h3> <p>It is also worth noting that ‘Embarked’ has 3 classes C, Q, S which have to be converted into individual attributes.</p> <p><img src="/blog/LogisticRegression/Logistic8.jpg" /></p> <p>Here, we’ve used the ‘get_dummies’ function to create separate variables for each Embarked class. And we’ve joined the new dataframe with the original dataframe.</p> <p>We’ve created two dataframes X and y, which will be used for Logistic Regression and learning. And dropped multiple non-numeric attributes which have no effect on the survival of a passenger.</p> <p><img src="/blog/LogisticRegression/Logistic9.jpg" /></p> <p>We see that ‘Age’ has many null values, so we use ‘mean’ to impute null values.</p> <p><img src="/blog/LogisticRegression/Logistic10.jpg" /></p> <h3>Creating the Model</h3> <p>We’ll use ScikitLearn to create the Logistic Regression model, and split the dataset into 80% (used for training the model) and 20% (for testing the model).</p> <p><img src="/blog/LogisticRegression/Logistic11.jpg" /></p> <p>After the model is created, it is necessary to check how well it has performed. The model score for testing is 0.754, which means that 75.4% of the time the model correctly predicts if a passenger has survived the disaster or not. We also figured out the intercept and coefficients (the array is made up of all the attributes used in the model).</p> <p><img src="/blog/LogisticRegression/Logistic12.jpg" /></p> <p>However, the correlation between the attributes and survival can be better understood with a visual.</p> <p><img src="/blog/LogisticRegression/Logistic13.jpg" /></p> <p>This cell’s output is a heatmap that shows the correlation between all the attributes. <img src="/blog/LogisticRegression/Logistic14.jpg" /></p> <h1>Conclusion</h1> <p>Logistic regression is one of the most exciting concepts in statistics and a powerful tool to classify data. However, one shortcoming of Logit functions is that they are not able to work well with outliers and leads to overfitting. Hence, we must try and remove outliers from the data provided to make the model more accurate.</p> <p>This was the most simple method that can be used to train a ML Logistic model. We can always use more sophisticated models for better prediction and classification using a more detailed analysis of the data and more complex feature engineering.</p> </p> <p><strong>Recommended</strong> <a href="/blog/Gradient-Descent/">» Gradient Descent </a><br> </p> </article> </div> </main> <!-- <footer id="footer"> <div class="row"> <div class="col-xs-12 col-sm-3 footer-social center-xs"> <small class="social"> <div> <span class="icons"> <a href="https://facebook.com/nucleus.cbs"> <span class="icon icon--instagram"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M22.675 0h-21.35c-.732 0-1.325.593-1.325 1.325v21.351c0 .731.593 1.324 1.325 1.324h11.495v-9.294h-3.128v-3.622h3.128v-2.671c0-3.1 1.893-4.788 4.659-4.788 1.325 0 2.463.099 2.795.143v3.24l-1.918.001c-1.504 0-1.795.715-1.795 1.763v2.313h3.587l-.467 3.622h-3.12v9.293h6.116c.73 0 1.323-.593 1.323-1.325v-21.35c0-.732-.593-1.325-1.325-1.325z"/></svg></span></a> <a href="https://www.linkedin.com/company/nucleus-cbs/"> <span class="icon icon--twitter"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg></span></a> <a href="https://instagram.com/nucleus.cbs"> <span class="icon icon--instagram"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M12 2.163c3.204 0 3.584.012 4.85.07 3.252.148 4.771 1.691 4.919 4.919.058 1.265.069 1.645.069 4.849 0 3.205-.012 3.584-.069 4.849-.149 3.225-1.664 4.771-4.919 4.919-1.266.058-1.644.07-4.85.07-3.204 0-3.584-.012-4.849-.07-3.26-.149-4.771-1.699-4.919-4.92-.058-1.265-.07-1.644-.07-4.849 0-3.204.013-3.583.07-4.849.149-3.227 1.664-4.771 4.919-4.919 1.266-.057 1.645-.069 4.849-.069zm0-2.163c-3.259 0-3.667.014-4.947.072-4.358.2-6.78 2.618-6.98 6.98-.059 1.281-.073 1.689-.073 4.948 0 3.259.014 3.668.072 4.948.2 4.358 2.618 6.78 6.98 6.98 1.281.058 1.689.072 4.948.072 3.259 0 3.668-.014 4.948-.072 4.354-.2 6.782-2.618 6.979-6.98.059-1.28.073-1.689.073-4.948 0-3.259-.014-3.667-.072-4.947-.196-4.354-2.617-6.78-6.979-6.98-1.281-.059-1.69-.073-4.949-.073zm0 5.838c-3.403 0-6.162 2.759-6.162 6.162s2.759 6.163 6.162 6.163 6.162-2.759 6.162-6.163c0-3.403-2.759-6.162-6.162-6.162zm0 10.162c-2.209 0-4-1.79-4-4 0-2.209 1.791-4 4-4s4 1.791 4 4c0 2.21-1.791 4-4 4zm6.406-11.845c-.796 0-1.441.645-1.441 1.44s.645 1.44 1.441 1.44c.795 0 1.439-.645 1.439-1.44s-.644-1.44-1.439-1.44z"/></svg></span></a> </span> <form action="https://formspree.io/xeqrjbrn" method="POST"> <input type="email" name="replyto" id="Email" placeholder="Mailing List Email" class="email" required> <input type="hidden" name="_next" value="https://nucleuscbs.github.io/thanks/" /> <input type="submit" value="Join"> </form> </div> </small> </div> <div class="col-xs-12 col-sm-6 footer"> <h1 style="color: #fff">Collegiate Entrepreneurs Organisation</h1> <h1>Delhi University</h1> </div> <div class="col-xs-12 col-sm-3 footer"> <a href="/noprivacy/">Privacy Policy</a> | &copy; CEO, Delhi University </div> </div> </footer> --> <footer id="footer" style=" display:grid; grid-template-columns: repeat(20,5%); grid-template-rows: repeat(20,5%); height: 40vh; "> <div class="footer-social footer-layout-social"> <small class="social"> <div> <span class="icons"> <a href="https://facebook.com/nucleus.cbs"> <span class="icon icon--instagram"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M22.675 0h-21.35c-.732 0-1.325.593-1.325 1.325v21.351c0 .731.593 1.324 1.325 1.324h11.495v-9.294h-3.128v-3.622h3.128v-2.671c0-3.1 1.893-4.788 4.659-4.788 1.325 0 2.463.099 2.795.143v3.24l-1.918.001c-1.504 0-1.795.715-1.795 1.763v2.313h3.587l-.467 3.622h-3.12v9.293h6.116c.73 0 1.323-.593 1.323-1.325v-21.35c0-.732-.593-1.325-1.325-1.325z"/></svg></span></a> <a href="https://www.linkedin.com/company/nucleus-cbs/"> <span class="icon icon--twitter"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg></span></a> <a href="https://instagram.com/nucleus.cbs"> <span class="icon icon--instagram"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M12 2.163c3.204 0 3.584.012 4.85.07 3.252.148 4.771 1.691 4.919 4.919.058 1.265.069 1.645.069 4.849 0 3.205-.012 3.584-.069 4.849-.149 3.225-1.664 4.771-4.919 4.919-1.266.058-1.644.07-4.85.07-3.204 0-3.584-.012-4.849-.07-3.26-.149-4.771-1.699-4.919-4.92-.058-1.265-.07-1.644-.07-4.849 0-3.204.013-3.583.07-4.849.149-3.227 1.664-4.771 4.919-4.919 1.266-.057 1.645-.069 4.849-.069zm0-2.163c-3.259 0-3.667.014-4.947.072-4.358.2-6.78 2.618-6.98 6.98-.059 1.281-.073 1.689-.073 4.948 0 3.259.014 3.668.072 4.948.2 4.358 2.618 6.78 6.98 6.98 1.281.058 1.689.072 4.948.072 3.259 0 3.668-.014 4.948-.072 4.354-.2 6.782-2.618 6.979-6.98.059-1.28.073-1.689.073-4.948 0-3.259-.014-3.667-.072-4.947-.196-4.354-2.617-6.78-6.979-6.98-1.281-.059-1.69-.073-4.949-.073zm0 5.838c-3.403 0-6.162 2.759-6.162 6.162s2.759 6.163 6.162 6.163 6.162-2.759 6.162-6.163c0-3.403-2.759-6.162-6.162-6.162zm0 10.162c-2.209 0-4-1.79-4-4 0-2.209 1.791-4 4-4s4 1.791 4 4c0 2.21-1.791 4-4 4zm6.406-11.845c-.796 0-1.441.645-1.441 1.44s.645 1.44 1.441 1.44c.795 0 1.439-.645 1.439-1.44s-.644-1.44-1.439-1.44z"/></svg></span></a> </span> <form action="https://formspree.io/xeqrjbrn" method="POST"> <input type="email" name="replyto" id="Email" placeholder="Mailing List Email" class="email" required> <input type="hidden" name="_next" value="https://nucleuscbs.github.io/thanks/" /> <input type="submit" value="Join"> </form> </div> </small> </div> <div class="footer-layout-text"> <h1 style="color: #000; margin:0; line-height: 1.2; font-family: Gotham XNarrow;letter-spacing: 0.05rem;">THE ANALYTICS BAY</h1> <img src="/nucleus.png" style="height:50px;width:auto;margin-left:0;"> <!-- <h1 style="line-height: 1.2;font-family: Gotham XNarrow;letter-spacing: 0.05rem; font-size: 3rem;color:#000">Nucleus, the analytics society of SSCBS</h1> --> </div> <div class="footer-layout-copy"> &copy; The Analytics Bay </div> </footer> <script async src=""></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); </script> <!-- Rotator --> </body>
