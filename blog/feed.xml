<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-12-02T18:51:35+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">The Analytics Bay</title><subtitle>Always Analyzing</subtitle><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><entry><title type="html">Autocorrelation</title><link href="http://localhost:4000/blog/Autocorrelation/" rel="alternate" type="text/html" title="Autocorrelation" /><published>2020-12-02T00:00:00+05:30</published><updated>2020-12-02T00:00:00+05:30</updated><id>http://localhost:4000/blog/Autocorrelation</id><content type="html" xml:base="http://localhost:4000/blog/Autocorrelation/">&lt;p&gt;Autocorrelation shows the degree of similarity between a given time series and a lagged version of itself over successive time intervals. Lagged version of time series is simply the past period values of the concerned variable. This seemingly simple concept has wide application in fields of mathematics, statistics, and sciences. It is used in measuring optical spectra, short duration light pulses, in GPS system, portfolio and return optimization and much more. Unit root processes, trend-stationary processes, autoregressive processes, and moving average processes are specific forms of processes with autocorrelation.&lt;br /&gt;
The resulting values ranges between -1 to 1, like the traditional correlation statistic. An autocorrelation of +1 represents a perfect positive correlation (an increase seen in one time series leads to a proportionate increase in the other time series). An autocorrelation of negative 1, on the other hand, represents perfect negative correlation (an increase seen in one time series results in a proportionate decrease in the other time series). But it must be remembered as words of caution that autocorrelation measures linear relationships. So even if the autocorrelation is minuscule, there may still be a nonlinear relationship between a time series and a lagged version of itself.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Reasons of autocorrelation &lt;/b&gt;
Some major reasons of autocorrelation include-&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Carryover of effect- Carry over effect of variables in data causes autocorrelation. For ex- Data on monthly expenditure of a family in a particular city, preceding month expenditure will influence the expenditure of next month. This is called carry over effect.&lt;/li&gt;
  &lt;li&gt;Mis-specification of form of relationship in a model also causes autocorrelation. For ex- a study assumes that explanatory and study variables have linear relationship but they have logarithmic or exponential relation.&lt;/li&gt;
  &lt;li&gt;Measurement error- Errors may creep in due to difference between observed and true value of variable, data collection issues etc.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt; Problems caused by autocorrelation &lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Inefficient (but unbiased) ordinary least square estimate which fails to achieve smallest variance and any forecast based on them is hence inefficient. Efficient estimator gives most information about a sample and inefficient estimator may perform well but requires larger sample sizes to do so.&lt;/li&gt;
  &lt;li&gt;Exaggerated goodness of fit for a time series with positive serial correlation and an independent variable that grows over time.&lt;/li&gt;
  &lt;li&gt;Leads to overly optimistic view of R2.&lt;/li&gt;
  &lt;li&gt;Large variance in predictions based on the model and narrow confidence interval. The T statistics and F value are often not reliable.&lt;/li&gt;
  &lt;li&gt;It increases the occurrence of false negative for significant regression coefficient. Regression coefficients appear significant when they are not.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt; Graphical approach through MS-Excel &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Autocorrelation/1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Plotting the data with its lag clearly shows us there exist a relationship between the two, because the plot denotes a nearly definite pattern. This high relation is also denoted with correlation coefficient of 0.83.&lt;/p&gt;

&lt;p&gt;But things are not always this simple. Such high value of correlation of a data with its past values would have made everything so predictable and easy eliminating the need of complex statistical analysis and modeling. Let’s take another example-&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Autocorrelation/2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The plot shows data points paired with their lagged values are scattered without any pattern or direction. This is again confirmed with a low correlation coefficient.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Example 3 &lt;/b&gt;- For finding autocorrelation of a series with different lags on excel we simply create a table with suitable lags and find the correlation using CORREL() function. We can plot the various correlations so obtain to have a better picture of the case.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Autocorrelation/3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Autocorrelation in R &lt;/b&gt;
Finding autocorrelation is not always that simple because we test it with many lags. Repeating the same process with huge data sets becomes a tedious task in MS-Excel. Not to worry, we are again there for your rescue. Let’s try it out on R.
Documentation-
acf(x, lag.max = NULL,
type = c(“correlation”, “covariance”, “partial”),
plot = TRUE, na.action = na.fail, demean = TRUE, …)&lt;/p&gt;

&lt;p&gt;x&amp;lt;- Time series data
lag.max&amp;lt;- Number of lags upta which correlation needs to be calculated
type&amp;lt;- Character string giving the type of acf to be computed. Allowed values are”correlation” (the default), “covariance” or “partial”
plot&amp;lt;- True or False based on whether you want plot or not&lt;/p&gt;

&lt;p&gt;&lt;b&gt; What can you do? &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Removing autocorrelation involves making changing to the data, improvising the model and other tests and modifications. In layman language, for positive serial correlation one can add lags of dependent or independent variable in the model. For negative serial correlation one must check that none of the variables in over-differenced and for seasonal correlation one can add seasonal dummy variable to the model. It itself is a wide concept and we will surely cover it in detail in one of our subsequent blogs.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Example &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Mr. X is a trader. While monitoring stock of ABD ltd he realized that usually whenever stock price rallies on Monday it is followed by decrease in price by the next day and a increase in price on Wednesday. Amused to know this, he tries to prove his observations quantitatively. He takes the daily returns of the share price and run autocorrelation on it with lag 1 to lag 10. He finds that returns one day prior have a negative autocorrelation of -0.74, while the returns two days prior have a positive autocorrelation of +0.83. Past returns seem to influence future returns. Therefore, Mr.X can adjust his position to take advantage of the autocorrelation and resulting momentum by selling the share the next day after a rally and buy them on the following day again. While doing this he never forgets that the chances of error remains high because what was right yesterday may prove wrong tomorrow and he continuously monitors other factors that govern the stock price while repeatedly checking the relevance of auto-correlation at appropriate intervals.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Conclusion &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Autocorrelation is a wide concept. If discovered it can help us to detect patterns in our data, find seasonality and other interesting insights. If ignored it may distort our results. Just a basic awareness about its presence will help us improve our models and analysis. And I hope that we were successful in introducing and explaining the concept to you. Practice the methods we shared, explore new ones, and don’t forget to share your views and findings with us.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Autocorrelation shows the degree of similarity between a given time series and a lagged version of itself over successive time intervals. Lagged version of time series is simply the past period values of the concerned variable. This seemingly simple concept has wide application in fields of mathematics, statistics, and sciences. It is used in measuring optical spectra, short duration light pulses, in GPS system, portfolio and return optimization and much more. Unit root processes, trend-stationary processes, autoregressive processes, and moving average processes are specific forms of processes with autocorrelation. The resulting values ranges between -1 to 1, like the traditional correlation statistic. An autocorrelation of +1 represents a perfect positive correlation (an increase seen in one time series leads to a proportionate increase in the other time series). An autocorrelation of negative 1, on the other hand, represents perfect negative correlation (an increase seen in one time series results in a proportionate decrease in the other time series). But it must be remembered as words of caution that autocorrelation measures linear relationships. So even if the autocorrelation is minuscule, there may still be a nonlinear relationship between a time series and a lagged version of itself.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Random Forest</title><link href="http://localhost:4000/blog/Random-Forest/" rel="alternate" type="text/html" title="Random Forest" /><published>2020-10-27T00:00:00+05:30</published><updated>2020-10-27T00:00:00+05:30</updated><id>http://localhost:4000/blog/Random-Forest</id><content type="html" xml:base="http://localhost:4000/blog/Random-Forest/">&lt;p&gt;&lt;img src=&quot;/blog/RF/rf.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Random Forest is a versatile machine learning method capable of performing both the regression and classification tasks. It is a type of the ensemble learning method, in which a group of weak models combine to form a powerful model.&lt;/p&gt;

&lt;p&gt;It can be properly defined as &lt;b&gt; a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset. &lt;/b&gt;
Instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; The Algorithm of Random Forest &lt;/b&gt;
Random forest is like the bootstrapping algorithm with Decision tree (CART) model. Let’s say, we have 1000 observations in the complete population with 10 variables. Random forest tries to build multiple CART (decision tree) models with different samples and different initial variables. For instance, it would take a random sample of 100 observations and 5 randomly chosen initial variables to build a CART model. It will repeat the process, say 10 times, and then make a final prediction on each observation. The final prediction is the function of each prediction. This final prediction can simply be the mean of each of the predictions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/RF/1_each of the predictions.png&quot; /&gt;
&lt;b&gt; The Working Process of Random Forest can be explained in the below steps- &lt;/b&gt;
Step 1 - Firstly, select random K data points from the training set.
Step 2 - Now, build the decision trees associated with the selected data points (Subsets).
Step 3 - Choose the number N for the decision trees that you want to build.
Step 4 - Repeat Steps 1 &amp;amp; 2.
Step 5 - For new data points, find out the predictions of each decision tree, and assign the new data points to the category that wins the majority votes.
Random forest prediction pseudo-code-
To perform predictions; the trained random forest algorithm uses the below pseudo-code.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Take the test features and use the rules of each randomly created decision tree to predict the outcome and stores the predicted outcome (target)&lt;/li&gt;
  &lt;li&gt;Calculate number of votes for each predicted target.&lt;/li&gt;
  &lt;li&gt;Always consider the high voted predicted target as the final prediction from the random forest algorithm.
Now, we do understand that, some of the words or terms used above must have gone above your head; that’s no problem at all; we will understand the concept with the help of a real life example as well as a case study.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Random Forest Algorithm; A Basic Real Life Example.
Suppose, Saksham wants to go to different places that he may like for his vacation, and he asks his friend for advice. His friend will ask which places he has been to already, and whether he likes the places that he’s visited. Based on Saksham’s answers, his friend starts to give the recommendation. Here, his friend forms the decision tree.
Saksham wants to ask more friends for advice because he thinks only one friend cannot help him make an accurate decision. So, his other friends also ask him some random questions, and finally, provide an answer. He will consider the place with the most votes as his vacation decision.
His friends asked him some questions and gave the recommendation of the best place based on the answers. The friends created the rules based on the answers and used the rules to find the answer that matched the rules. Saksham’s friends also randomly asked him different questions and gave answers, which for they are the votes for the place. At the end, the place with the highest votes is the one he will select to go. This is the typical Random Forest algorithm approach.
Now, we move on to a serious and intuitive case study to actually understand why the concept of Random Forest is so useful, and how it is used practically.&lt;/p&gt;

&lt;p&gt;Case Study on Usage of Random Forest in ML
Following is the distribution of Annual income GINI Coefficients across different countries-&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/RF/2_Gini.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Mexico has a population of 118 Million. Let’s assume that, the algorithm Random forest picks up 10k observations with only one variable (for simplicity) to build each Decision tree (CART model). In total, we are looking at 5 CART models being built with different variables. In a real life problem, you will have more number of population samples and different combinations of input variables.
Salary bands-
Band 1 - Below $40,000
Band 2 - $40,000 – 150,000
Band 3 - More than $150,000
Following are the outputs of the 5 different CART models.
CART (Decision Tree) 1- Variable Age
&lt;img src=&quot;/blog/RF/3_Cart1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CART 2 - Variable Gender
&lt;img src=&quot;/blog/RF/4_Cart2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CART 3 - Variable Education
&lt;img src=&quot;/blog/RF/5_Cart3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CART 4 - Variable Residence
&lt;img src=&quot;/blog/RF/6_Cart4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CART 5 - Variable Industry
&lt;img src=&quot;/blog/RF/7_Cart_5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using these 5 Decision Tree models (CART models), we need to come up with single set of probability to belong to each of the salary classes. For simplicity, we will just take the mean of probabilities in this case study. Other than simple mean, we will also consider the vote method to come up with the final prediction. To come up with the final prediction let’s locate the following profile in each of the CART models-&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Age- 35 years,&lt;/li&gt;
  &lt;li&gt;Gender- Male,&lt;/li&gt;
  &lt;li&gt;Highest Educational Qualification- Diploma holder,&lt;/li&gt;
  &lt;li&gt;Industry- Manufacturing,&lt;/li&gt;
  &lt;li&gt;Residence- Metro
For each of these Decision tree (CART models), following is the distribution across salary bands-&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/blog/RF/8_finalcart.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The final probability is simply the average of the probability in the same salary bands in different CART models. As we can see from this analysis, that there is 70% chance of the individual falling in class 1 (less than $40,000) and around 24% chance of the individual falling in class 2.
Final Inference
Random forest gives much more accurate predictions when compared to simple Decision Tree (CART) or regression models in many scenarios. These cases generally have high number of predictive variables and huge sample size. This is because it captures the variance of several input variables at the same time and enables high number of observations to participate in the prediction.&lt;/p&gt;

&lt;p&gt;So, we hope that you must have understood the concept of ‘Random Forest’ in Machine Learning through the blog. Do write your precious feedback, and feel free to ask any doubt related.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Business Intelligence Project</title><link href="http://localhost:4000/blog/The-Business-Intelligence-Project/" rel="alternate" type="text/html" title="The Business Intelligence Project" /><published>2020-09-08T00:00:00+05:30</published><updated>2020-09-08T00:00:00+05:30</updated><id>http://localhost:4000/blog/The-Business-Intelligence-Project</id><content type="html" xml:base="http://localhost:4000/blog/The-Business-Intelligence-Project/">&lt;p&gt;Initially, BI used to be the work of just IT professionals, who would supply the whole organisation with the reports for decision-making, however, now it is used at every stage of management by the managers (primarily due to the development in data science sphere and the rise of self- service BI tools).&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Ways in which BI can help organisations make smart, data- driven decisions-&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Tracking performance&lt;/li&gt;
  &lt;li&gt;Analysing Key Performance Indicators (KPI)&lt;/li&gt;
  &lt;li&gt;Analysing the market share and consumer behaviour&lt;/li&gt;
  &lt;li&gt;Optimizing business operations using historical data&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt; Business Intelligence Tools &lt;/b&gt;
&lt;br /&gt;
Business intelligence tools are nothing but software that are used to analyse trends and extract insights out of the data in order to make tactical and strategic business decisions.
There are many Business Intelligence tools like SAS BI, MicroStrategy, Datapine, Domo, etc that help in carrying out the necessary tasks but two of the most powerful and widely used tools in Business Intelligence on which this blog is also dedicated are Power BI and Tableau.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Power BI&lt;/b&gt;
&lt;br /&gt;
Power BI provides a summary of the data in the form of reports and dashboards while connecting with every data source across different. It makes data assessment, sharing scalable dashboards, embedded visualizations, interactive reports and various another feature which we will see further in the blog. It is amazing at importing visualizations with easy-to-use and user-friendly interfaces like Excel, etc. Power BI is simple for using that provides a full overview of your business performance.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Tableau&lt;/b&gt;
&lt;br /&gt;
While there are numerous intuitive business intelligence tools, Tableau uses data discovery and its interpretation for all types of the business user. It is sometimes quoted as the most user-friendly data science tool available. Being simple to handle, any user without much of a stretch can perform reading or writing data related tasks along with analysing key data insights, while creating visualizations and innovative reports, and finally sharing insights throughout the enterprise with help of dashboards and stories. Tableau is good at connecting to any data source with a drag-and-drop interface that is easy-to-use and makes transferring data simple. However, unlike Power BI it is weak at combining with different data sources for analysis.&lt;/p&gt;

&lt;p&gt;INSIGHTS&lt;/p&gt;

&lt;p&gt;As we have always said, hands-on real-time experience on analytics software are any day better than the theoretical concepts and is also our core strength, so to compare how the visualisations would look in Power BI and Tableau we took a same dataset and created a dashboard. We undertook a project under the supervision of our esteemed alumnus, &lt;u&gt;Mr. Vikrant Sharma&lt;/u&gt; (Analyst, InMobi) and &lt;u&gt;Mr. Shoury Anand&lt;/u&gt;(IIM Lucknow, PGP’22). This included a study on COVID-19 scenario where we took 5 datasets which contained data for number of COVID cases worldwide and in India. Also, to study what impact the current pandemic has on the indices we took time series data for S&amp;amp;P Global, FTSE 100 and Dow Jones along with Gold and Crude Oil prices starting from 1st January 2020 to 31st July 2020. To study how GDP has fared in the past years, we took Real GDP for 180 countries for past 13 years. Not surprisingly, the visualizations said for itself all the data which we collected and wanted to concluded our results. Finally, both the dashboards looked pretty amazing and gave us a hard time to choose one over another. So the question in on our readers which visualisation did they find better!&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Tableau Dashboard&lt;/b&gt;
&lt;img src=&quot;/blog/tableau/Tableau Dashboard.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Power BI Dashboard&lt;/b&gt; 
&lt;img src=&quot;/blog/tableau/PowerBI Dashboard.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;
&lt;br /&gt;
We have conducted a survey of sample size of 100 to gather and access the opinion of college students on Business Intelligence tools namely Power BI and Tableau.
The respondents had to answer various questions; like on what parameters would they favour one software from the other, rating these softwares on User- friendliness and attractiveness, their personal opinions/ experiences among others.
Most of the students replied on the same lines and the results matched our findings/ expectations. Tableau was found to be the favourable of the two.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Some of the opinions-&lt;/b&gt;
&lt;i&gt;“The Power BI Student Version has a lot of limitations and thus though it is capable to a large extent, cannot be used to its full capacity. Tableau has some limited features as compared to Power BI w.r.t graph styles, colour available, etc. Overall, Both the tools are very powerful for Data Viz”&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;“I feel that use of visualizations created in Tableau should be restricted to Tableau itself. Any presentation that requires only and only visualizations with little to none supporting text can be presented through Tableau’s story mode. For PowerPoint presentations, I like to connect excel sheets with it for charts, as it makes it easy for me to change the formatting of the chart within PowerPoint and any change in figures can also be speedily done.
And about dashboards in presentations, I don’t think that is a good choice because the number of visualizations in a dashboard might distract an audience, and it is difficult to demonstrate its dynamic features on a presentation.”&lt;/p&gt;

&lt;p&gt;&lt;i&gt;I have had a better experience working with Power BI over tableau due to its user-friendly nature and a relatively easier visualisation capability.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Result of the survey were as follows-&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/tableau/1.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;
&lt;img src=&quot;/blog/tableau/2.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BUSINESS INTELLIGENCE TOOLS FROM A STUDENTS’ PERSPECTIVE
Microsoft’s Power BI is a suite of data visualization and business analytic tools. It offers tools to easily analyse, transform and visualize data pipelines, including the ability to build reusable models. The software enables users to integrate their apps, to deliver reports along with real-time dashboards.
Tableau is a visualization tool that helps businesses transform their data into insights that can lead to action. The tool makes it easy to connect data in almost any format from almost any source. Interactive dashboard with visual analytics can be created with simple dragging and dropping, and data transformed in graphs, maps, charts, and other visualizations.&lt;/p&gt;

&lt;p&gt;Data Visualizations tools are necessary when it comes to creating a visual representation of analytics and sharing insights with other.
And as Management students, we faced difficulty initially in choosing the best tool for making reports and dashboards, because both Power BI and Tableau were terrific and had a lot to offer. So, to make life simpler and easier, we’ll be comparing these two on a range of parameters-&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Cost&lt;/b&gt;
Tableau is expensive than Power BI when it comes to buying the complete version which usually is bought by the businesses and working professionals. Tableau’s annual price ranges around $1000 while Power BI’s annual price ranges around $100. Even though there is a great price difference in the full version, however if we compare only the free version Tableau has an upper hand. This is because many useful features were not included in Power BI’s free desktop version and this certainly impaired us during this project. We also had to face many hassles when it came to editing the shared files.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Data Analysis&lt;/b&gt;
Power BI should be the choice if someone wants to have an in- depth analysis of the data. It offers DAX (Data Analysis Expressions) which is a delight to work with. It is designed to work with tables and relational database, creating meaningful relationships between various data sources.
Tableau has in-built features like data blending and drill-down, which one can use to determine the variations, data patterns and for further data analysis.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Data Visualization&lt;/b&gt;
There is no one best tool when it comes to Data Visualization as both Power BI and Tableau has some really powerful visuals to offer. While talking about simplicity, surely Tableau creates fascinating dashboards through simple drag- and- drop, and complex calculations can also be made with the help of simple line of codes. It offers various types of visualizations such as Heat maps, Treemaps, Scatter Plots etc. One can also create ‘Word clouds’ and ‘Bubble charts’ in Tableau.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;On the other hand, Power BI has loads of visualisations which help in depth analysis. Power BI boasts of a wide variety of visualizations, such as R script visuals and Python visuals as well. These visuals can be created in Power BI Desktop and then published online.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Performance&lt;/b&gt;
The Clock Rate in Power BI is faster than that in Tableau, it loads data sets faster than the latter, plus it saves the files in a compressed manner and takes lower disk space. One can also publish their Power BI Desktop reports online and thus can have an easy access.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;User community&lt;/b&gt;
Both these tools offer great customer support, in terms of services and learning material. However, Tableau may have an upper edge in community support due to its huge user base and awareness in general.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;User Interface&lt;/b&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Tableau has an edge over Power BI when it comes to the user interface. It has an intuitive user interface along with versatile features. Its Tool Tip is more efficient than Power BI’s and provides elaborate details.
Power BI’s user interface is no less than that of Tableau’s but it can be a little intimidating for a novice.&lt;/p&gt;

&lt;p&gt;After comparing the 2 tools on various parameters and from our personal experience, we felt that Tableau is the better of the two when it comes to creating Dashboards and visualizations.&lt;/p&gt;

&lt;p&gt;Conclusion-
Tableau remains the choice of BI Tool for students from non-technical background like us. However, the professional reports and perfect visualisations by Power BI would certainly add a feather in your cap in terms of both skillset and employability.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;&lt;u&gt;Authored by&lt;/u&gt;&lt;/b&gt;: Chaitanya Gupta and Ritik Garg&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;&lt;u&gt;Co-Authored by&lt;/u&gt;&lt;/b&gt;: Mr Shoury Anand and Mr Vikrant Sharma
&lt;img src=&quot;/blog/tableau/3.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;
&lt;img src=&quot;/blog/tableau/4.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;
&lt;img src=&quot;/blog/tableau/5.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Initially, BI used to be the work of just IT professionals, who would supply the whole organisation with the reports for decision-making, however, now it is used at every stage of management by the managers (primarily due to the development in data science sphere and the rise of self- service BI tools).</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Decoding APIs?</title><link href="http://localhost:4000/blog/API-Basics/" rel="alternate" type="text/html" title="Decoding APIs?" /><published>2020-09-08T00:00:00+05:30</published><updated>2020-09-08T00:00:00+05:30</updated><id>http://localhost:4000/blog/API-Basics</id><content type="html" xml:base="http://localhost:4000/blog/API-Basics/">&lt;p&gt;What is an API?&lt;/p&gt;

&lt;p&gt;Application Program Interface. Wait you didn’t ask for its expansion, you wanted to know what an API actually meant and what it is used for; but sadly, this what you get to know once you google for an API and then you get extremely confused. As always, we’re here to help.&lt;/p&gt;

&lt;p&gt;An API, enables developers to integrate one app with another. They are the interfaces provided by servers that you can use to, among others, retrieve and send data using code.&lt;/p&gt;

&lt;p&gt;A simpler explanation?&lt;/p&gt;

&lt;p&gt;Let’s assume you wanted to ask Github to send you all the user details for your study. Even though you wanted the data for the study, Github cannot give you login credentials (email, password, etc.) of other users in your hands. But Github can do one thing, it can provide you with a portal/interface from where you could fetch limited user data (like only no. of commits made, no. of repositories, etc.). This interface is called an API. An API enables developers (like you ;)) to integrate one app (your python code) with another (like Github). They are the interfaces provided by servers that you can use to, among others, retrieve and send data using code.&lt;/p&gt;

&lt;p&gt;Why use APIs?&lt;/p&gt;

&lt;p&gt;APIs can help you fetch real-time data for your application in a well-structured way. They help in automating things that might have taken months if you did them manually using your web browser.&lt;/p&gt;

&lt;p&gt;How do APIs send data?&lt;/p&gt;

&lt;p&gt;Most of the APIs send data in JSON (JavaScript Object Notation). JSON is a lightweight format for storing and transporting data. It looks like a python dictionary or a python array. Hence, all the manipulation you did on python dictionaries and arrays come in handy here. It looks like:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;err&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;analytics&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bay&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
	&lt;span class=&quot;err&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Organization&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Nuclues&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
	&lt;span class=&quot;err&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;helpful&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Status Codes&lt;/p&gt;

&lt;p&gt;Before we deep dive into using APIs, we need to know how the server will respond to our requests. When a request fails, we can have different reasons for the same; maybe the server wasn’t functioning properly or we requested for something strange. All of this information is communicated using HTTP status codes, which are 3-digit numbers divided into categories.&lt;/p&gt;

&lt;p&gt;Informational responses (100–199),&lt;/p&gt;

&lt;p&gt;Successful responses (200–299), =&amp;gt;This is what we aim for&lt;/p&gt;

&lt;p&gt;Redirects (300–399),&lt;/p&gt;

&lt;p&gt;Client errors (400–499) =&amp;gt; The errors from client side (due to us)&lt;/p&gt;

&lt;p&gt;and Server errors (500–599) =&amp;gt; The errors from server side&lt;/p&gt;

&lt;p&gt;What is a request?&lt;/p&gt;

&lt;p&gt;You would encounter three major terminologies that sum up to make a request:&lt;/p&gt;

&lt;p&gt;Endpoints =&amp;gt; This is usually a url from which you need to fetch data&lt;/p&gt;

&lt;p&gt;Methods =&amp;gt; either GET, PUT, POST, or DELETE.&lt;/p&gt;

&lt;p&gt;Headers =&amp;gt; this contains information regarding authentication.&lt;/p&gt;

&lt;p&gt;When are we going to actually work with APIs?&lt;/p&gt;

&lt;p&gt;Now, Let’s fetch data from covid19api.com. First, we need to find the right endpoint. You can do so by looking up the API documentation of the same https://documenter.getpostman.com/view/10808728/SzS8rjbc?version=latest#7934d316-f751-4914-9909-39f1901caeb8&lt;/p&gt;

&lt;p&gt;We would be using an inbuilt library json (to convert json response to python object) and install the requests library (to use its methods to make requests for data)&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;		&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;installing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;json&lt;/span&gt;		&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;importing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;requests&lt;/span&gt;		&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;importing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;api_base_url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;https&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;covid19api&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;com&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dayone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;country&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;india&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confirmed&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;	&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endpoint&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt;	&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SOME&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KEY&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;	&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;only&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;you&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;need&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;API&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Content-Type'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'application/json'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Authorization'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Bearer {0}'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endpoint&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;protected&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_base_url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;		&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;json_response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'utf-8'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;		&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loads&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;convert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;python&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s Print json_response to see what we got
print(json_response)&lt;/p&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Cases':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'City':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'CityCode':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Country':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'India'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'CountryCode':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'IN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Date':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2020-01-30&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Z'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Lat':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;20.59&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Lon':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;78.96&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Province':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Status':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'confirmed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Cases':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'City':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'CityCode':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Country':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'India'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'CountryCode':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'IN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Date':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2020-01-31&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Z'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Lat':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;20.59&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Lon':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;78.96&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Province':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Status':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'confirmed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Cases':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;456183&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'City':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'CityCode':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Country':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'India'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'CountryCode':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'IN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Date':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2020-06-23&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Z'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Lat':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;20.59&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Lon':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;78.96&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Province':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Status':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'confirmed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Cases':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;473105&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'City':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'CityCode':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Country':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'India'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'CountryCode':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'IN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Date':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2020-06-24&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Z'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Lat':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;20.59&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Lon':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;78.96&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Province':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Status':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'confirmed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You can see we received an array of dictionaries. Now we need to iterate between the array to access data (keys and values) inside dictionary.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;Dates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Cases&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json_response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;Cases&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Cases'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;Dates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s plot the Dates with the no. of cases.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;12.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cases&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Close'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'0.95'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/blog/api.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And we told you! API’s did it. They can do much more wonders when used the right way for the right projects. You can even create one (but that’s a blog for another day). Hope you have a good understanding of the use case of this beautiful piece of technology in data science.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">What is an API?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ML CHALLENGE - CHURN MODELLING</title><link href="http://localhost:4000/blog/ML-CHALLENGE-CHURN-MODELLING/" rel="alternate" type="text/html" title="ML CHALLENGE - CHURN MODELLING" /><published>2020-07-28T00:00:00+05:30</published><updated>2020-07-28T00:00:00+05:30</updated><id>http://localhost:4000/blog/ML-CHALLENGE-CHURN-MODELLING</id><content type="html" xml:base="http://localhost:4000/blog/ML-CHALLENGE-CHURN-MODELLING/">&lt;p&gt;&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/ML CHALLENGE – CHURN MODEL.jpg&quot; alt=&quot;ML Challenge&quot; /&gt;
The topics covered in the series were,&lt;/p&gt;

&lt;p&gt;All about Machine Learning
Random Forest
Logistic Regression
K-Nearest Neighbours
Support Vector Machines (SVMs)
Clustering
Association rule learning with Apriori
Deep Learning and AI
To provide hands on practice so that our followers can now learn while practicing we opened a Machine Learning Challenge based on Churn Modelling. An important area of operation of banks is Churn Analysis. Churn Modelling or Analysis is the evaluation of the customer loss rate in order to reduce it. Banks are intrigued to identify segments of such customer as the cost for associating with a new customer is usually higher than retaining the old one.&lt;/p&gt;

&lt;p&gt;For this challenge, a dataset was provided wherein, a Bank is witnessing some unusual churn rates and thus they want to predict whether a customer will leave the bank or not, based on the data provided.&lt;/p&gt;

&lt;p&gt;The winner of the Challenge, Soham Mukherjee, majorly performed following 4 steps to solve this Challenge-&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Exploratory Data Analysis&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data Pre-Processing&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data Modelling&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Boosting Techniques&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Exploratory Data Analysis (EDA)
Imagine your friends decide to go out for dinner to a restaurant you have never heard of before. Being a foodie, you will straight away find yourself perplexed with several questions. As a rule of thumb, you would go online to check for menu, reviews, and ratings. Furthermore, you will dig out details on the experience of people from the restaurant.&lt;/p&gt;

&lt;p&gt;Whatever research you will undertake before finally booking your table for the dinner is nothing but what Data Scientists in their language call “Exploratory Data Analysis”&lt;/p&gt;

&lt;p&gt;EDA refers to the analytical procedure of performing preliminary investigations on data in order to discover patterns, to detect abnormalities in data, to test hypothesis and to check assumptions with the support of insights on synopsis and graphical representations of data provided.&lt;/p&gt;

&lt;p&gt;In this case, our Winner performed multiple EDA techniques including checking for skewness, visualizing with help of histogram plots, and checking the relationship between variables. Further distribution of dependent variable was depicted using a pie chart.&lt;/p&gt;

&lt;p&gt;Data is always first available in human readable form due to which the data is labelled with words. However, Label Encoding is applied to convert the words into numeric values and make the data in machine-readable form. Therefore, the categorical variables (Geography, Gender) were first encoded with the help of following code-
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/1.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once data was verified for duplications, the participant checked the skewness of the variables and then further illustrated the frequency-variable chart for each variable as shown below-
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/2.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then, some detailed visualizations were prepared with help of Pair Plots. One of the most significant analysis is to check the relationship between the two variables. To accomplish that so that further decisions could be made, a heat map was plotted that illustrating the correlation between the variables-
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/3.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since Correlation of X with itself is always 1, we get a diagonal which divides data into two triangles.&lt;/p&gt;

&lt;p&gt;Now that Exploratory Data Analysis has been performed and required results fetched, by going into the intricacies of data with help of certain visualizations, the next step would be Data Pre-processing to make data fit for Machine Learning Modelling.&lt;/p&gt;

&lt;p&gt;Data Pre-processing
This stage is one of the most significant steps in Machine Learning and no data scientist can ever skip this step in his/her analysis.&lt;/p&gt;

&lt;p&gt;To define in simple words, Data Preprocessing is a technique to convert the raw data which has been gathered from numerous sources into cleaner and more meaningful pieces of information. Datasets are incredibly massive and usually contain unnecessary data as well. That vast amount of information is also heterogeneous by nature, which means that they don’t share the same structure and thus there is also a need to standardize the data first. Data Preprocessing technically deals with issues such as inconsistent data, missing values, insufficient data, imbalance data, etc.&lt;/p&gt;

&lt;p&gt;In the winner’s entry, special attention was given to this step and he removed the “Balance” variable as it would decrease accuracy. It was observed from the correlation matrix between all the variables that Balance variable can reduce the accuracy of model as it was less related to the target and other variables.  Further to handle the imbalance and inconsistent data following two techniques were performed-&lt;/p&gt;

&lt;p&gt;SMOTE for Handling Imbalanced Data- 
Data imbalance usually reflects an unequal distribution of classes within a dataset and a major issue associated is that there are very few examples of the minority class in data for a model to effectively learn and predict. Therefore, to overcome this issue, one way is to oversample the examples in the minority class. The same can be achieved by replicating examples from the minority class in the training dataset before fitting a model. This can balance the class distribution without providing any additional information to the model. An improvement over replicating examples from the minority class is to synthesize new examples from the minority class.&lt;/p&gt;

&lt;p&gt;For the same, the most widely used approach to synthesizing new examples is called the Synthetic Minority Oversampling Technique (SMOTE). SMOTE first selects a minority class instance at random and finds its k-nearest minority class neighbours. These synthetic training records are generated by randomly selecting one or more of the k-nearest neighbours for each example in the minority class. After the oversampling process, the data is reconstructed and several classification models can be applied for the processed data.
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/4.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Standardization of Inconsistent Data-
The purpose of standardising the dataset is to ensure that each of the variables contributes equally to the analysis without any bias. It becomes essential to do standardization before applying Machine Learning Models as variables are the core of the model and the analysis is pretty much sensitive regarding the variance of initial variables. That is if there are variables with large differences between the initial variables, the variable with larger range will dominate over the smaller ranges.&lt;/p&gt;

&lt;p&gt;For example, consider two variables, distance within city and distance between cities. One of the variables is measured in meters while the other in Kilometres. Now if the data is not standardised then distance of 500 meters would be considered as greater than distance of 5KM, which isn’t true. Therefore, transforming the data to comparable scales can prevent this problem.&lt;/p&gt;

&lt;p&gt;Mathematically, it is done by subtracting the mean and dividing by the standard deviation for each value of each variable.&lt;/p&gt;

&lt;p&gt;z = (value – mean)/ (standard deviation)&lt;/p&gt;

&lt;p&gt;All the variables will be transformed to the same scale once the standardization is completed. Finally, the data is divided into training and test sets for further steps.&lt;/p&gt;

&lt;p&gt;Data Modelling
This is a technique to analyse data and acquiring certain results from it. There are various data modelling techniques which can be used such as Random Forest Regression, Logistic Regression, K Nearest Neighbours etc. All these techniques can be used to fit in the dataset and then predict the required outcome. For this purpose, dataset is divided into two categories-&lt;/p&gt;

&lt;p&gt;Train set- For fitting the model.
Test set- To predict the required outcome.
The various data modelling techniques used in this case are-&lt;/p&gt;

&lt;p&gt;Logistic Regression
Random Forest Classifier
Decision Tree Classifier
Support Vector Machines
K nearest neighbours
Here is the code and confusion matrix for the Random Forest Classifier;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/5.png&quot; alt=&quot;ML Challenge&quot; /&gt;
Heatmap from the seaborn library has been used in the Confusion Matrix for better understanding-&lt;/p&gt;

&lt;p&gt;The confusion matrix shows that among all positive predictions, 84.4% of the predictions were true positives and it was correctly predicted that a customer will churn. Rest 15.6% were false positives which means that it was wrongly predicted that these customers will churn.&lt;/p&gt;

&lt;p&gt;Similarly, out of the negative predictions, 83.8% were true negatives while rest 16.2% were false negatives and they were wrongly predicted.
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/6.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Boosting Techniques-
Boosting techniques helps to convert weak learners into strong learners. The various boosting techniques used in the case are-&lt;/p&gt;

&lt;p&gt;Gradient Boosting Classifier
Ada boost Classifier
XG Boost
Here is a sample code for Gradient Boosting Classifier
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/7.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The winner has used certain techniques to increase the accuracy of the model in the XG Boost technique-&lt;/p&gt;

&lt;p&gt;Random search has been used to find out the best parameters with the help of a Param grid. It searches among a given set of values to find out parameters with highest accuracy.
Stratified-K-Fold shuffles the data and then split the dataset into n-splits (specified number of splits) Then it uses each part as a test set.
When applying machine learning models, one usually does data pre-processing (as explained above), feature engineering, feature extraction and, feature selection. After this, he/ she selects the best algorithm and tuning of the parameters is done in order to obtain the best results.&lt;/p&gt;

&lt;p&gt;AutoML is a series of concepts and techniques used to automate these processes. It reduces the bias and errors that occur when a human being is designing the machine learning models. An organization can also reduce the cost of hiring many experts by applying AutoML in their data pipeline. The winner achieved the best accuracy with the help of Stacked Ensemble (Auto ML).&lt;/p&gt;

&lt;p&gt;We hope that you understood the framework used in order to solve this Challenge. Also, if the training time of model was increased from 5 minutes to probably a couple of hours, the model, the accuracy would have been around a remarkable 94-96%. In case of any queries, feel free to reach out to us.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">The topics covered in the series were,</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Data Visualization in R using ggplot2 (PART 2)</title><link href="http://localhost:4000/blog/Data-Visualization-in-R-Part2/" rel="alternate" type="text/html" title="Data Visualization in R using ggplot2 (PART 2)" /><published>2020-07-14T00:00:00+05:30</published><updated>2020-07-14T00:00:00+05:30</updated><id>http://localhost:4000/blog/Data-Visualization-in-R-Part2</id><content type="html" xml:base="http://localhost:4000/blog/Data-Visualization-in-R-Part2/">&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Featured.jpg&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
PLOT 11 - JITTER
&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot11 jitter.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
So, what’s a jitter? Jitter is a random value that is assigned to the variable so they don’t fall on top of each other.&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y= price,x=cut,color=cut))  + geom_jitter(size=1.2, alpha= 0.5)&lt;/p&gt;

&lt;p&gt;PLOT 12 - Box Plot&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y= price,x=cut,color=cut))  + geom_boxplot(size=1.2)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/2.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Box pot is a method to show data through their quartiles. The line in middle shows the median of the range. This gives us a hint about skewness and some other characteristics of the data.&lt;/p&gt;

&lt;p&gt;The dots out of the box represent outliers. It is a useful tool to quickly represent outliers.&lt;/p&gt;

&lt;p&gt;Here it can be observed that median of the premium quality cuts is higher, this sounds very reasonable because the premium quality does have high prices.&lt;/p&gt;

&lt;p&gt;PLOT 13 -  Histogram&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, color= “Blue”, fill= “White”)
&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/3.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The histogram represents the price of a diamond and the no. of diamonds in each price range. Binwidth here represents the width of each bar in the histograms. Please note that here colour represents the outline colour of the bar and fill represents the inside. A key take away from here is that almost more than 15000 diamonds lie in the range of 1000$-2000$. But remember, what we’ve tried now is setting i.e assigning one colour to the whole plot. Let’s try mapping -&lt;/p&gt;

&lt;p&gt;PLOT 14 -  Mapping Histogram&lt;/p&gt;

&lt;p&gt;Trying out mapping -&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, aes( fill= clarity))
&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/4-Plot15-Adding Color.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now we have the categorization based on clarity in each price range but the graph looks distorted and cluttered in. Let’s try to add a borderline to see a better view.&lt;/p&gt;

&lt;p&gt;PLOT 15 -  Adding Color&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, aes( fill= clarity),color=”Black”)&lt;/p&gt;

&lt;p&gt;The chart now looks good and is understandable. Let’s move on to explore facets now -&lt;/p&gt;

&lt;p&gt;Facets –&lt;/p&gt;

&lt;p&gt;Facets are used to create subdivision. For example - we want to see the price of each diamond concerning its cut independently. We will use facets to divide the diamonds according to their cuts. The code for creating distinct plots in one bar is – facet_grid(Rows~Columns).&lt;/p&gt;

&lt;p&gt;Replace rows for dividing charts in the form of rows and columns for the same. Let’s try it out -&lt;/p&gt;

&lt;p&gt;PLOT 16 -  Scatter Plot (Facet Version)&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(x = carat ,y=price)) + geom_point() + facet_grid(clarity~.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot-16-Scatter Plot.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
Here, we have a relation between price and carat of each category of clarity. Each category of clarity has a different scatter plot in the form of rows. We would like you to try the same thing in the column form of facet_grid. Code would appear something like this -&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(x = carat ,y=price)) + geom_point() + facet_grid(~Clarity)&lt;/p&gt;

&lt;p&gt;PLOT 17 -&lt;/p&gt;

&lt;p&gt;Now we’ll use both row and column&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(x = carat ,y=price)) + geom_point() + facet_grid(clarity~cut)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot 17.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The chart now is depicting relationship between carat and price on the basis of the quality of cut as well as clarity.Let’s refine our chart to make it visually appealing.&lt;/p&gt;

&lt;p&gt;PLOT 18 -  Refining Visualization&lt;/p&gt;

&lt;p&gt;Code -  ggplot(data=diamond, aes(x = carat ,y=price)) + geom_point(aes(color= cut),size=0.5,alpha=0.5) + facet_grid(clarity~cut)+geom_smooth()&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot 18 Refining viz.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ve reduced the size of the dots to make it easy for us to interpret. Smoother is added to identify trends, if there’s any. It can be seen from the chart that diamonds with I1 clarity and very good cut has almost equal price range as the premium cut. 
The cheapest diamonds are here seen to be of VVS2 category with fair cut, which makes sense.
Coordinates –&lt;/p&gt;

&lt;p&gt;Co-ordinates deal with adjusting or zooming in and out of your coordinates. Let’s zoom in to one of the previously made visualizations.&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, aes( fill= clarity),color=”Black”) + coord_cartesian(ylim = c(0,10000))&lt;/p&gt;

&lt;p&gt;PLOT 19 -&lt;/p&gt;

&lt;p&gt;The y-axis is zoomed in from (0-10,000), there is also another method of limiting your axes. You can similarly limit the x-axis as well. However, that method sometimes cuts your data and is not advisable. The code is given here, we would suggest you to try it yourself and identify this limitation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot 19.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, aes( fill= clarity),color=”Black”) + ylim(0,10000)&lt;/p&gt;

&lt;p&gt;PLOT 20 -&lt;/p&gt;

&lt;p&gt;Let’s now try zooming in both the axes -&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, aes( fill= clarity),color=”Black”) +&lt;/p&gt;

&lt;p&gt;coord_cartesian(ylim = c(0,10000),xlim = c(0,10000))&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot 20.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ve successfully zoomed in both the axes, we believe that is all there is tell about axes. Let’s move on to themes.&lt;/p&gt;

&lt;p&gt;Themes –&lt;/p&gt;

&lt;p&gt;Theme is all about formatting your axes labels, adjusting and positioning legend. This should’ve been done to every chart but you can of course go and apply this to every chart.&lt;/p&gt;

&lt;p&gt;Let’s start with one of our most basic plots and start giving it the elements of themes.&lt;/p&gt;

&lt;p&gt;Starting with labels and titles -&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y=price,x=carat, color= cut)) + geom_point(alpha=0.5) + xlab(“Diamond carat”) + ylab(“Price”) + ggtitle(“Relationship between Price and Carat”)&lt;/p&gt;

&lt;p&gt;PLOT 21 -  Giving label&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot 21 giving labe_.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
The chart looks more descriptive, but there is still a lot of scope for improvement. Let’s try to change the font of labels.&lt;/p&gt;

&lt;p&gt;PLOT 22 -  Adjusting Label&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y=price,x=carat, color= cut)) + geom_point(alpha=0.5) + xlab(“Diamond carat”) + ylab(“Price”) +&lt;/p&gt;

&lt;p&gt;ggtitle(“Relationship between Price and Carat”) +&lt;/p&gt;

&lt;p&gt;theme(axis.title.x = element_text(color = “Dark Blue”, size = 30),&lt;/p&gt;

&lt;p&gt;axis.title.y = element_text(color = “Dark Blue”, size = 30),&lt;/p&gt;

&lt;p&gt;axis.text.x = element_text(size=20), axis.text.y = element_text(size=20))&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot22 Adjust label.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
Finally, Lets adjust labels and get done with it.&lt;/p&gt;

&lt;p&gt;PLOT 23 -  Final Touch&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y=price,x=carat, color= cut)) + geom_point(alpha=0.5) + xlab(“Diamond carat”) + ylab(“Price”) +  ggtitle(“Relationship between Price and Carat”) +&lt;/p&gt;

&lt;p&gt;theme(axis.title.x = element_text(color = “Dark Blue”, size = 20),axis.title.y = element_text(color = “Dark Blue”, size = 20),axis.text.x = element_text(size=20),axis.text.y = element_text(size=20),legend.title = element_text(size = 15),legend.text  = element_text(size = 15),legend.position = c(1,1),legend.justification = c(1,1), plot.title = element_text(color=”Dark Blue”,size=20,family = “Courier”))&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot23 final touch.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
Finally, we’re done with everything. I would suggest you to try things on your own and experiment things in order to get a better view yourself.&lt;/p&gt;

&lt;p&gt;Before proceeding towards visualization, it is highly suggested to be aware about all the elements of your data set and what is to be achieved from it.&lt;/p&gt;

&lt;p&gt;GGPLOT2, in it has even more fascinating and useful tools. We suggest our readers to explore more of it and learn more about it.&lt;/p&gt;

&lt;p&gt;The use of visualization techniques differ according to the purpose it serves but the underlying idea essentially remains the same. We hope that the basic idea and concept of visualization is clear to all of you. So, play around with these visualizations and do write to us in case of any queries. All the best!!!&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">PLOT 11 - JITTER So, what’s a jitter? Jitter is a random value that is assigned to the variable so they don’t fall on top of each other.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SPURIOUS CORRELATION - A CORRELATION WITH NO RELATION</title><link href="http://localhost:4000/blog/SPURIOUS-CORRELATION/" rel="alternate" type="text/html" title="SPURIOUS CORRELATION - A CORRELATION WITH NO RELATION" /><published>2020-06-30T00:00:00+05:30</published><updated>2020-06-30T00:00:00+05:30</updated><id>http://localhost:4000/blog/SPURIOUS-CORRELATION</id><content type="html" xml:base="http://localhost:4000/blog/SPURIOUS-CORRELATION/">&lt;p&gt;&lt;img src=&quot;/blog/2020-06-30- Spurious Correlation_ A Correlation with no Relation/sp.jpg&quot; alt=&quot;Spurious Correlation&quot; /&gt;
Correlation is used to test the relationship between 2 or more variables. For instance, Weight and height are correlated. A taller person would tend to have more weight than a shorter person. However, this may not be always true. Hence, we use the Correlation coefficient, which measures as to how strong/ weak, the relation is between the variables. The range of correlation coefficient varies between (-1,1). The higher the correlation coefficient, the higher is the positive correlation between the variables and vice versa.&lt;/p&gt;

&lt;p&gt;ROLE OF CORRELATION IN BUSINESS LIFE&lt;/p&gt;

&lt;p&gt;Simply, the importance of correlation in business decision- making means better weighing of factors that have an effect on its performance.&lt;/p&gt;

&lt;p&gt;For instance, given are some common business correlations -&lt;/p&gt;

&lt;p&gt;Allocating more resources on Research &amp;amp; Development (R&amp;amp;D) correlates with more innovation.
The hiring of employees with certain personality characteristics correlates with higher productivity.
Correlation is greatly used in predicting the future of a business direction using Regression. If marketers identify a correlation between consumer behavior and events and a particular type of product or service, then they can take advantage of the relationship to boost business.&lt;/p&gt;

&lt;p&gt;If measures are taken to correlate unknown factors with business performance, then it may lead to less uncertainty in the Business since these unknown factors may have a huge impact but sometimes, these factors are volatile, complex, unknown; hence it is difficult to take them into consideration.&lt;/p&gt;

&lt;p&gt;Correlation is greatly used in various business industries, one such being Marketing Analytics. Managers lookout for the following correlations -&lt;/p&gt;

&lt;p&gt;Correlations between on-page keyword use and rankings.
Correlation with the type of top-level domain (.com, .org, etc.) and rankings in Google search.
Relation of posts with images getting more shares across social media.&lt;/p&gt;

&lt;p&gt;Above is a snapshot of an article in The Washington Post about the relationship between the gender of hurricanes’ names and the number of deaths the hurricane causes.&lt;/p&gt;

&lt;p&gt;The article’s title is “Female-named hurricanes kill more than male hurricanes because people don’t respect them, study finds.” The author concluded that the number of hurricane-related deaths is caused due to the gender of the hurricane’s name. Now, there may be (read ‘is’) relation between these two variables, however, there is no reason to believe that due to specific naming of the hurricane, it leads to more deaths, since ‘people take feminine hurricane names less seriously than their male counterparts’ and hence, do not prepare themselves as they should.&lt;/p&gt;

&lt;p&gt;The author realized the mistake and hence, the title was changed.&lt;/p&gt;

&lt;p&gt;Correlation Vs Causation&lt;/p&gt;

&lt;p&gt;A basic fact with respect to both these terms is that - “Correlation doesn’t imply Causation”. Let’s understand what this statement means;&lt;/p&gt;

&lt;p&gt;Causation, also referred to as ‘ Cause and effect’, is just an extension of Correlation, which says that a change in one variable will cause a change in the value of another value.&lt;/p&gt;

&lt;p&gt;Hence, correlation and causation must not be mixed up.&lt;/p&gt;

&lt;p&gt;Since now we understand the difference between Correlation and Causation, Let’s move to Spurious Correlation.&lt;/p&gt;

&lt;p&gt;SPURIOUS CORRELATION is a relationship wherein two events or variables are associated or correlated with each other, but not causally related i.e. they have no relation or meaning between them. A spurious correlation is usually caused due to coincidence or a third factor that may be ignored at the time of examination, usually known as the ‘lurking variable’ or ‘confounding factor’.&lt;/p&gt;

&lt;p&gt;Whenever two events are related in the same direction, we say that there exists a correlation between them but most of the events are spuriously related as we do not find any cause of this relation.&lt;/p&gt;

&lt;p&gt;Given is an interesting example of spurious correlation, you may notice that the graph is perfectly correlated but there is no reason or cause between this way- apart- from each other events&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2020-06-30- Spurious Correlation_ A Correlation with no Relation/http_www.tylervigen.com_spurious-correlations.jpg&quot; alt=&quot;Spurious Correlation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Photo credit - http -//www.tylervigen.com&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2020-06-30- Spurious Correlation_ A Correlation with no Relation/image-18.png&quot; alt=&quot;Spurious Correlation&quot; /&gt;
In the above example, Ageing is a confounding variable. The apparent association between living in old age homes and having Alzheimer’s is confounded by age.&lt;/p&gt;

&lt;p&gt;If a researcher doesn’t take age into consideration, then he/ she may draw incorrect conclusions about the correlation between living in an old age home and Alzheimer’s.&lt;/p&gt;

&lt;p&gt;Misrepresentation of Data to show correlation -&lt;/p&gt;

&lt;p&gt;Skewed Scales Manipulating Ranges to Align Data&lt;/p&gt;

&lt;p&gt;Even when Y axes measure the same category, changing the scales can alter the lines to suggest a correlation. These Y axes for a company’s yearly revenue from a particular country, differ in range and proportional increase.
&lt;img src=&quot;/blog/2020-06-30- Spurious Correlation_ A Correlation with no Relation/graph 1.png&quot; alt=&quot;Spurious Correlation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Eliminating the second axis shows how skewed this chart is. Such problems can arise during the analysis due to the misrepresentation of the data.
&lt;img src=&quot;/blog/2020-06-30- Spurious Correlation_ A Correlation with no Relation/graph 2.png&quot; alt=&quot;Spurious Correlation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;How to Spot Spurious Correlations -&lt;/p&gt;

&lt;p&gt;It is very important to spot a Spurious correlation. Some of the methods that can be used are as follows -&lt;/p&gt;

&lt;p&gt;•Using a null hypothesis and checking for a high p-value.&lt;/p&gt;

&lt;p&gt;•By ensuring there is a proper representative sample.&lt;/p&gt;

&lt;p&gt;•By having an adequate sample size.&lt;/p&gt;

&lt;p&gt;Well, the truth is that Spurious Correlations are everywhere, you will find it in the news, websites, blog posts, etc. Many times, these correlations may even be used to share fake news.&lt;/p&gt;

&lt;p&gt;So, what are you waiting for start googling up and find correlation between events that may amuse you and us!&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Correlation is used to test the relationship between 2 or more variables. For instance, Weight and height are correlated. A taller person would tend to have more weight than a shorter person. However, this may not be always true. Hence, we use the Correlation coefficient, which measures as to how strong/ weak, the relation is between the variables. The range of correlation coefficient varies between (-1,1). The higher the correlation coefficient, the higher is the positive correlation between the variables and vice versa.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Data Visualization in R using ggplot2 (PART 1)</title><link href="http://localhost:4000/blog/Data-Visualization-in-R-Part1/" rel="alternate" type="text/html" title="Data Visualization in R using ggplot2 (PART 1)" /><published>2020-06-16T00:00:00+05:30</published><updated>2020-06-16T00:00:00+05:30</updated><id>http://localhost:4000/blog/Data-Visualization-in-R-Part1</id><content type="html" xml:base="http://localhost:4000/blog/Data-Visualization-in-R-Part1/">&lt;p&gt;&lt;img src=&quot;/blog/Data Visualization in R Part1.jpg&quot; alt=&quot;Data Visualization&quot; /&gt;
&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/1.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;s this image speaks for itself, we think this gives enough clarity about the elements, which are essential in making the plot. Before we proceed further, we’d like to tell that the first 3 layers are the essentials to making a plot.Otherwise, we’ll have nothing but an empty graph. Now, as the old saying goes “learn by doing”. We’ll jump on to GGPLOT, start from the basics and explore all layers.&lt;/p&gt;

&lt;p&gt;Let’s Begin&lt;/p&gt;

&lt;p&gt;As you know before starting anything we need to make sure we’ve installed the package and activated it in our library. Please note the code for installing and activating the package&lt;/p&gt;

&lt;p&gt;Instal.package(“ggplot2”)&lt;/p&gt;

&lt;p&gt;library(ggplot2)&lt;/p&gt;

&lt;p&gt;As soon as the package is installed and the data set is imported, we will start working on the visualization part.&lt;/p&gt;

&lt;p&gt;Please do follow the codes and practice them simultaneously on R for a thorough and complete understanding of these beautiful visualizations.&lt;/p&gt;

&lt;p&gt;Dataset Link- https-//vincentarelbundock.github.io/Rdatasets/datasets.html&lt;/p&gt;

&lt;p&gt;PLOT 1 - Blank Graph&lt;/p&gt;

&lt;p&gt;The data taken here is life expectancy over the years in India since 1960.&lt;/p&gt;

&lt;p&gt;Code - ggplot(data= life_expectancy, aes(x=expectancy, y = Years))
&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot 1-blank graph.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, the plot is empty; this is because the minimum 3 elements i.e data, aesthetics, and geometries are not supplied in the code. The plot does contain data and aesthetics but before knowing what type of geometry is to be made, how can anything move forward?&lt;/p&gt;

&lt;p&gt;PLOT 2  - Scatter Plot&lt;/p&gt;

&lt;p&gt;Let’s make a simple point chart!&lt;/p&gt;

&lt;p&gt;ggplot(data=x, aes(x=years, y=Expectancy) + geom_point
 Now that we’ve added geometry the plot is finally formed with a scatter plot.
&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot2 scatter plot.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;MOVING FORWARD!&lt;/p&gt;

&lt;p&gt;But why would anyone go through all the trouble of making a visualization as simple as this, which can even be made in Excel? So let us look at some things which cannot be done in Excel. The data set we have taken has 6 columns. It is a data set of different categories of diamond according to their price, cut, clarity and some other elements.&lt;/p&gt;

&lt;p&gt;PLOT 3 -  Scatter Plot&lt;/p&gt;

&lt;p&gt;Preview of the data can be taken by&lt;/p&gt;

&lt;p&gt;Summary(diamonds)&lt;/p&gt;

&lt;p&gt;ggplot(data = x, aes(x=carat,y=price)) + geom_point()
&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot 3.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The plot is again a simple scatter plot with the relationship between price and carat. It can roughly be concluded that there exists a positive relationship between price and carat i.e as the carat increases price also increase. But again, this can also be done in excel, then why should we prefer R over excel? Let’s try to build over this only-&lt;/p&gt;

&lt;p&gt;PLOT 4-  Adding Colors and Size&lt;/p&gt;

&lt;p&gt;Now what If you want to work with more than 2 variables? let’s say we want to see the price, carat as well as the cut in each carat category and its price&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y=price,x=carat, color= cut)) + geom_point()&lt;/p&gt;

&lt;p&gt;Now,we have Price of diamonds with each carat along with the cut, so the color signifies the quality of cut in each category and its price.So, now we have a strong reason for choosing R over excel when it comes to visualizing big data&lt;/p&gt;

&lt;p&gt;Even 3 variables is not enough and we want to classify the data  on the basis of the size too.&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y=price,x=carat, color= cut,size=depth)) + geom_point()&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot4.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see the size of the points is adjusted according to the depth. The description is given in the legend. However, the plot looks horrible and hardly any interpretation can be made from it. It is just to give you an idea of how elements can be used.&lt;/p&gt;

&lt;p&gt;SIMPLIFYING CODING&lt;/p&gt;

&lt;p&gt;PLOT 5 -  Scatter Plot&lt;/p&gt;

&lt;p&gt;It gets tedious to add again and again the primitive part of the code and then to add layers when it is nothing but the same code?&lt;/p&gt;

&lt;p&gt;So ,we will store the code in an object to save us from the labor work and make things easier.&lt;/p&gt;

&lt;p&gt;Code - q &amp;lt;- ggplot(data=diamond, aes(y=price,x=carat, color= cut))&lt;/p&gt;

&lt;p&gt;We have saved the code in ‘q’ we don’t need to do anything but build layers on top  of the basic structure&lt;/p&gt;

&lt;p&gt;q + geom_point()&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot5.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It gave you the same result as plot 5 just with a lot of ease. You can develop a basic code store it an object just like we did and then explore your data set with convenience&lt;/p&gt;

&lt;p&gt;OVERRIDING AESTHETICS&lt;/p&gt;

&lt;p&gt;Now that we’ve stored our basic plot details such as data and aesthetics, what if we change our mind and now we want different variables in the x and y-axis. Don’t you think going back and changing the plot again and storing it would be a lot of work? If yes, then you can try a thing called overriding aesthetics, all you need to do is re-enter the new aesthetics in your geometry, which in our case is geom_point. This will override the previously entered aesthetics in our basic object i.e ‘q’ and give us a greater flexibility and encourage to explore even more. Let’s try it out!&lt;/p&gt;

&lt;p&gt;Code - q + geom_point(aes(x= depth, y = price)) + xlab(“Depth”)&lt;/p&gt;

&lt;p&gt;PLOT 6&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot6.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The object q already had a command of depicting the relationship between price and carat but the command in geom_point overrode the command present in the basic plot. Hence, we will display the relationship between depth and price. Now you must be wondering why we had to put a label on the x-axis. This is because one of the limitations of overriding aesthetics is that the label doesn’t get changed. Hence, in this case if it weren’t for me changing the label. The label would’ve been Price and Carat giving out a false representation of the data. This is why you have to be extremely careful while overriding aesthetics.&lt;/p&gt;

&lt;p&gt;MAPPING VS  SETTING&lt;/p&gt;

&lt;p&gt;Mapping is what we’ve been doing till now i.e assigning colorsto a level detail. That means colour here is signifying a bifurcation based on different characteristics of the data set. Plot 4 is a perfect representation of mapping. But what if we don’t want to add color to a level of detail, we just want to change the color from black to Red. That’s where the setting comes in. Assigning one color to the whole data set is called setting. Let’s try it out.&lt;/p&gt;

&lt;p&gt;PLOT 7 - SETTING&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot7.png&quot; alt=&quot;Data Visualization&quot; /&gt;
Code - q + ggplot(aes(x= depth, y = price), color= “Red”) + xlab(“Depth”)&lt;/p&gt;

&lt;p&gt;This is what setting is, assigning a whole new color to the data set. Please note, while setting a color we don’t use a prefix of “aes”. Using aes will cause a misunderstanding and lead to an error. Hence, be cautious with that. We think we don’t need to present mapping as it has already been presented in the former part. We’ve explored the aesthetics enough, now let’s move to on exploring geometries.&lt;/p&gt;

&lt;p&gt;PLOT 8 - Line Chart&lt;/p&gt;

&lt;p&gt;How about we make a line plot instead of a scatter plot&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y= price,x=carat)) +  geom_smooth(fill=NA)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot8.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This chart is simply depicting the relationship with carat. However, this chart in itself is not that informative, so lets also add some more geometries in it.&lt;/p&gt;

&lt;p&gt;PLOT 9 - Line Plot + Scatter plot&lt;/p&gt;

&lt;p&gt;Code  - ggplot(data=diamond, aes(y= price,x=carat)) + geom_point() +  geom_smooth(fill=NA)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot9.png&quot; alt=&quot;Data Visualization&quot; /&gt;
This looks better. But, still it can be misleading as carat is not only deciding factor of price and looking only on this relationship independently can be misleading&lt;/p&gt;

&lt;p&gt;PLOT 10 - Mapping Clarity&lt;/p&gt;

&lt;p&gt;To solve the problem let’s add variable  ‘clarity’ to our plot
Code - ggplot(data=diamond, aes(y= price,x=carat,color=cut)) + geom_point() + geom_smooth(fill=NA)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot10.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We believe this gives a better picture of the prices as the data depicts the prices according to carat as well as clarity
So, we hope that you must have understood the concept of visualization using ggplot2 in R. Worry not, because we will be posting the second part soon covering even more insightful and amazing visualization techniques under ggplot2. Hit the like button, if you liked our post; and subscribe our blog to be connected to us, as many more wonderful topics are underway-)&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Accounting Analytics</title><link href="http://localhost:4000/blog/Accounting-Analytics/" rel="alternate" type="text/html" title="Accounting Analytics" /><published>2020-06-03T00:00:00+05:30</published><updated>2020-06-03T00:00:00+05:30</updated><id>http://localhost:4000/blog/Accounting-Analytics</id><content type="html" xml:base="http://localhost:4000/blog/Accounting-Analytics/">&lt;p&gt;&lt;img src=&quot;/blog/Accounting Analytics.jpg&quot; alt=&quot;Accounting Analytics&quot; /&gt;
But what exactly is ‘accounting analytics’?&lt;/p&gt;

&lt;p&gt;So, in layman’s language, Accounting analytics is the examination of big data using data science or data analytics tools to help answer accounting-related questions. We live in a world which is full of huge piles of online data. The Accountants, now a days, are required to have the skill of analyzing this huge data in the form of financial statements of a company. That is why, it is very important to be familiar with the concept of accounting analytics.&lt;/p&gt;

&lt;p&gt;Here, we would learn about how data science is practically applied in the field of accounting analytics.There are various functions which the accountants are now a days using which originate from analytics. For example- budgeting, planning, data management, auditing; these are some fields where accountants are used to dealing with data. In fact, most professionals have already mastered two types of analytics-&lt;/p&gt;

&lt;p&gt;Descriptive Analytics- By summarizing and interpreting raw data, accountants find answers to what has happened. For example, we see that usually, analysts use sums, averages, and percent changes to calculate sales results, inventory stock, cost per customer, average dollars spent, year-over-year change in sales, etc.&lt;/p&gt;

&lt;p&gt;Diagnostic Analytics- Accountants often deploy data analytics and data mining to discover why something happened. They also create variance reports to show differences between budgeted amounts and actual income or expenses, employ tools and software to look for patterns and problems in large data sets.&lt;/p&gt;

&lt;p&gt;These skills are great, but they’re not enough. As the automation takes over day-to-day tasks, accountants are increasingly being asked to act as data scientists. Accountants can use various tools of analytics such as- Excel, Tableau, Python, SQL, etc&lt;/p&gt;

&lt;p&gt;Let us now examine how 3 different types of accounting studies which may use analytics to modify their current work through examples.&lt;/p&gt;

&lt;p&gt;Managerial Accounting
Financial Accounting
Auditing
In this blog, we’re going to talk about three major aspects of accounting Analytics with respect to each type of accounting study mentioned above-&lt;/p&gt;

&lt;p&gt;1) Variance Analysis in Management Accounting&lt;/p&gt;

&lt;p&gt;2) The concept of SVA in Financial Accounting&lt;/p&gt;

&lt;p&gt;3) Trend Analysis in Auditing&lt;/p&gt;

&lt;p&gt;1) Variance Analysis in Managerial Accounting
&lt;img src=&quot;/blog/01.png&quot; alt=&quot;Accounting Analytics&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Managerial Accounting is a domain in which accountants prepare and use data for internal decision making process. We’d like to introduce you to a couple of ways in which advanced data analytics skills will help you make better internal decisions.&lt;/p&gt;

&lt;p&gt;Case study of the usage of analytics in Dominos&lt;/p&gt;

&lt;p&gt;Dominos India is a popular pizza restaurant chain in India. So, Dominos India implements standards to keep the costs of cheese in control with respect to every normal pizza size. They also measure how much cheese they start a day with, how much is prepared, and how much they end with so that they can see how much of it was used.&lt;/p&gt;

&lt;p&gt;When a customer buys a pizza, say a medium size, the employees at Dominos are supposed to use a standard amount of cheese.&lt;/p&gt;

&lt;p&gt;•If the employee doesn’t put enough cheese in the pizza, then customers may be dissatisfied.&lt;/p&gt;

&lt;p&gt;•On the other hand, if employees put too much cheese on the pizza, then the profit margin decreases and Dominos’ shareholders are dissatisfied.&lt;/p&gt;

&lt;p&gt;The company has to minimize the un-evenness.&lt;/p&gt;

&lt;p&gt;Solution&lt;/p&gt;

&lt;p&gt;Combining the point of sale data with inventory data and data about standard quantities of cheese can calculate variances to help identify if the employees are using the right amount of cheese.&lt;/p&gt;

&lt;p&gt;Variance information can be criticized for being too old to make a difference. However, if one knows how to systematically fetch data at regular intervals and combine data, perhaps from, say, a point of sale system, spreadsheets, and QuickBooks, then one can help set up a process to calculate and communicate variances on a daily basis.&lt;/p&gt;

&lt;p&gt;This is just the start. If you can gather the names of the employees who are responsible for serving the pizza, then you can also quantify the extent to which each employee is serving the right amount of cheese, helping managers to identify employees who may need additional training. All of this can be done with the help of Excel.&lt;/p&gt;

&lt;p&gt;2) The Concept of SVA in Financial Accounting
&lt;img src=&quot;/blog/02.png&quot; alt=&quot;Accounting Analytics&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Financial accounting is a domain of accounting that focuses on summarizing accounting transactions during a period of time for users external to an organization like investors, creditors, and regulators.&lt;/p&gt;

&lt;p&gt;Shareholder Value Analytics&lt;/p&gt;

&lt;p&gt;Shareholder value analysis (SVA) is one of several non-traditional metrics being used in business today. SVA determines the financial value of a company by looking at the returns it gives its stockholders and is based on the view that the objective of company directors is to maximize the wealth of company stockholders.&lt;/p&gt;

&lt;p&gt;How is shareholder value calculated?&lt;/p&gt;

&lt;p&gt;Shareholder value is calculated by dividing the estimated total net value of a company based on its present and future cash flows by the value of its shares of stock. The resulting figure indicates the company’s value to stockholders.&lt;/p&gt;

&lt;p&gt;The Formula for Shareholder Value Added Is-&lt;/p&gt;

&lt;p&gt;SVA=NOPAT−CC&lt;/p&gt;

&lt;p&gt;Where-&lt;/p&gt;

&lt;p&gt;NOPAT=Net operating profit after tax&lt;/p&gt;

&lt;p&gt;CC=Cost of capital​&lt;/p&gt;

&lt;p&gt;Why do the companies adopt SVA?&lt;/p&gt;

&lt;p&gt;The principle of shareholder value is that a company adds value for its stockholders only when equity returns exceed equity costs. Once the amount of value is calculated, targets for improvement can be set and shareholder value can be used as a measure for managing performance.&lt;/p&gt;

&lt;p&gt;Some of the value investors use SVA as a tool to judge the corporation’s profitability and management efficacy. This line of thinking often runs congruent with value-based management, which assumes that the foremost consideration of a corporation should be to maximize economic value for its shareholders.&lt;/p&gt;

&lt;p&gt;Shareholder value is created when a company’s profits exceed its costs. But there is more than one way to calculate this.&lt;/p&gt;

&lt;p&gt;3) Trend Analysis in Auditing
&lt;img src=&quot;/blog/03.png&quot; alt=&quot;Accounting Analytics&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Auditing is the domain within accounting which focuses on whether the control procedures are being followed and whether the reported financial statements are accurate.&lt;/p&gt;

&lt;p&gt;Case Study for the Application of Trend Analysis&lt;/p&gt;

&lt;p&gt;Comfort Ltd. is a major player in the shoe industry in India and worldwide. We are here to have an audit examining how the number of orders shipped impacts its sales.&lt;/p&gt;

&lt;p&gt;Let’s take a look at the following table showing the sales and number of orders shipped over five periods-&lt;/p&gt;

 	Period 1	Period 2	Period 3	Period 4	Period 5
&lt;p&gt;Sales	750,000	850,000	900,000	1,100,000	1,150,000
Change ($)	 	100,000	50,000	200,000	50,000
Change (%)	 	13%	6%	22%	5%&lt;/p&gt;

&lt;p&gt;Orders Shipped	700	1,200	1,600	2,300	3,000
Change (Count)	 	500	400	700	700
Change (%)	 	71%	33%	44%	30%
We can see that the information in the table shows solid performance over the five periods.  Sales and orders shipped increase during each period.&lt;/p&gt;

&lt;p&gt;So, can the auditors come to the conclusion that the company has faired well in the 5 periods?&lt;/p&gt;

&lt;p&gt;It’s hard to answer without digging a little deeper.  So, let’s take a look at the graph below which shows the sales and orders shipped information-&lt;/p&gt;

&lt;p&gt;The graph clearly shows the same increasing sales and orders shipped. However, upon a closer examination, we see that the sales do not increase at the same rate as orders shipped.  This shows us that the sales per order shipped are deteriorating.&lt;/p&gt;

&lt;p&gt;Does this decrease in sales per order shipped represent an alarming development over the five periods?  Not necessarily.  It can be the company’s intent to increase the orders shipped over time.  However, the company may want to investigate and take measures if the decreasing sales per order shipped numbers are not what the company wants or expects.&lt;/p&gt;

&lt;p&gt;This is just an example of how analytics can be used in auditing. There are many more applications being used in the corporate world.&lt;/p&gt;

&lt;p&gt;So, we hope that you must have understood the concept of accounting analytics and how analytics is widely used in the field of accounting. Hit the like button, if you liked our post; and subscribe our blog to be connected to us, as many more wonderful topics are underway-)&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">But what exactly is ‘accounting analytics’?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Modelling the 4P Mix</title><link href="http://localhost:4000/blog/Modelling-the-4P-Mix/" rel="alternate" type="text/html" title="Modelling the 4P Mix" /><published>2020-05-30T00:00:00+05:30</published><updated>2020-05-30T00:00:00+05:30</updated><id>http://localhost:4000/blog/Modelling-the-4P-Mix</id><content type="html" xml:base="http://localhost:4000/blog/Modelling-the-4P-Mix/">&lt;p&gt;&lt;img src=&quot;/blog/Modelling-the-4P-Mix/Modelling the 4P Mix.png&quot; alt=&quot;Modelling the 4P mix&quot; /&gt;
Marketing Mix Modelling is one of the best methods available to value and analyze various marketing tactics on sales and/or market share, and then forecast the future impact of the similar activities. This basically means quantifying the information available with the firm to analyze their impact on sales and market share.&lt;/p&gt;

&lt;p&gt;It uses both the internal information (Pricing, Quality Metrics, Inventory Levels, Spending on Print Media) as well as the external information (Macroeconomic Situation, Seasonality and Competitors). The key objective is to maximize the return on investment while minimizing the budget required for the same.&lt;/p&gt;

&lt;p&gt;Digital Transformation of various business processes have helped the collection of better data for modelling purposes. This is because in order to create an effective model, dynamic metrics, such as social media and customer engagement metrics, help. GRP (gross rating point) in TVs and Click-through rates in Online Advertising are some of the popular metrics often used to understand the impact of different promotion methods.&lt;/p&gt;

&lt;p&gt;We will be illustrating two important concepts of Marketing Mix Modelling- Linear Regression and Incremental Drivers.&lt;/p&gt;

&lt;p&gt;Using Linear Regression for Marketing Mix Modelling&lt;/p&gt;

&lt;p&gt;Marketing Mix Modelling uses the concept of Multiple Regression. The dependent variable can be Sales while the independent variables commonly used are Price, Advertising spends, Location, product discounts, and so on.&lt;/p&gt;

&lt;p&gt;Before diving into the exacts of Linear Regression, let’s understand an important component of Marketing Mix Modelling- Adstock. Adstock refers to the prolonged effect of advertising on consumers. This tool can help us quantify the impact of advertising.&lt;/p&gt;

&lt;p&gt;The basic Adstock model is as follows&lt;/p&gt;

&lt;p&gt;At = Tt + λAt-1&lt;/p&gt;

&lt;p&gt;Where, At is Adstock at time t, Tt is value of advertising variable at time t, and λ refers to the decay parameter. Thus, when t approaches infinity, the Adstock value approaches 0. We will use Adstock as a component of the Linear Regression equation that we will create.&lt;/p&gt;

&lt;p&gt;We will also include one component for seasonal trend in our equation, as well as one for the number of distribution points. To illustrate, imagine that there is a Clothing store which wants to know the relative importance of parts of its Marketing Mix. The store usually experiences higher sales in Summers than at other times of the year (Seasonal Trend). It has around 100 outlets across India (Number of Distribution Points) and advertises only through Online Media (for simplicity purposes).&lt;/p&gt;

&lt;p&gt;Here is an example of how the regression equation would look like-&lt;/p&gt;

&lt;p&gt;Sales = β0 + β1 (Seasonal Trend) + β2(Number of Distribution Points) + β3(Adstock of at time t)&lt;/p&gt;

&lt;p&gt;The betas generated from Regression analysis will help in quantifying the impact of each of the inputs. Basically, the beta depicts that one unit increase in the input value would increase the sales by Beta times those units, keeping the other inputs constant.&lt;/p&gt;

&lt;p&gt;Many other attributes can be added to this equation; even polynomial features can be used in order to show non-linear changes. Actual models are much more complicated than the one shown above.&lt;/p&gt;

&lt;p&gt;Distinguishing Base Sales from Incremental Sales&lt;/p&gt;

&lt;p&gt;Marketing Mix Modelling breaks down the Business metrics into two major contributors- Base Drivers and Incremental Drivers.&lt;/p&gt;

&lt;p&gt;Incremental Drivers refer to the business results generated by using marketing tactics different from those used in the normal course of business, while Base Drivers refer to the business results generated in the normal course of business, mainly due to the goodwill and brand equity developed over the years.&lt;/p&gt;

&lt;p&gt;Using the same example as before of a clothing store, the base drivers would be that of the number of outlets, its advertisement stock, and the average price level. A temporary reduction in the price would be an example of an incremental driver. The effect of this incremental driver can be separated and analysed to measure its impact.&lt;/p&gt;

&lt;p&gt;There are innumerable other concepts that can be applied to model the 4 Ps of Marketing than Linear Regression and Incremental Drivers, such as Budget Optimization or Deep-dive analysis. Usually these concepts are not used in isolation, but rather a collection of all these concepts is used. Marketing Mix Modelling has a much wider scope and shows us how Analytics can be useful for business.&lt;/p&gt;

&lt;p&gt;A customer’s journey of ‘from thinking to buying’ takes him/her through multiple touch points before deciding the final product to buy. As marketing is becoming more customer centric, the need of identification of right channels to target potential customers has become critical for companies. This helps companies to utilise their marketing funds in a better way and target the right customers in right places. So, we hope that you must have got the gist of the concept. Hit the like button, if you liked our post; and subscribe our blog to be connected to us, as many more wonderful topics are underway-)&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Marketing Mix Modelling is one of the best methods available to value and analyze various marketing tactics on sales and/or market share, and then forecast the future impact of the similar activities. This basically means quantifying the information available with the firm to analyze their impact on sales and market share.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>