<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-05-26T19:36:29+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">The Analytics Bay</title><subtitle>Always Analyzing</subtitle><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><entry><title type="html">MAPPING POPULATION GROWTH WITH LOGISTIC MAP</title><link href="http://localhost:4000/blog/Logistic-Map/" rel="alternate" type="text/html" title="MAPPING POPULATION GROWTH WITH LOGISTIC MAP" /><published>2021-05-26T00:00:00+05:30</published><updated>2021-05-26T00:00:00+05:30</updated><id>http://localhost:4000/blog/Logistic-Map</id><content type="html" xml:base="http://localhost:4000/blog/Logistic-Map/">&lt;p&gt;&lt;img src=&quot;/blog/LogisticMap/TheLogisticMap.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Did not understand?! Don’t worry we are here to make it simple. But let’s dive into some history first.&lt;/p&gt;

&lt;p&gt;Popularized in 1976, by Robert May (a biologist), which was related to the logistic equation written down by Pierre François Verhulst.&lt;/p&gt;

&lt;p&gt;Mathematically, the logistic map is written as:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticMap/Equation.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where xn is a number between zero and one that represents the ratio (percentage) of the existing population to the maximum possible population.&lt;/p&gt;

&lt;p&gt;The values of interest for the parameter r (sometimes also denoted μ) is the growth rate of the population.
Here xn+1 shows the population in the next period (Let’s assume a period to be of a year here).&lt;/p&gt;

&lt;p&gt;To understand its real-life application let’s take the following case:&lt;/p&gt;

&lt;p&gt;CASE&lt;/p&gt;

&lt;p&gt;Analyse the population of a species by using The Logistic Map equation. Remember Darwin’s famous lines “Survival of the fittest” which means that organisms of a species do compete for resources for survival. Remember this is just a theoretical equation. Population sizes in real life scenarios may vary significantly due to changes in many factors and figures.&lt;/p&gt;

&lt;p&gt;Here the objective is to define how ‘The Logistic Equation’ fits into the patterns of population growth-&lt;/p&gt;

&lt;p&gt;ANALYSIS&lt;/p&gt;

&lt;p&gt;Firstly, defining the situation and the variables used&lt;/p&gt;

&lt;p&gt;Population (Xn) is expressed as a percentage of the maximum possible population (which could have exhausted the entire resources available).&lt;/p&gt;

&lt;p&gt;The rate of growth (r) is the growth rate at which Xn percentage of populations is growing (i.e., the percentage of the maximum possible population).&lt;/p&gt;

&lt;p&gt;Before you begin with the analysis, you have to make certain assumptions to understand the nature of the population growth:&lt;/p&gt;

&lt;p&gt;Population is able to develop the resources available to it, which is done by any naturally existing population.&lt;/p&gt;

&lt;p&gt;The population takes into account that some organisms would be younger and others older. (i.e., not all the organism of the population are of the same age).&lt;/p&gt;

&lt;p&gt;Organisms in the population fight for the resources to survive.&lt;/p&gt;

&lt;p&gt;Now what you have here is a python code you can use to draw the graphs that are going to be referred to below.&lt;/p&gt;

&lt;p&gt;{CODE STARTS HERE}&lt;/p&gt;

&lt;p&gt;import matplotlib.pyplot as plt&lt;/p&gt;

&lt;p&gt;#Enter the GROWTH_RATE(R), INITAL_POPULATION(Xn), and NO. OF YEARS YOU WANT TO SHOW THE PROJECTION FOR&lt;/p&gt;

&lt;p&gt;growth_rate = float(input(“Enter the growth rate for the population: “))&lt;/p&gt;

&lt;p&gt;initial_population = float(input(“Enter the initial population (i.e. population for the first year): “))&lt;/p&gt;

&lt;p&gt;years = int(input(“Enter the no. of years you want to find the projection for: “)) + 1&lt;/p&gt;

&lt;p&gt;def logistic_equation(initial_population, growth_rate):&lt;/p&gt;

&lt;p&gt;population_next_year = growth_rate&lt;em&gt;initial_population&lt;/em&gt;(1-initial_population)&lt;/p&gt;

&lt;p&gt;return population_next_year&lt;/p&gt;

&lt;p&gt;x = [0]&lt;/p&gt;

&lt;p&gt;y = [initial_population]&lt;/p&gt;

&lt;p&gt;for i in range(0, years):&lt;/p&gt;

&lt;p&gt;initial_population = logistic_equation(initial_population,growth_rate)&lt;/p&gt;

&lt;p&gt;x.append(i+1)&lt;/p&gt;

&lt;p&gt;y.append(initial_population)&lt;/p&gt;

&lt;p&gt;plt.plot(x, y, color = ‘green’, linestyle = ‘solid’, marker = ‘o’, markerfacecolor = ‘red’, markersize = 9)&lt;/p&gt;

&lt;p&gt;plt.title(“Population growth at “ + str(growth_rate) + “ Growth Rate”)&lt;/p&gt;

&lt;p&gt;plt.xlabel(“Year no.”)&lt;/p&gt;

&lt;p&gt;plt.ylabel(“Population (as a percentage of max population)”)&lt;/p&gt;

&lt;p&gt;plt.show()
{CODE ENDS HERE}&lt;/p&gt;

&lt;p&gt;In the graphs you have:&lt;/p&gt;

&lt;p&gt;At
r = 0.49 and Xn = 0.4&lt;/p&gt;

&lt;p&gt;The following Is the graph for population projection over 50 years:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticMap/Figure_1_Population_GrowthRate.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here you can see that after a certain no. of years it becomes 0.&lt;/p&gt;

&lt;p&gt;Next let’s take&lt;/p&gt;

&lt;p&gt;r = 0.99 and Xn = 0.4&lt;/p&gt;

&lt;p&gt;A projection for 50 years reveals such an image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticMap/Figure_2_Population_GrowthRate.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here also, the population becomes extinct beyond 50 years of growth.&lt;/p&gt;

&lt;p&gt;Now following are the graphs for:&lt;/p&gt;

&lt;p&gt;r = 1.49 and Xn = 0.4
&lt;img src=&quot;/blog/LogisticMap/Figure_3_Population_GrowthRate.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;r = 2.37 and Xn = 0.4&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticMap/Figure_4_Population_GrowthRate.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;r = 3 and Xn = 0.4&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticMap/Figure_5_Population_GrowthRate.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;r = 3.49 and Xn = 0.4&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticMap/Figure_6_Population_GrowthRate.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What you observe in graph 1, 2, 3 is that the growth rate of less than 1.6(BAR) resulted in a continuous fall in population ultimately leading to its extinction in some future years.&lt;/p&gt;

&lt;p&gt;In graph no. 2, you observe that there was some chaotic movement in the initial years but in the later years the population attained equilibrium resting itself at 0.575 approximately.&lt;/p&gt;

&lt;p&gt;In the graph 5 &amp;amp; 6, you can see a cyclic movement as population goes through a cycle of 2 every consecutive year and a cycle of 4 every consecutive year respectively.&lt;/p&gt;

&lt;p&gt;What one can analyse from this data is how well this simple mathematical operation exhibits the working of a real-life population as follows:
When the growth rate was low at a population of 40% (of what can be the maximum population) the population became 0 by the 10th year.&lt;/p&gt;

&lt;p&gt;This explains how a small population with a low growth rate can become extinct over the years as they will not perish. New ones will not be born and old ones will die. This leads to continuous deduction in adults who can reproduce. Leading to the extinction of the species.&lt;/p&gt;

&lt;p&gt;When the growth rate reached 0.99 the fall in population was gradual but in growth rate 1.49 the fall was again steep. Does this defy the 1st analysis?&lt;/p&gt;

&lt;p&gt;The answer is No.&lt;/p&gt;

&lt;p&gt;What happens here is as the small population grows at a rate of 0.99 it can sustain itself since the population can be fed considerably, reproduce and still survive for a countable 50+ years but ultimately become extinct in a certain year.&lt;/p&gt;

&lt;p&gt;However, for the growth rate of 1.49 the fall is steep as the increased population creates competition for resources. This is not good as this leads to the fight for survival, which leads to premature deaths. Even if you keep into account the increased no. of the population as a resource still many will not be able to become of any use and be reduced for the lack of resources.&lt;/p&gt;

&lt;p&gt;When the growth rate reached 2.37 it was the right growth rate as the population was able to sustain itself. The growth rate was just right for the population to be fed, taught/learn(Itself in case of unicellular and mute species), to reproduce future progenies. Ultimately they reached an equilibrium population that can thrive forever if the growth rate is stable (which does not happen in normal cases).&lt;/p&gt;

&lt;p&gt;Surprisingly when the growth rate reaches 3 you can see cyclic population growth. One year it increases and the other it decreases. This is because there are constant cases of lack of resources which leads to a decline in population in one year, but as soon as the population declines it again grows back the next year only to decline the other and repeat the cycle.&lt;/p&gt;

&lt;p&gt;If you go on increasing the rate the cycle will disperse its repetition intervals. Now instead of 2-year, you observe a 4-year cycle. The reason again being the same scarcity of resources in one year and abundance in the other.&lt;/p&gt;

&lt;p&gt;When you increase growth rate beyond 4 it rises suddenly and then falls to almost a nil compared to the previous year, then rising and falling erratically, finally going to be 0 at some future year, which again comes somewhere after 10 years or so as seen in CASE 1.&lt;/p&gt;

&lt;p&gt;(In this case, you can see a negative population which can be neglected as the population can never be negative.)&lt;/p&gt;

&lt;p&gt;For low values of R you see the populations always go extinct so the equilibrium value is zero, but once R hits 1 the population stabilizes on to a constant value and the higher R is the higher the equilibrium population, but once R passes three the graph splits in two.&lt;/p&gt;

&lt;p&gt;The equation never settles on to a single constant value instead it oscillates back and forth between two values one year the population is higher the next year lower and then the cycle repeats the cyclic nature of populations is observed in nature to one year there might be more rabbits and then fewer the next year and more again the year after.&lt;/p&gt;

&lt;p&gt;This was a simple equation starting with only 2 variables which can help us understand the nature of population growth. Now comes the reason, where can it be applied.&lt;/p&gt;

&lt;p&gt;By now you should have had a fair idea of it, some places where its expected to return good returns would be:
Understanding sustainable population growth rates for a current population.&lt;/p&gt;

&lt;p&gt;How to track population growth of a certain organism (for eg. Bacterial growth) and ways to eradicate or sustain it (either through low growth rates or by very high growth rates.)&lt;/p&gt;

&lt;p&gt;Creating equilibrium populations for different species in an environment and understanding how long the different species’ populations will survive.&lt;/p&gt;

&lt;p&gt;(Just a hypothetical example) It can be used to eradicate the zombie population if there is a probable outbreak in the future.
These were some of the applications on biology. It can be applied to other fields also as it has been able to define:&lt;/p&gt;

&lt;p&gt;The rhythmic pattern of a dripping faucet&lt;/p&gt;

&lt;p&gt;Thermal convection in a fluid
And, The firing of neurons in your brain&lt;/p&gt;

&lt;p&gt;So, fire your neurons right away and figure out how you can use this equation to predict chaos in population growth rate or any other growth rate in your field interest.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Monty Hall Problem</title><link href="http://localhost:4000/blog/Monty-Hall/" rel="alternate" type="text/html" title="The Monty Hall Problem" /><published>2021-05-19T00:00:00+05:30</published><updated>2021-05-19T00:00:00+05:30</updated><id>http://localhost:4000/blog/Monty-Hall</id><content type="html" xml:base="http://localhost:4000/blog/Monty-Hall/">&lt;p&gt;&lt;img src=&quot;/blog/B.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To provide a prior context, this game was played in a 1960’s Canadian show called ‘Let’s make a deal, hosted by a person called Monty Hall. The show contestants had the opportunity of winning a huge prize only based on a simple probability-based choice (and some luck). We will be explaining the same in detail in this blog. Treat this as a game if you haven’t read about this before.&lt;/p&gt;

&lt;p&gt;Let’s say we have 3 inverted cups in front of you, namely A, B, and C. And one of them has a piece of diamond underneath it, and the other two don’t.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Monty.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since we are your host, we know which one of the cups has a reward you would want to grab. And we offer you to tap on the cup you think possesses the diamond. Let’s say you choose cup A. So far so simple, a one-third probability for the reward.&lt;/p&gt;

&lt;p&gt;Now here’s a twist you’d like. Out of the remaining 2 cups B and C, we lift cup C and you find there’s nothing below it. we give you another opportunity, either switch to cup B or stay at your original choice, i.e., cup A. Take a minute to think and make a decision before you read further.&lt;/p&gt;

&lt;p&gt;Before revealing the answer, we would like to pose some questions that might have popped up in your head. Does it really matter if you switch or stay? It’s 50-50, isn’t it? One cup gives you a diamond the other one doesn’t, that’s just a simple chance.&lt;/p&gt;

&lt;p&gt;How about we tell you that there’s a definite strategy to this paradox that increases your probability of winning? Switch. Switch every time.&lt;/p&gt;

&lt;p&gt;To put things in perspective, initially, when we gave you this choice, you had a one-third chance of winning with cup A and a two-third chance of winning with cup B and cup C put together. But when we flicked open cup C, suddenly with cup A, you still had a one-third chance of winning. However, with the unopened cup B, your probability of winning the diamond suddenly shot up to two-thirds. Voila! How did this happen? Let’s boil it down a bit further.&lt;/p&gt;

&lt;p&gt;To simplify this problem, let’s consider that all of us know which cup has the diamond underneath it, say cup B. We will now encompass all different ways a person can operate with the “Switch Every Time Strategy”. Now if we ask a person to choose one of the 3 cups, he has 3 options to do so.&lt;/p&gt;

&lt;p&gt;Choose cup A. Now we as the host would naturally pick open cup C because obviously, that’s the empty one. Now we ask the person again, “Would you like to switch or stay?” With the switch option, the player wins the diamond with cup B. Keep in mind here that the player was initially wrong with the choice of his cup.&lt;/p&gt;

&lt;p&gt;Choose cup B. Now here it does not matter which cup we open because both A and C are empty, hence we flick open cup C (the outcome would have no difference even if you open cup A). If the player switches and chooses cup A, the player returns home empty-handed although he was initially correct.&lt;/p&gt;

&lt;p&gt;Choose cup C. This is technically the same as the Choose Cup A strategy. We as the host turn open cup A. With the switch strategy, the player again chooses cup B and wins. Note that, even in this scenario, the player was initially wrong.&lt;/p&gt;

&lt;p&gt;To sum up the narrative, with the “Switch Strategy”, you win 2 out of 3 times. However, scroll up again to note that if you would have relied on the “Stay Strategy” (i.e. staying with your original choice), you would have won only 1 of 3 times.&lt;/p&gt;

&lt;p&gt;Clearly, not a 50-50 chance.&lt;/p&gt;

&lt;p&gt;If you still haven’t understood the logic behind this, don’t worry, we will be taking another shot to help you comprehend this by modifying the game a little. Instead of having 3 cups, this time we offer you 100 cups, but only one of them has the diamond. Seems a relatively tough choice, but you are offered to guess which one of these 100 cups behold the precious item. With a poor chance of 1% of winning, supposedly you choose cup number 100.&lt;/p&gt;

&lt;p&gt;This time we blow away 98 other cups which don’t possess the reward, leaving cup number 50. And now you could feel what’s happening. In the beginning, you had the mere probability of 0.01 to win. With the switch strategy, the probability of winning is an astounding 99% or 0.99 when all it seems to the naked eye is the mere option of choosing 1 out of 2.&lt;/p&gt;

&lt;p&gt;What’s even more fascinating is that every time the player was wrong in his/her initial choice, the player would always bag the prize by switching. And when the wind of luck is blowing against you, you might lose with switching, i.e. choosing the correct option initially. Of course, the strategy doesn’t guarantee a win, but when observed with the large sample size, you would certainly notice that the odds shoot up instantly.&lt;/p&gt;

&lt;p&gt;So, in case you are offered a choice like this in the future, don’t forget to switch and pray to God that you make the wrong choice initially.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">BACK TESTING OF TRADE STRATEGIES</title><link href="http://localhost:4000/blog/Backtesting/" rel="alternate" type="text/html" title="BACK TESTING OF TRADE STRATEGIES" /><published>2021-05-13T00:00:00+05:30</published><updated>2021-05-13T00:00:00+05:30</updated><id>http://localhost:4000/blog/Backtesting</id><content type="html" xml:base="http://localhost:4000/blog/Backtesting/">&lt;p&gt;&lt;img src=&quot;/blog/Backtesting/1.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The most exciting thing stock markets teach is the art of proper application of knowledge, patience, and perseverance.
There are majorly 3 types of investors in the market:&lt;/p&gt;

&lt;p&gt;• Conservatives: Averse to market volatility
• Balanced: Can accept moderate volatility
• Aggressive: Willing to accept consequences of high volatility. Invests in high-risk and leveraged instruments for high returns.
These investors have their trading strategies majorly based on 2 types of analysis:
• Fundamental Analysis: Analysing the annual reports of a stock company, its performance and analysing the proper future based on in-depth analysis
• Technical Analysis: Analysing chart patterns, finding out the strong demand and supply zones, figuring out the shapes of hammers (shooting star, hammer, bullish engulfing, bearish engulfing, the indecisive Doji, the maribuzo patterns, etc).
Constructing a profitable and everlasting strategy&lt;/p&gt;

&lt;p&gt;The strategies need to be very accurate, to an extent that its probability of success is much more as compared to its loss. The decisions in the stock market have to be very quick and should have a high probability to result in success (as nothing about the future can be predicted very accurately, the high probability trades need to be identified and the low probability trades are to be rejected).&lt;/p&gt;

&lt;p&gt;This synergy of taking and rejecting a particular trade decision can be optimally done when the algorithm is used by the people (which include retail investors, institutional investors, etc). This testing of the trade strategies can be done through a process called BACKTESTING OF TRADE STRATEGIES.&lt;/p&gt;

&lt;p&gt;What does backtesting mean?&lt;/p&gt;

&lt;p&gt;In simple words, backtesting a trading strategy is the technique of testing a trading hypothesis/strategy on prior timeframes, instead of applying a strategy for the period forward (to judge performance), which could take years, a trader can simulate his or her trading strategy on relevant past data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Backtesting/2.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For example, say, a trader wants to test a strategy based on the notion that Nifty Bank will outperform the overall market. But if you tested it during the US Financial Crisis, this strategy would not work properly. So, this thing should be kept in mind that backtesting does not necessarily guarantee higher or better returns.&lt;/p&gt;

&lt;p&gt;Why do investors backtest their strategies?&lt;/p&gt;

&lt;p&gt;Backtesting assesses the viability of a trading strategy by discovering how it would play out using historical data. If backtesting works, traders and analysts may have the confidence to employ it going forward. Another reason for traders to backtest is that they get a clear picture of how the market worked in the past. The trends by the charts show, not a complete, but an elaborate movement of a particular stock, commodity, indexes, currencies, and futures, and options of the same.&lt;/p&gt;

&lt;p&gt;The only assumption taken for backtesting is:
Any strategy that worked well in the past is likely to work the same way in the future, and conversely, any strategy that performed poorly in the past is likely to perform badly in the future.&lt;/p&gt;

&lt;p&gt;Rules to backtest a strategy&lt;/p&gt;

&lt;p&gt;• A broad market trend should be taken which includes different market conditions.
• Backtesting in a particular sector company would be helpful in the same sector company majorly. As a general rule, if a strategy is targeted toward a particular type of stock, limit the testing to that genre.
• Exposure is an important aspect. (It is the amount invested in the market). Increased exposure leads to high risks (which further leads to higher profits or losses), whereas lower exposure leads to lower risk (which further leads to lesser profits or losses).
• Volatility measures are extremely significant and hold immense value. Traders should seek to keep volatility low to reduce risk and enable easier entry and exit points in the strategy.
• Selecting the time intervals. For a long position, more period for backtesting, and a short buy position, less period for backtesting.
Procedure for backtesting&lt;/p&gt;

&lt;p&gt;1.Have a trading plan – A proper and systematic trading strategy need to be constructed through proper analysis before starting to backtest it.&lt;/p&gt;

&lt;p&gt;Some important aspects to build a trading strategy are:&lt;/p&gt;

&lt;p&gt;• Where should you trade?
• According to your analysis the trend, the particular market follows?
• Entering a buy or a short sell position at different market conditions?
• Stop-loss is a particular trade?
• Targets needed to be achieved?
• How to exit the winning trades?&lt;/p&gt;

&lt;p&gt;After answering the questions, a brief algorithm or a strategy can be made so that the strategy can be further used for testing.&lt;/p&gt;

&lt;p&gt;2.Heading to previous time frames to understand if the trading algorithm built would have been successful at that moment in the past.&lt;/p&gt;

&lt;p&gt;3.Check the results you receive after applying your strategy an appropriate number of times. Say 100 times for each stock, you are willing to trade-in.&lt;/p&gt;

&lt;p&gt;4.Leaving the personal bias aside, record the number of times your algorithm worked accurately and the number of times, it gave the wrong outcomes than what was accepted.&lt;/p&gt;

&lt;p&gt;AN EXAMPLE&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Backtesting/3.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let a backtesting strategy be:&lt;/p&gt;

&lt;p&gt;CONDITION 1 – Buy if the lowest point of the next candle is above the highest point of the previous candle.
CONDITION 2 – When the RSI is at 80, then sell, if it is at 20, then buy. (RSI being from 20 to 80). Keeping the stop loss at 20 and 80 respectively.&lt;/p&gt;

&lt;p&gt;Try using this strategy on historical data. And then work out the probability of accuracy of this backtesting strategy.
Every strategy cannot be the best one. One out of many can be approved to be the optimum strategy.&lt;/p&gt;

&lt;p&gt;Common Backtesting measures&lt;/p&gt;

&lt;p&gt;• Net profit/Loss
• Return: the total return of the portfolio in a specific time frame
• Risk-adjusted return: The return of portfolio adjusted for a level of risk
• Market exposure: Degree of exposure to different segments of the market
• Volatility: The dispersion of returns on the portfolio&lt;/p&gt;

&lt;p&gt;Identifying the right one&lt;/p&gt;

&lt;p&gt;The future of the trend cannot be predicted, so this concept works on probability. If the probability is greater than half or more precisely and to be on a safer side, is greater than 65%, then the strategy is good to be applied in the live markets.
Points to be kept in mind:&lt;/p&gt;

&lt;p&gt;• Using proper common sense informing the strategy- E.g. The hypothesis of a trend of Bank Nifty should not be affected by the increase in the price of Apples, as this correlation does not make sense.
• Using Blind data – The data should not be chosen which matches the requirements of your strategy. The data should not have a biased movement. This is a mistake which is usually encountered by any trader, unconsciously.
• Continue backtesting – Now and then, new demand zones, supply zones, and breakouts can be identified, so backtesting should be continued so that the strategy does not become outdated.
• Identify key metrics, indicators, and results before your test- It is recommended to use several different indicators and metrics as well as using multiple data sets whenever possible. This will improve the accuracy of your results.
• Be ready to change your strategy – The conditions are highly volatile. It may happen that the strategy may have worked in the past, but it won’t work in the live markets due to a certain event or sudden breakouts in the zones may force the investors to change strategies as the markets change their trends.&lt;/p&gt;

&lt;p&gt;Does backtesting help in the markets?&lt;/p&gt;

&lt;p&gt;Backtesting is no doubt, a very important aspect in building trading strategies and testing them through historical trends and data. But backtesting has its limitations too. They are:&lt;/p&gt;

&lt;p&gt;• Market conditions constantly change. Factors that have affected the market in the past may not affect the market in the present day or the future.
• New conditions such as volume, interest rate, and volatility may affect the market differently.&lt;/p&gt;

&lt;p&gt;Solution&lt;/p&gt;

&lt;p&gt;An integral way to use backtesting, to reduce the adverse effect of the limitations&lt;/p&gt;

&lt;p&gt;The best way to get started on trading a new strategy is to keep the leverage minimum and the possible losses under control. After all, losing is inevitable in trading, and losing an affordable amount of money in testing a new strategy may be the best way to go as it can be a vital part of the learning and tweaking process.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Birthday Problem</title><link href="http://localhost:4000/blog/Birthday-Problem/" rel="alternate" type="text/html" title="The Birthday Problem" /><published>2021-05-05T00:00:00+05:30</published><updated>2021-05-05T00:00:00+05:30</updated><id>http://localhost:4000/blog/Birthday-Problem</id><content type="html" xml:base="http://localhost:4000/blog/Birthday-Problem/">&lt;p&gt;&lt;img src=&quot;/blog/BirthdayProblem/BirthdayProblem%20(1).png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Alternatively, how many people must be there in a room such that the probability of two people sharing the same birthday is pretty high, 50% to 90%? Quite intriguing questions, right? Think about the problem before reading further on. Assume that there are no twins in the room, the no. of days in the year is 365 (sorry people who have birthdays on 29th Feb) and the distribution of birthdays throughout the year is uniform (Actually it’s the worst case, the results will be even better in real-world scenarios because the birthdays are more closely distributed in the real-world than the ideal world of uniform distribution).&lt;/p&gt;

&lt;p&gt;The Math&lt;/p&gt;

&lt;p&gt;Let’s turn to math for an answer because our intuition can be wrong (wasn’t yours?) but math can’t be wrong. We as humans are a bit selfish (weren’t you thinking about your birthday matching with someone else, rather than any two people having the same birthday). Combinatorics tells us that there are nC2 pairs possible when we take any 2 people from the total of n people in the room.&lt;/p&gt;

&lt;p&gt;For these calculations, we’ll make a few assumptions. First, we’ll disregard leap years. It simplifies the math without having too great an effect on the results. Second, we assume that birthdays are uniformly distributed throughout the year and have an equal probability of occurring (However, that is not usually the case. Studies have shown that more people are born in the first half of the year in India than the second, especially from April to June). Taking uniform distribution gives a truer mathematical approximation as the probability of two people sharing the same birthday is the least in case of uniform distribution.&lt;/p&gt;

&lt;p&gt;We’ll start with one person, and then add people in one at a time to illustrate how the calculations work. It is simpler to find the probability that no one shares a birthday. We’ll then take that probability and subtract it from one to derive the probability that at least two people share a birthday because these two are complements of each other.&lt;/p&gt;

&lt;p&gt;Using, P(A) = 1 - P(A’),&lt;/p&gt;

&lt;p&gt;Probability of at least one same birthday = 1 – Probability of no same birthday&lt;/p&gt;

&lt;p&gt;For the first person, there are no birthdays already reserved, which means that there is a 365/365 chance that there is not a shared birthday. That makes sense since we have only one person.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/BirthdayProblem/BirthdayProblem%20(2).PNG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Adding a second person to the mix gives us an interesting result. The first person covers one possible birthday, so the second person has a 364/365 chance of not sharing the same day. To find the probability of no match, we’ll multiply the probabilities of the first two people and subtract from one to calculate the probability of them sharing the same birthday.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/BirthdayProblem/BirthdayProblem%20(3).PNG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When the third person comes, the previous two people already cover two dates. So, the third person has a probability of 363/365 for not sharing a birthday.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/BirthdayProblem/BirthdayProblem%20(4).PNG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The pattern for how to calculate the probability for a given number of people must be quite visible now, though no worries if you couldn’t figure it out. Here’s the general form of the equation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/BirthdayProblem/BirthdayProblem%20(5).PNG&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By assessing the probabilities, the solution to the Birthday Problem is that you need a group of 23 people to have a 50.73% chance of people sharing a birthday! Most people expect the group to be considerably larger than that (what was you guess?). The chart also depicts that a group of 57 has a probability of 0.99. It’s practically a guarantee that in a group of 57 people, two will share a birthday.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/BirthdayProblem/BirthdayProblem%20(6).png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(Credit: statisticsbyjim.com)&lt;/p&gt;

&lt;p&gt;Explaining the small size of the group.&lt;/p&gt;

&lt;p&gt;People consider the answer to the Birthday Problem difficult to believe: How can it be so small? However, the answer is correct, proven by probability and statistics. When thinking of the problem, people usually consider themselves. However, it could be any two people in the group. If we pair each person with another, this leads to the formation of a large number of pairs and therefore increases the possibility of the twin-birthday pair. The number of different pairs or unique combinations formed for 23 people (probability of 0.5) is 253.&lt;/p&gt;

&lt;p&gt;=&amp;gt; nC2 = 23C2 = 253&lt;/p&gt;

&lt;p&gt;As seen earlier, each pair’s probability of sharing a birthday is 0.0027, but for 253 pairs that number changes drastically.&lt;/p&gt;

&lt;p&gt;Similarly, when there are 57 people when the probability is ~1, you have 1596 pairs. Now, we would find it absurd if none of the pairs share a birthday.&lt;/p&gt;

&lt;p&gt;=&amp;gt; nC2 = 57C2 = 1596&lt;/p&gt;

&lt;p&gt;Conclusion&lt;/p&gt;

&lt;p&gt;We conclude that intuition doesn’t work in such problems because we humans are pretty bad at visualizing nonlinear functions and grossly underestimate the number of pairs formed from n number of people. Mathematics and statistics hence come to our rescue and prove to be pretty useful in such situations. So next time whenever you are in a room with 50 people make sure to find your birthday match.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Behavioral Analytics</title><link href="http://localhost:4000/blog/Behavioral-Analytics/" rel="alternate" type="text/html" title="Behavioral Analytics" /><published>2021-04-21T00:00:00+05:30</published><updated>2021-04-21T00:00:00+05:30</updated><id>http://localhost:4000/blog/Behavioral-Analytics</id><content type="html" xml:base="http://localhost:4000/blog/Behavioral-Analytics/">&lt;p&gt;&lt;img src=&quot;/blog/BehavioralAnalytics/feature.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The number of clicks you made, the amount of time you spent on each item, the type of items you spend your time on, etc. is stored. This massive volume of data is captured and analyzed by companies. You must have noticed, when you spend time on apps like Instagram, if you started to look at a lot of cute dog pictures on Instagram, your feed will automatically start having more and more dog content. This is because Instagram has been capturing your behavior and analyzing it to provide more personalized content for you. This process of analyzing user behavior is called behavioral analytics.
Incorporating behavioral analytics into your operations can be a little intimidating, both in terms of implementation and expense. However, according to a report by McKinsey, organizations that use customer data to produce behavioral insights outperform their peers by 85 percent in sales growth and more than 25 percent in gross margin.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/BehavioralAnalytics/11.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We usually see behavioral analytics being used by companies regularly (but often don’t realize it):&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Netflix:
Netflix provides its recommendations through a complex algorithm that takes into consideration the preferences of the customer who is watching and also the shows which were watched by people with similar preferences:
&lt;img src=&quot;/blog/BehavioralAnalytics/16.png&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Analyzing user behavior like this has helped Netflix increase the lifetime of their customers and helped in making their content more personalized for them. Netflix executives estimated that this analysis saves the company $1 Billion a year.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Amazon:
Amazon gives product recommendations through a complex machine learning algorithm that combines behavioral data such as:
● A user’s purchase history
● Items in their cart
● Items they’ve liked and rated
● What other customers have viewed and purchased
This algorithm is estimated to be responsible for around 35% of Amazon’s total revenue.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/blog/BehavioralAnalytics/13.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What are Behavioral Analysis Tools?&lt;/p&gt;

&lt;p&gt;There are three main behavioral analysis tools involved in building a picture of your customer journey: segmentation analysis, funnel analysis, and cohort analysis.&lt;/p&gt;

&lt;p&gt;SEGMENTATION ANALYSIS&lt;/p&gt;

&lt;p&gt;The study of customers divided into smaller groups to understand specific characteristics such as their behavior, age, income, and personality is known as segmentation analysis. When a company is marketing a smaller segment of consumers, it is easier for them to advertise since each advertisement can be highly tailored and precise to the features of each group.&lt;/p&gt;

&lt;p&gt;FUNNEL ANALYSIS&lt;/p&gt;

&lt;p&gt;Funnel analysis is a method of evaluating the steps taken to achieve a certain outcome on a website, as well as the number of users who complete each step. Funnel analysis helps you spot where users are leaving your website, so you can optimize the problem area and increase conversion rates.
To analyze a funnel, you have to find:
● User Conversion rates
● User Drop-off rates
&lt;img src=&quot;/blog/BehavioralAnalytics/14.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;COHORT ANALYSIS&lt;/p&gt;

&lt;p&gt;Cohort analysis breaks the data into a data set into related groups before analysis. These groups, or cohorts, usually share some common characteristics. Cohort analysis allows a company to find patterns throughout the life-cycle of a customer. By seeing these patterns, a company can adapt its service to those specific cohorts.&lt;/p&gt;

&lt;p&gt;The 2 most common cohort types are:&lt;/p&gt;

&lt;p&gt;● Acquisition Cohorts: divides users by when they signed up first for your product. For app users, you might break down cohorts by the day, the week, or the month they launched an app, and track daily weekly, or monthly cohorts.&lt;/p&gt;

&lt;p&gt;● Behavioral Cohorts: divides users by their behavior in your app within a given period. These could be any number of actions that a user can perform – App Install, App Uninstall, Transaction, or any combination of these events.&lt;/p&gt;

&lt;p&gt;Benefits of Behavioral Analytics&lt;/p&gt;

&lt;p&gt;Behavioral analytics is critical for increasing conversion, commitment, and retention at a company. Every member of a team should be able to obtain the actionable knowledge they need to answer their questions and exploit data in ways that didn’t seem possible before with the right behavioral analytics tool.&lt;/p&gt;

&lt;p&gt;According to the results, a large percentage of users use a particular e-commerce platform after searching “Thai food” on Google. Most of the users spent time on the homepage and went to the “Asian Food” tab and end up buying nothing. Examining each of these incidents as a single data point fails to shows what is going in consumers mind and not able to analyze why consumer isn’t buying the product&lt;/p&gt;

&lt;p&gt;Both web traffic and page views are viewed as a timeline of related events that did not result in orders in behavioral analytics. Since the majority of users left after seeing the “Asian Food” page, there might be a discrepancy between what they’re looking for on Google and what the “Asian Food” page reveals. Knowing this, a glance at the “Asian Food” page shows that Thai food is not prominently displayed, leading people to assume it is not available, even though it is.&lt;/p&gt;

&lt;p&gt;Let’s look at a few examples of how behavioral analytics may be used:
A travel company decides to use its website to monitor consumer events to improve its marketing efforts. A consumer might have looked at visiting a particular destination with a specific airline but abandoned the process before making an order. The business sends an email to the prospect in response to this material. To inspire a booking, the email may contain a travel discount or bid, as well as any valuable information on the destination of interest.&lt;/p&gt;

&lt;p&gt;Another scenario may be that a car dealership sends out an email campaign with a PDF attachment providing information about a variety of vehicles. They can detect user events using behavioral analytics. They will see who opened the attachment, how long they left the PDF open, how much they got into it, and where they spent the most time. The data is then sent to the sales team, which is now in a stronger position to initiate talks with prospects. Prospects become more involved in discussions, and sales representatives may provide more accurate information and tailored deals.&lt;/p&gt;

&lt;p&gt;Here are a couple of the benefits of behavioral data analytics:
● Customized advertising and marketing campaigns
● Heightened customer interaction and subsequent fulfillment
● Better customer relations, and
● Eventually more sales&lt;/p&gt;

&lt;p&gt;To conclude, a company should use behavioral analytics to help understand its target audience’s expectations and preferences. In today’s world, not understanding it means resorting to a scattershot campaign, which does not work.&lt;/p&gt;

&lt;p&gt;Criticism of Behavioral Analytics&lt;/p&gt;

&lt;p&gt;Behavioral analytics raises substantial privacy questions because it necessitates the processing and aggregation of vast volumes of personal data, including extremely confidential data (such as sexual identity or sexual interests, health conditions, and location), which is then exchanged between hundreds of parties interested in targeted ads.
Starting in 2015, Amazon joined Google and other tech giants in launching in-home voice devices that are expected to become a gold mine of behavioral insights for off-line life, just as the activities on their pages are a repository of data for your online life. Some people think this is invasive and unnecessarily informative to data providers and the government, but when they buy something, they are de facto subscribing to the terms.&lt;/p&gt;

&lt;p&gt;Various Products and Websites
&lt;img src=&quot;/blog/BehavioralAnalytics/15.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Amplitude, Indicative, and Mixpanel are common behavioral analytics providers, and each has its own set of advantages and disadvantages.&lt;/p&gt;

&lt;p&gt;If a company is looking for the right behavioral analytics supplier, it’s critical to take your time to thoroughly study each tool. Some companies have a free trial period, which will help you get a clearer understanding of how the tools function and whether the product is right for your market and research needs.
It’s important to check out behavioral intelligence resources that can help you to&lt;/p&gt;

&lt;p&gt;● Optimize behavior through multiple paths and isolate the efficient ones
● Diagnose and remove unwanted steps for customers
● Focus on key behaviors that result in higher total customer value
● Use targeted customer segments (cohorts) to inform and launch campaigns
● Isolate and aim users at risk of churn ahead of time
● Develop at-a-glance dashboards that can be shared with teams and executives&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Quantitative Forecasting</title><link href="http://localhost:4000/blog/Quantitative-Forecasting/" rel="alternate" type="text/html" title="Quantitative Forecasting" /><published>2021-04-14T00:00:00+05:30</published><updated>2021-04-14T00:00:00+05:30</updated><id>http://localhost:4000/blog/Quantitative-Forecasting</id><content type="html" xml:base="http://localhost:4000/blog/Quantitative-Forecasting/">&lt;p&gt;&lt;img src=&quot;/blog/Quantitative_Forecasting/feature.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What is Forecasting?&lt;/p&gt;

&lt;p&gt;Forecasting in analytics carries the same meaning as in the English language i.e., predicting values and outcomes. As the term Quantitative suggests, we rely on mathematical data or numbers to forecast. And exactly how do we do that? What methods do we use? That’s the focus of this blog.&lt;/p&gt;

&lt;p&gt;Before delving any deeper, let’s understand the importance of forecasting and why exactly do we need to understand the methods used for the same. Forecasting is valuable as it gives us the ability to make informed business decisions and develop data-driven strategies.&lt;/p&gt;

&lt;p&gt;Forecasting quantitatively i.e., using data analysis to predict future trends, advances and changes helps us make calculated and prudent decisions. Most financial and operational decisions are made based on current market conditions and predictions on how the future looks, for which we naturally need to forecast. You can’t predict uncertainties, but forecasting helps you to be proactive rather than reactive when faced with dire situations.&lt;/p&gt;

&lt;p&gt;The models we use to forecast outcomes quantitatively can be broadly classified into two categories:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Associative models
Associative models are where we identify and analyze a causal relationship between the given variables. For that we require Pattern. A pattern between the outcome and the factors is what enables us to understand the model and extend it to predict the results for other data values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Time series models
So, what do we mean by a time series? Let’s just break it in simpler terms. It refers to any series that represents data in a chronological order. These models examine the past data patterns and predict the future outcomes based on underlying patterns which we identify in the given data.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ASSOCIATIVE MODELS&lt;/p&gt;

&lt;p&gt;Linear Regression is perhaps the most famous tool used for the same at the basic level. It basically identifies the pattern and states it in the form of a linear equation. Another thing it calculates is the P-Value, which is nothing but the coefficient of correlation between the dependent and independent variables.&lt;/p&gt;

&lt;p&gt;As an example, let’s see if we can use a student’s reading capability to predict his acumen in mathematics. (Refer to the first 150 data entries in the dataset available at https://www.kaggle.com/spscientist/students-performance-in-exams).&lt;/p&gt;

&lt;p&gt;First, we calculate the correlation (Multiple R) between reading score and the maths score, which comes out to be 0.867. This gives us the R-Squared as 0.751.&lt;/p&gt;

&lt;p&gt;What R-Squared tells us is that how much of the variation seen in y-variable can be explained by the given x-variable. In this case, it is 0.751 i.e., around 75% of the variation in maths score can be explained by the variation in Reading score. Now this does not mean that the change is x-variable causes the change in y-variable. This has to do with the fact that correlation does not necessarily mean causality.&lt;/p&gt;

&lt;p&gt;R-Squared as 0.751 is significant, which means that variables are linearly related to each other and the data is a good fit for linear regression. This can be visually seen in the following graph as the data points seem to neatly arrange themselves in a straight line (linear manner).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Quantitative_Forecasting/3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now that we have established that the relationship exists, let’s analyze the linear equation received through the regression analysis. This is the equation of the line of the best fit (see the previous graph):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Quantitative_Forecasting/2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This means that an increase of 10 marks in reading score leads to an increase of around 9 marks in the math score. How do we interpret this? Logically, reading and mathematics are two polar opposite skills with seemingly no connection to each other. One possible explanation is that there might be a third factor (such as Parental Education level) which enables these students to be good at both the skills. This leaves the data open to further analysis.&lt;/p&gt;

&lt;p&gt;One last factor we must take into account is the confidence we should have in the given results. It could be possible that the relationship we observed between the values was merely a coincidence. What is the chance that if we were to take some random values instead of the given dataset, we would get the same (or even stronger) relationship?&lt;/p&gt;

&lt;p&gt;For that we turn to p-value. For the time being, we need not concern ourselves with its calculation. In this case the p-value is 1.42E-46, which is nearly zero. It means, that there is nearly 0% chance that we will be able to replicate the results by taking some random values. So, we can be confident of the relationship established by the regression analysis. Traditionally speaking, a p-value of less than 0.05 is a good indicator that the result established is not just a mere coincidence.&lt;/p&gt;

&lt;p&gt;In Statistical Lingo, we have something called a Null Hypothesis, which generally means that result obtained is not special and is just a fluke. p-value is the evidence in the favour of Null Hypothesis. So higher the p-value, the less evidence we have in favour of the Null Hypothesis, and more confidence we have in the analysis.&lt;/p&gt;

&lt;p&gt;But what if the data arranged itself into a shape other than a straight line? That’s when we turn to other more advanced forms of regressions viz: Polynomial Regression, Logistic Regression, among others. But again, this is a story for another time.&lt;/p&gt;

&lt;p&gt;TIME SERIES MODELS&lt;/p&gt;

&lt;p&gt;Perhaps the most famous example for a data set in this category is the stock price. A good way to predict the future prices is to analyze past trends.&lt;/p&gt;

&lt;p&gt;For that, first, we must account for extreme short-term fluctuations by making the curve smoother to focus on the long-term trends. Moving Average does the same, as can be seen in the following chart.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Quantitative_Forecasting/1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In Moving Average, each data entry is equated to the average of previous N entries, where N is a natural number whose magnitude depends upon the level of smoothness required.&lt;/p&gt;

&lt;p&gt;Once we have the smooth curve, we can then use other more complex and advanced analytic tools to predict the prices. We have various types of time series models and methods to choose from based on the intended future like trend projections, simple mean, and exponential smoothing among many.&lt;/p&gt;

&lt;p&gt;But once again, that is a story for another time…&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Principal Component Analysis</title><link href="http://localhost:4000/blog/Principal-Component-Analysis/" rel="alternate" type="text/html" title="Principal Component Analysis" /><published>2020-12-16T00:00:00+05:30</published><updated>2020-12-16T00:00:00+05:30</updated><id>http://localhost:4000/blog/Principal-Component-Analysis</id><content type="html" xml:base="http://localhost:4000/blog/Principal-Component-Analysis/">&lt;p&gt;&lt;img src=&quot;/blog/pca/1.png&quot; /&gt;
Let’s say we want to predict the impact of COVID-19 on Stock Markets and General Business Activities for different countries. For this we have tons of information like the opening and closing price, lowest and highest price, correlation of different indices over the world, inflation rate, unemployment rate, days of lockdown, cases reported, and so on… This sort of problem has an overwhelming number of variables, and someone who has worked earlier with a lot of variables knows what all obstructions this may present. The question is, “How do we take all of the variables collected and focus on only few of them?” In terms of Analytics, we simply want to “Reduce the Dimension Space.” By Dimensionality Reduction, we have fewer variable relationship to consider which makes data easy to explore and visualize to get the desired outcomes.&lt;/p&gt;

&lt;p&gt;Principal Component Analysis is nothing but a Dimensionality Reduction technique to reduce the feature space of large data-sets with numerous variables by transforming a large data-set into a smaller one but still with maximum information so to make analysing data much easier and quicker for machine learning algorithms and tuning techniques without unrelated variables to process. PCA helps in Noise Filtering, Visualization, Feature Extraction, Stock Market Predictions, Gene Data Analysis and many more. Another fascination property of PCA is that it is both a technique of Feature Extraction as well as a medium to perform Feature Extraction, as it combines the input variables in such a way that it drops the extraneous variables while keeping the more important ones.
Now since What, Why and When parts have been answered, let’s move to the most important question, that is How?&lt;/p&gt;

&lt;p&gt;&lt;b&gt; HOW DOES PCA WORK? &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Let’s look at the step by step how Principal Component Analysis works-&lt;/p&gt;

&lt;p&gt;&lt;b&gt; 1. STANDARDIZATION &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;The purpose of standardising the dataset is to ensure that each of the variable contributes equally to the analysis without any bias. It becomes essential to do standardization before PCA as variables are the core of the PCA and the analysis is pretty much sensitive regarding the variance of initial variables. That is if there are variables with large differences between the initial variables, the variable with larger range will dominate over the smaller ranges.
For example, consider two variables, Distance within city and distance between cities. One of the variables is measured in meters while other in Kilometers. Now if the data is not standardised than distance of 500 meters would be considered as greater than distance of 5KM, which isn’t true. Therefore, transforming the data to comparable scales can prevent this problem. Mathematically, it is done by subtracting the mean and dividing by the standard deviation for each value of each variable.
&lt;i&gt; z = value – mean/(standard deviation) &lt;/i&gt;&lt;/p&gt;

&lt;p&gt;All the variables will be transformed to the same scale once the standardization is completed.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; 2. COVARIENCE MATRIX COMPUTATION &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/pca/2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Often variables are correlated in such a way that they contain redundant information. Thus, to understand the relationship between the variables and identify such correlations, we compute the covariance matrix. The covariance matrix is an n×n symmetric matrix, where n is the number of dimensions. For example, for a 5-dimensional data set with 4 variables a, b, c, d and e, the covariance matrix is a 5×5 matrix of the from-&lt;/p&gt;

&lt;p&gt;Since covariance of a Variable with itself is nothing but its variance [Cov(a,a) = V(a)] therefore the diagonal elements in the covariance matrix are the variance of each variable.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; 3. COMPUTING THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Eigenvectors and eigenvalues are the linear algebra concepts that we are required to process from the covariance matrix to decide the “principal components” of the data. Principal components are the new variables that are constructed as a result of combinations done in a manner that the new components are uncorrelated and most of the information within the initial variables are compressed into the first components. For example, there is 10-dimensional data that provides 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on as shown in the plot below-&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/pca/3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Computing the eigenvectors and eigenvalues to organising the principal components in this manner helps if dimensionalty reduction without losing much information. Statistically, Principal Components represent the direction of data that explains the maximum amount of variance.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; 4. FEATURE VECTOR &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Feature Vector is the first visible step of dimensionality reduction with help of Principal Component Analysis. It is a a matrix of the columns of those eigenvectors that we decide to keep say k, where k &amp;lt; n, that is k is always less than the n, which was the number of initial Eigenvectors. The decision of selection is based on the eigenvalues, i.e. components with lesser significance are discared.&lt;/p&gt;

&lt;p&gt;For Example, suppose we create a feature vector by discarding one of the eigenvector as it was carrying only 2% information and therefore even though dimension now becomes k=n-1, we still have the necessary 98% of information carried by the other eigenvectors.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; 5. LAST STEP - RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;In all the previous steps, excluding Standardisation, we didn’t make any change with the data and just selected the principal components and form the feature vector. The purpose of this step is to use the feature vector and reorient the data from the axis of the original variables (n axis) to the ones represented by the principal components, k axis (hence, the name Principal Components Analysis)&lt;/p&gt;

&lt;p&gt;This can be achieved by multiplying the transpose of the original data set by the transpose of the feature vector which we formed with the help of principal components.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; FinalData = Feature_VectorT * Standardized_Original_Data &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Here’s an example from setosa.io where we transform 5 data points using PCA-&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/pca/4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, we hope that you must have understood the concept of this not so explored technique in data science and statistics as we answered the final question of how PCA works. Hit the like button, if you liked our post; and subscribe our blog to be connected to us, as many more wonderful topics are underway -)&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Let’s say we want to predict the impact of COVID-19 on Stock Markets and General Business Activities for different countries. For this we have tons of information like the opening and closing price, lowest and highest price, correlation of different indices over the world, inflation rate, unemployment rate, days of lockdown, cases reported, and so on… This sort of problem has an overwhelming number of variables, and someone who has worked earlier with a lot of variables knows what all obstructions this may present. The question is, “How do we take all of the variables collected and focus on only few of them?” In terms of Analytics, we simply want to “Reduce the Dimension Space.” By Dimensionality Reduction, we have fewer variable relationship to consider which makes data easy to explore and visualize to get the desired outcomes.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Autocorrelation</title><link href="http://localhost:4000/blog/Autocorrelation/" rel="alternate" type="text/html" title="Autocorrelation" /><published>2020-12-02T00:00:00+05:30</published><updated>2020-12-02T00:00:00+05:30</updated><id>http://localhost:4000/blog/Autocorrelation</id><content type="html" xml:base="http://localhost:4000/blog/Autocorrelation/">&lt;p&gt;Autocorrelation shows the degree of similarity between a given time series and a lagged version of itself over successive time intervals. Lagged version of time series is simply the past period values of the concerned variable. This seemingly simple concept has wide application in fields of mathematics, statistics, and sciences. It is used in measuring optical spectra, short duration light pulses, in GPS system, portfolio and return optimization and much more. Unit root processes, trend-stationary processes, autoregressive processes, and moving average processes are specific forms of processes with autocorrelation.&lt;br /&gt;
The resulting values ranges between -1 to 1, like the traditional correlation statistic. An autocorrelation of +1 represents a perfect positive correlation (an increase seen in one time series leads to a proportionate increase in the other time series). An autocorrelation of negative 1, on the other hand, represents perfect negative correlation (an increase seen in one time series results in a proportionate decrease in the other time series). But it must be remembered as words of caution that autocorrelation measures linear relationships. So even if the autocorrelation is minuscule, there may still be a nonlinear relationship between a time series and a lagged version of itself.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Reasons of autocorrelation &lt;/b&gt;
Some major reasons of autocorrelation include-&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Carryover of effect- Carry over effect of variables in data causes autocorrelation. For ex- Data on monthly expenditure of a family in a particular city, preceding month expenditure will influence the expenditure of next month. This is called carry over effect.&lt;/li&gt;
  &lt;li&gt;Mis-specification of form of relationship in a model also causes autocorrelation. For ex- a study assumes that explanatory and study variables have linear relationship but they have logarithmic or exponential relation.&lt;/li&gt;
  &lt;li&gt;Measurement error- Errors may creep in due to difference between observed and true value of variable, data collection issues etc.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt; Problems caused by autocorrelation &lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Inefficient (but unbiased) ordinary least square estimate which fails to achieve smallest variance and any forecast based on them is hence inefficient. Efficient estimator gives most information about a sample and inefficient estimator may perform well but requires larger sample sizes to do so.&lt;/li&gt;
  &lt;li&gt;Exaggerated goodness of fit for a time series with positive serial correlation and an independent variable that grows over time.&lt;/li&gt;
  &lt;li&gt;Leads to overly optimistic view of R2.&lt;/li&gt;
  &lt;li&gt;Large variance in predictions based on the model and narrow confidence interval. The T statistics and F value are often not reliable.&lt;/li&gt;
  &lt;li&gt;It increases the occurrence of false negative for significant regression coefficient. Regression coefficients appear significant when they are not.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt; Graphical approach through MS-Excel &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Autocorrelation/1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Plotting the data with its lag clearly shows us there exist a relationship between the two, because the plot denotes a nearly definite pattern. This high relation is also denoted with correlation coefficient of 0.83.&lt;/p&gt;

&lt;p&gt;But things are not always this simple. Such high value of correlation of a data with its past values would have made everything so predictable and easy eliminating the need of complex statistical analysis and modeling. Let’s take another example-&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Autocorrelation/2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The plot shows data points paired with their lagged values are scattered without any pattern or direction. This is again confirmed with a low correlation coefficient.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Example 3 &lt;/b&gt;- For finding autocorrelation of a series with different lags on excel we simply create a table with suitable lags and find the correlation using CORREL() function. We can plot the various correlations so obtain to have a better picture of the case.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Autocorrelation/3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Autocorrelation in R &lt;/b&gt;
Finding autocorrelation is not always that simple because we test it with many lags. Repeating the same process with huge data sets becomes a tedious task in MS-Excel. Not to worry, we are again there for your rescue. Let’s try it out on R.
Documentation-
acf(x, lag.max = NULL,
type = c(“correlation”, “covariance”, “partial”),
plot = TRUE, na.action = na.fail, demean = TRUE, …)&lt;/p&gt;

&lt;p&gt;x&amp;lt;- Time series data
lag.max&amp;lt;- Number of lags upta which correlation needs to be calculated
type&amp;lt;- Character string giving the type of acf to be computed. Allowed values are”correlation” (the default), “covariance” or “partial”
plot&amp;lt;- True or False based on whether you want plot or not&lt;/p&gt;

&lt;p&gt;&lt;b&gt; What can you do? &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Removing autocorrelation involves making changing to the data, improvising the model and other tests and modifications. In layman language, for positive serial correlation one can add lags of dependent or independent variable in the model. For negative serial correlation one must check that none of the variables in over-differenced and for seasonal correlation one can add seasonal dummy variable to the model. It itself is a wide concept and we will surely cover it in detail in one of our subsequent blogs.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Example &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Mr. X is a trader. While monitoring stock of ABD ltd he realized that usually whenever stock price rallies on Monday it is followed by decrease in price by the next day and a increase in price on Wednesday. Amused to know this, he tries to prove his observations quantitatively. He takes the daily returns of the share price and run autocorrelation on it with lag 1 to lag 10. He finds that returns one day prior have a negative autocorrelation of -0.74, while the returns two days prior have a positive autocorrelation of +0.83. Past returns seem to influence future returns. Therefore, Mr.X can adjust his position to take advantage of the autocorrelation and resulting momentum by selling the share the next day after a rally and buy them on the following day again. While doing this he never forgets that the chances of error remains high because what was right yesterday may prove wrong tomorrow and he continuously monitors other factors that govern the stock price while repeatedly checking the relevance of auto-correlation at appropriate intervals.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Conclusion &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Autocorrelation is a wide concept. If discovered it can help us to detect patterns in our data, find seasonality and other interesting insights. If ignored it may distort our results. Just a basic awareness about its presence will help us improve our models and analysis. And I hope that we were successful in introducing and explaining the concept to you. Practice the methods we shared, explore new ones, and don’t forget to share your views and findings with us.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Autocorrelation shows the degree of similarity between a given time series and a lagged version of itself over successive time intervals. Lagged version of time series is simply the past period values of the concerned variable. This seemingly simple concept has wide application in fields of mathematics, statistics, and sciences. It is used in measuring optical spectra, short duration light pulses, in GPS system, portfolio and return optimization and much more. Unit root processes, trend-stationary processes, autoregressive processes, and moving average processes are specific forms of processes with autocorrelation. The resulting values ranges between -1 to 1, like the traditional correlation statistic. An autocorrelation of +1 represents a perfect positive correlation (an increase seen in one time series leads to a proportionate increase in the other time series). An autocorrelation of negative 1, on the other hand, represents perfect negative correlation (an increase seen in one time series results in a proportionate decrease in the other time series). But it must be remembered as words of caution that autocorrelation measures linear relationships. So even if the autocorrelation is minuscule, there may still be a nonlinear relationship between a time series and a lagged version of itself.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Random Forest</title><link href="http://localhost:4000/blog/Random-Forest/" rel="alternate" type="text/html" title="Random Forest" /><published>2020-10-27T00:00:00+05:30</published><updated>2020-10-27T00:00:00+05:30</updated><id>http://localhost:4000/blog/Random-Forest</id><content type="html" xml:base="http://localhost:4000/blog/Random-Forest/">&lt;p&gt;&lt;img src=&quot;/blog/RF/rf.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Random Forest is a versatile machine learning method capable of performing both the regression and classification tasks. It is a type of the ensemble learning method, in which a group of weak models combine to form a powerful model.&lt;/p&gt;

&lt;p&gt;It can be properly defined as &lt;b&gt; a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset. &lt;/b&gt;
Instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; The Algorithm of Random Forest &lt;/b&gt;
Random forest is like the bootstrapping algorithm with Decision tree (CART) model. Let’s say, we have 1000 observations in the complete population with 10 variables. Random forest tries to build multiple CART (decision tree) models with different samples and different initial variables. For instance, it would take a random sample of 100 observations and 5 randomly chosen initial variables to build a CART model. It will repeat the process, say 10 times, and then make a final prediction on each observation. The final prediction is the function of each prediction. This final prediction can simply be the mean of each of the predictions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/RF/1_each of the predictions.png&quot; /&gt;
&lt;b&gt; The Working Process of Random Forest can be explained in the below steps- &lt;/b&gt;
Step 1 - Firstly, select random K data points from the training set.
Step 2 - Now, build the decision trees associated with the selected data points (Subsets).
Step 3 - Choose the number N for the decision trees that you want to build.
Step 4 - Repeat Steps 1 &amp;amp; 2.
Step 5 - For new data points, find out the predictions of each decision tree, and assign the new data points to the category that wins the majority votes.
Random forest prediction pseudo-code-
To perform predictions; the trained random forest algorithm uses the below pseudo-code.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Take the test features and use the rules of each randomly created decision tree to predict the outcome and stores the predicted outcome (target)&lt;/li&gt;
  &lt;li&gt;Calculate number of votes for each predicted target.&lt;/li&gt;
  &lt;li&gt;Always consider the high voted predicted target as the final prediction from the random forest algorithm.
Now, we do understand that, some of the words or terms used above must have gone above your head; that’s no problem at all; we will understand the concept with the help of a real life example as well as a case study.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Random Forest Algorithm; A Basic Real Life Example.
Suppose, Saksham wants to go to different places that he may like for his vacation, and he asks his friend for advice. His friend will ask which places he has been to already, and whether he likes the places that he’s visited. Based on Saksham’s answers, his friend starts to give the recommendation. Here, his friend forms the decision tree.
Saksham wants to ask more friends for advice because he thinks only one friend cannot help him make an accurate decision. So, his other friends also ask him some random questions, and finally, provide an answer. He will consider the place with the most votes as his vacation decision.
His friends asked him some questions and gave the recommendation of the best place based on the answers. The friends created the rules based on the answers and used the rules to find the answer that matched the rules. Saksham’s friends also randomly asked him different questions and gave answers, which for they are the votes for the place. At the end, the place with the highest votes is the one he will select to go. This is the typical Random Forest algorithm approach.
Now, we move on to a serious and intuitive case study to actually understand why the concept of Random Forest is so useful, and how it is used practically.&lt;/p&gt;

&lt;p&gt;Case Study on Usage of Random Forest in ML
Following is the distribution of Annual income GINI Coefficients across different countries-&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/RF/2_Gini.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Mexico has a population of 118 Million. Let’s assume that, the algorithm Random forest picks up 10k observations with only one variable (for simplicity) to build each Decision tree (CART model). In total, we are looking at 5 CART models being built with different variables. In a real life problem, you will have more number of population samples and different combinations of input variables.
Salary bands-
Band 1 - Below $40,000
Band 2 - $40,000 – 150,000
Band 3 - More than $150,000
Following are the outputs of the 5 different CART models.
CART (Decision Tree) 1- Variable Age
&lt;img src=&quot;/blog/RF/3_Cart1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CART 2 - Variable Gender
&lt;img src=&quot;/blog/RF/4_Cart2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CART 3 - Variable Education
&lt;img src=&quot;/blog/RF/5_Cart3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CART 4 - Variable Residence
&lt;img src=&quot;/blog/RF/6_Cart4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CART 5 - Variable Industry
&lt;img src=&quot;/blog/RF/7_Cart_5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using these 5 Decision Tree models (CART models), we need to come up with single set of probability to belong to each of the salary classes. For simplicity, we will just take the mean of probabilities in this case study. Other than simple mean, we will also consider the vote method to come up with the final prediction. To come up with the final prediction let’s locate the following profile in each of the CART models-&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Age- 35 years,&lt;/li&gt;
  &lt;li&gt;Gender- Male,&lt;/li&gt;
  &lt;li&gt;Highest Educational Qualification- Diploma holder,&lt;/li&gt;
  &lt;li&gt;Industry- Manufacturing,&lt;/li&gt;
  &lt;li&gt;Residence- Metro
For each of these Decision tree (CART models), following is the distribution across salary bands-&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/blog/RF/8_finalcart.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The final probability is simply the average of the probability in the same salary bands in different CART models. As we can see from this analysis, that there is 70% chance of the individual falling in class 1 (less than $40,000) and around 24% chance of the individual falling in class 2.
Final Inference
Random forest gives much more accurate predictions when compared to simple Decision Tree (CART) or regression models in many scenarios. These cases generally have high number of predictive variables and huge sample size. This is because it captures the variance of several input variables at the same time and enables high number of observations to participate in the prediction.&lt;/p&gt;

&lt;p&gt;So, we hope that you must have understood the concept of ‘Random Forest’ in Machine Learning through the blog. Do write your precious feedback, and feel free to ask any doubt related.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Business Intelligence Project</title><link href="http://localhost:4000/blog/The-Business-Intelligence-Project/" rel="alternate" type="text/html" title="The Business Intelligence Project" /><published>2020-09-08T00:00:00+05:30</published><updated>2020-09-08T00:00:00+05:30</updated><id>http://localhost:4000/blog/The-Business-Intelligence-Project</id><content type="html" xml:base="http://localhost:4000/blog/The-Business-Intelligence-Project/">&lt;p&gt;Initially, BI used to be the work of just IT professionals, who would supply the whole organisation with the reports for decision-making, however, now it is used at every stage of management by the managers (primarily due to the development in data science sphere and the rise of self- service BI tools).&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Ways in which BI can help organisations make smart, data- driven decisions-&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Tracking performance&lt;/li&gt;
  &lt;li&gt;Analysing Key Performance Indicators (KPI)&lt;/li&gt;
  &lt;li&gt;Analysing the market share and consumer behaviour&lt;/li&gt;
  &lt;li&gt;Optimizing business operations using historical data&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt; Business Intelligence Tools &lt;/b&gt;
&lt;br /&gt;
Business intelligence tools are nothing but software that are used to analyse trends and extract insights out of the data in order to make tactical and strategic business decisions.
There are many Business Intelligence tools like SAS BI, MicroStrategy, Datapine, Domo, etc that help in carrying out the necessary tasks but two of the most powerful and widely used tools in Business Intelligence on which this blog is also dedicated are Power BI and Tableau.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Power BI&lt;/b&gt;
&lt;br /&gt;
Power BI provides a summary of the data in the form of reports and dashboards while connecting with every data source across different. It makes data assessment, sharing scalable dashboards, embedded visualizations, interactive reports and various another feature which we will see further in the blog. It is amazing at importing visualizations with easy-to-use and user-friendly interfaces like Excel, etc. Power BI is simple for using that provides a full overview of your business performance.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Tableau&lt;/b&gt;
&lt;br /&gt;
While there are numerous intuitive business intelligence tools, Tableau uses data discovery and its interpretation for all types of the business user. It is sometimes quoted as the most user-friendly data science tool available. Being simple to handle, any user without much of a stretch can perform reading or writing data related tasks along with analysing key data insights, while creating visualizations and innovative reports, and finally sharing insights throughout the enterprise with help of dashboards and stories. Tableau is good at connecting to any data source with a drag-and-drop interface that is easy-to-use and makes transferring data simple. However, unlike Power BI it is weak at combining with different data sources for analysis.&lt;/p&gt;

&lt;p&gt;INSIGHTS&lt;/p&gt;

&lt;p&gt;As we have always said, hands-on real-time experience on analytics software are any day better than the theoretical concepts and is also our core strength, so to compare how the visualisations would look in Power BI and Tableau we took a same dataset and created a dashboard. We undertook a project under the supervision of our esteemed alumnus, &lt;u&gt;Mr. Vikrant Sharma&lt;/u&gt; (Analyst, InMobi) and &lt;u&gt;Mr. Shoury Anand&lt;/u&gt;(IIM Lucknow, PGP’22). This included a study on COVID-19 scenario where we took 5 datasets which contained data for number of COVID cases worldwide and in India. Also, to study what impact the current pandemic has on the indices we took time series data for S&amp;amp;P Global, FTSE 100 and Dow Jones along with Gold and Crude Oil prices starting from 1st January 2020 to 31st July 2020. To study how GDP has fared in the past years, we took Real GDP for 180 countries for past 13 years. Not surprisingly, the visualizations said for itself all the data which we collected and wanted to concluded our results. Finally, both the dashboards looked pretty amazing and gave us a hard time to choose one over another. So the question in on our readers which visualisation did they find better!&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Tableau Dashboard&lt;/b&gt;
&lt;img src=&quot;/blog/tableau/Tableau Dashboard.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Power BI Dashboard&lt;/b&gt; 
&lt;img src=&quot;/blog/tableau/PowerBI Dashboard.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;
&lt;br /&gt;
We have conducted a survey of sample size of 100 to gather and access the opinion of college students on Business Intelligence tools namely Power BI and Tableau.
The respondents had to answer various questions; like on what parameters would they favour one software from the other, rating these softwares on User- friendliness and attractiveness, their personal opinions/ experiences among others.
Most of the students replied on the same lines and the results matched our findings/ expectations. Tableau was found to be the favourable of the two.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Some of the opinions-&lt;/b&gt;
&lt;i&gt;“The Power BI Student Version has a lot of limitations and thus though it is capable to a large extent, cannot be used to its full capacity. Tableau has some limited features as compared to Power BI w.r.t graph styles, colour available, etc. Overall, Both the tools are very powerful for Data Viz”&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;“I feel that use of visualizations created in Tableau should be restricted to Tableau itself. Any presentation that requires only and only visualizations with little to none supporting text can be presented through Tableau’s story mode. For PowerPoint presentations, I like to connect excel sheets with it for charts, as it makes it easy for me to change the formatting of the chart within PowerPoint and any change in figures can also be speedily done.
And about dashboards in presentations, I don’t think that is a good choice because the number of visualizations in a dashboard might distract an audience, and it is difficult to demonstrate its dynamic features on a presentation.”&lt;/p&gt;

&lt;p&gt;&lt;i&gt;I have had a better experience working with Power BI over tableau due to its user-friendly nature and a relatively easier visualisation capability.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Result of the survey were as follows-&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/tableau/1.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;
&lt;img src=&quot;/blog/tableau/2.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BUSINESS INTELLIGENCE TOOLS FROM A STUDENTS’ PERSPECTIVE
Microsoft’s Power BI is a suite of data visualization and business analytic tools. It offers tools to easily analyse, transform and visualize data pipelines, including the ability to build reusable models. The software enables users to integrate their apps, to deliver reports along with real-time dashboards.
Tableau is a visualization tool that helps businesses transform their data into insights that can lead to action. The tool makes it easy to connect data in almost any format from almost any source. Interactive dashboard with visual analytics can be created with simple dragging and dropping, and data transformed in graphs, maps, charts, and other visualizations.&lt;/p&gt;

&lt;p&gt;Data Visualizations tools are necessary when it comes to creating a visual representation of analytics and sharing insights with other.
And as Management students, we faced difficulty initially in choosing the best tool for making reports and dashboards, because both Power BI and Tableau were terrific and had a lot to offer. So, to make life simpler and easier, we’ll be comparing these two on a range of parameters-&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Cost&lt;/b&gt;
Tableau is expensive than Power BI when it comes to buying the complete version which usually is bought by the businesses and working professionals. Tableau’s annual price ranges around $1000 while Power BI’s annual price ranges around $100. Even though there is a great price difference in the full version, however if we compare only the free version Tableau has an upper hand. This is because many useful features were not included in Power BI’s free desktop version and this certainly impaired us during this project. We also had to face many hassles when it came to editing the shared files.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Data Analysis&lt;/b&gt;
Power BI should be the choice if someone wants to have an in- depth analysis of the data. It offers DAX (Data Analysis Expressions) which is a delight to work with. It is designed to work with tables and relational database, creating meaningful relationships between various data sources.
Tableau has in-built features like data blending and drill-down, which one can use to determine the variations, data patterns and for further data analysis.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Data Visualization&lt;/b&gt;
There is no one best tool when it comes to Data Visualization as both Power BI and Tableau has some really powerful visuals to offer. While talking about simplicity, surely Tableau creates fascinating dashboards through simple drag- and- drop, and complex calculations can also be made with the help of simple line of codes. It offers various types of visualizations such as Heat maps, Treemaps, Scatter Plots etc. One can also create ‘Word clouds’ and ‘Bubble charts’ in Tableau.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;On the other hand, Power BI has loads of visualisations which help in depth analysis. Power BI boasts of a wide variety of visualizations, such as R script visuals and Python visuals as well. These visuals can be created in Power BI Desktop and then published online.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Performance&lt;/b&gt;
The Clock Rate in Power BI is faster than that in Tableau, it loads data sets faster than the latter, plus it saves the files in a compressed manner and takes lower disk space. One can also publish their Power BI Desktop reports online and thus can have an easy access.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;User community&lt;/b&gt;
Both these tools offer great customer support, in terms of services and learning material. However, Tableau may have an upper edge in community support due to its huge user base and awareness in general.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;User Interface&lt;/b&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Tableau has an edge over Power BI when it comes to the user interface. It has an intuitive user interface along with versatile features. Its Tool Tip is more efficient than Power BI’s and provides elaborate details.
Power BI’s user interface is no less than that of Tableau’s but it can be a little intimidating for a novice.&lt;/p&gt;

&lt;p&gt;After comparing the 2 tools on various parameters and from our personal experience, we felt that Tableau is the better of the two when it comes to creating Dashboards and visualizations.&lt;/p&gt;

&lt;p&gt;Conclusion-
Tableau remains the choice of BI Tool for students from non-technical background like us. However, the professional reports and perfect visualisations by Power BI would certainly add a feather in your cap in terms of both skillset and employability.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;&lt;u&gt;Authored by&lt;/u&gt;&lt;/b&gt;: Chaitanya Gupta and Ritik Garg&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;&lt;u&gt;Co-Authored by&lt;/u&gt;&lt;/b&gt;: Mr Shoury Anand and Mr Vikrant Sharma
&lt;img src=&quot;/blog/tableau/3.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;
&lt;img src=&quot;/blog/tableau/4.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;
&lt;img src=&quot;/blog/tableau/5.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Initially, BI used to be the work of just IT professionals, who would supply the whole organisation with the reports for decision-making, however, now it is used at every stage of management by the managers (primarily due to the development in data science sphere and the rise of self- service BI tools).</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>