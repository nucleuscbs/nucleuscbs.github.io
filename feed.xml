<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-10-14T16:35:51+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">The Analytics Bay</title><subtitle>Always Analyzing</subtitle><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><entry><title type="html">Law of Large Numbers</title><link href="http://localhost:4000/blog/Law-of-Large-Numbers/" rel="alternate" type="text/html" title="Law of Large Numbers" /><published>2021-10-14T00:00:00+05:30</published><updated>2021-10-14T00:00:00+05:30</updated><id>http://localhost:4000/blog/Law-of-Large-Numbers</id><content type="html" xml:base="http://localhost:4000/blog/Law-of-Large-Numbers/">&lt;p&gt;&lt;img src=&quot;/blog/2021-10-14-Law-of-Large-Numbers/Cover.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Law of Large Numbers is a concept of probability used in statistics which states that as the size of a sample grows or rises, its mean gets closer and closer to the average or mean of the whole population. This law in the financial context has a whole different connotation or inference, which is related to the growth rate.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2021-10-14-Law-of-Large-Numbers/1.jpeg&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;IN STATISTICS&lt;/h2&gt;
&lt;p&gt;The large numbers theorem in statistics proves that if the same study or experiment is repeated independently a great number of times, the average of the results of the experiments or trials will be close to the expected value. The average or mean of results becomes closer to the expected value as the frequency of trials increases.
The law of large numbers is an essential concept in statistics because it states that even random trials with a large number of trials may return stable, accurate, and long-term results. It is essential to understand that the average of the results of the experiment repeated a small number of times might give a substantially different value from that of the expected value. However, each additional trial increases the precision or accuracy of the average result.&lt;/p&gt;

&lt;h2&gt;COIN FLIPPING EXAMPLE&lt;/h2&gt;
&lt;p&gt;Now, let’s look at coin flips. This is a Bernoulli Trial as there are primarily two outcomes, heads and tails. The data are binary and follow the binomial distribution as provided by a proportion of events. For this scenario, an event as heads in the coin toss is defined. A coin toss is considered to be one trial. The law of large numbers proposes that when the frequency of trials increases, the proportion will converge and eventually coincide with the expected value of 0.50 (a head or a tail having a probability of 0.50 in each flip).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2021-10-14-Law-of-Large-Numbers/2.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The sample proportion becomes more stable and accurate. It converges with the expected probability value of 0.50 as the sample size is increased.&lt;/p&gt;

&lt;h2&gt;IN BUSINESS AND FINANCE TERMS&lt;/h2&gt;

&lt;p&gt;In business, the term “law of large numbers” is majorly used in relation to growth rates, in terms of a percentage. It suggests that, as a business grows, the percentage rate of growth becomes increasingly strenuous to maintain.&lt;/p&gt;

&lt;p&gt;The law of large numbers does not mean that a given sample or group of successive samples will always reflect the true and accurate population characteristics, true for especially small samples or a small number of results taken into consideration. It can also be inferred that if a given sample or series of samples drifts away from the true population mean or expected value, the law of large numbers does not strongly or firmly guarantee that successive samples will move the observed average toward the population mean (as provided by the Gambler’s Fallacy).
Basically, the concept of LLN in finance tells, that as a business grows, it gets hard to maintain the previous growth rates.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Practical Example&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2021-10-14-Law-of-Large-Numbers/3.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s assume there are two companies namely, ABC Ltd. and XYZ Ltd. The market capitalization of the companies ABC and XYZ are $1 million and $100 million respectively. Let’s further assume the growth of ABC is 50% for the current year, it is attainable as it will grow further by $500,000.
But the same is not possible for the company XYZ as it will have to grow by $50 million to attain the 50% growth rate point. Thus, as company XYZ Ltd. will continue to expand, the growth rate of the company will fall or decline.&lt;/p&gt;

&lt;h2&gt;TYPES OF LAW OF LARGE NUMBERS&lt;/h2&gt;

&lt;p&gt;There are two main parts of the law of large numbers. They are: the weak law and the strong law of large numbers. The difference between the two is mostly theoretical.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;THE WEAK LAW:&lt;/b&gt;
The mean of a sample gets nearer to, i.e., converges the population mean as the sample size grows bigger. This law is known as the Weak Law of Large Numbers or the Bienaymé–T Chebyshev Inequality.
In other words, The Weak Law of Large Numbers proves that the sample average value converges in probability towards the expected value of the population.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;THE STRONG LAW:&lt;/b&gt;
The strong law of large numbers (also called Kolmogorov’s law) tells that the sample average converges almost surely with the expected value of the population.&lt;/p&gt;

&lt;p&gt;The Strong Law of Large Numbers tells that when the sample size grows infinite times the average mean will converge to the one for sure.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;THE DIFFERENCE&lt;/b&gt; - Weak law of large numbers provides that it is a probability that the sample average will converge towards the expected value whereas Strong law of large numbers shows almost sure convergence. Weak law has a probability tending to 1 whereas Strong law has a probability equal to 1.&lt;/p&gt;

&lt;h2&gt;CONCLUSION&lt;/h2&gt;

&lt;p&gt;The law of large numbers plays an essential role because it guarantees accurate results from the averages of random events. For instance, while playing roulette, a casino may lose a sum of money in a single spin of the wheel, though its earnings will get closer to a predictable average value over a large number of spins. Any winning or losing streak by a player will certainly be overcome by the parameters of the game. It is integral to remember that the law of large numbers only applies when a large number of observations are taken into consideration. This principle does not work when a small number of observations is considered. Greater frequency of observations will eventually let the averages coincide with the expected value and that a streak of one value will immediately be balanced and compensated by the others.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Gambler’s Ruin</title><link href="http://localhost:4000/blog/Gamblers-Ruin/" rel="alternate" type="text/html" title="The Gambler’s Ruin" /><published>2021-09-30T00:00:00+05:30</published><updated>2021-09-30T00:00:00+05:30</updated><id>http://localhost:4000/blog/Gamblers-Ruin</id><content type="html" xml:base="http://localhost:4000/blog/Gamblers-Ruin/">&lt;p&gt;&lt;img src=&quot;/blog/2021-09-30-GamblersRuin/Cover.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Imagine that a reluctant gambler is dragged to a casino by his friends. He’s very conservative when it comes to gambling so he only takes $50 in the casino to gamble with. Since he doesn’t know a whole lot about gambling, he decides to play roulette. The gambler places a very simple bet of $25 on red. So with every spin, if red occurs he will win $25 but if black occurs he will lose $25. Therefore the odds of winning or losing are almost 50% each. A thing to notice is that in casinos, the odds are always slightly uneven because if they were even the casino would never make money. Casinos set up games that are slanted in their favor and so are the payouts. The gambler sets up some simple rules -  he decides that he will quit playing when he has either 0 money left or he is up by $25 i.e has $75. We can model this entire process as a Markov Chain and examine its long-term behavior. A Markov Chain is a model that helps in defining a sequence of all possible events where the probability of each event depends only on the state attained in the previous event. We realize that at any point in time, the only relevant information is the amount of money the gambler has available. How he got to that amount in the past is irrelevant. Firstly, we can set up a transition diagram to represent the possible evolution of this game:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2021-09-30-GamblersRuin/1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are 4 states in this transition diagram. The gambler is going to come into the casino with $50. There is a possibility that he loses all his money and goes broke (gets ruined) or he can end up winning up to $75 and then he stops and calls it a night. These two are the endpoints of our transition diagram.  Since he is betting $25 dollars at a time there is also a state that he has $25 left with him. Now at each step, if he actually has the money to make a bet, the chance of winning or losing is 0.5 so for example if he has $25 and he makes a bet the chance of winning and going to $50 is 0.5. Let’s go ahead and set up a matrix using this transition diagram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2021-09-30-GamblersRuin/2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the two ends we have what are called Absorbing States. An absorbing state is a state in which once you enter, there is no leaving. It’s almost like a black hole. Once the gambler is broke (or ruined), he’s always broke and once he wins $75 he quits and therefore always has that $75. So once he enters these absorbing states, he never leaves that state. In our transition matrix, on the left hand side we have the state we’re coming from and on the top we have the state we’re going to so you can see the one there in the top left that just means if he’s broke now he’ll be broke next time and on the lower right that just means if he had $75 he will always have $75 because he stopped playing and then the point five probabilities show the various transition stages so for example the probability of going from the $25 state to the broke state is 0.5. This matrix showcases the exact same information that is present in the transition diagram in matrix form.&lt;/p&gt;

&lt;p&gt;What do we find out when we analyze this matrix and look at some long-run probabilities of this game? Let’s start by taking the transition matrix 2 steps into the future.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2021-09-30-GamblersRuin/3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looking at the second row second column we get a 0.25 probability. This means that if our gambler starts with $25 when he walks into the casino and plays this game the probability of having the $25 in his pocket two spins of the game from now is 0.25 or 25%. We can even take this further and see what we would expect 10 spins into the game:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2021-09-30-GamblersRuin/4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we interpret the above matrix, we can see that after 10 spins of the wheel if he walks in with $25 the probability that he will be broke 10 spins from now is 0.667 but if we go down to the next row we see that if he comes in with $50 the chance of him being broke after 10 spins is 0.333. So we can see that the probability of being broke or ending up with $75 is dependent on the amount of money he started with as well.&lt;/p&gt;

&lt;p&gt;If we run this matrix into the future, we see that the probability of being broke if he comes in with $25 dollars is ⅔ but is ⅓ if he comes with $50. Similarly, the probability of having $75 if he comes in with $25 is 2/3 and is 2/3 if he comes with $50.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2021-09-30-GamblersRuin/5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It might be difficult to believe that, given a fair game, the probability that someone will win their desired amount or get ruined is determined by their initial wealth. Using Markov chains, we can determine the same probabilities between any sequences of games using the transition matrix. This concept has specific relevance towards gambling, however it is also used in various mathematical theorems with wide applications in probability and statistics.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Potato Paradox</title><link href="http://localhost:4000/blog/Potato-Paradox/" rel="alternate" type="text/html" title="The Potato Paradox" /><published>2021-09-16T00:00:00+05:30</published><updated>2021-09-16T00:00:00+05:30</updated><id>http://localhost:4000/blog/Potato-Paradox</id><content type="html" xml:base="http://localhost:4000/blog/Potato-Paradox/">&lt;p&gt;&lt;img src=&quot;/blog/PotatoParadox/1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Potato, one of the most commonly used vegetables in cooking, which we’ve long connected with for making us fat or being our favourite carbohydrate, actually has a perplexing paradox built into it. But potatoes have never been contradictory, have they? Wrong! THE POTATO PARADOX IS HERE.&lt;/p&gt;

&lt;p&gt;Assume you have 100 pounds of potatoes, and 99 % of their weight is water. The remaining 1% of their weight is made up of carbohydrates, proteins, pectins, and minerals. They dry out a little after being left out overnight. They’re only 98 % water when you wake up, thus just 2% of it is solid. So, how much does your sack of potatoes now weigh? Your sack of potatoes would now weigh 50 pounds after this minor adjustment. So, what causes this? Is it true that the potato went on a complete carb-free diet in one night? Maybe not, because they’d been killed doing it. Clearly not. Let’s delve into the math now to better grasp this.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/PotatoParadox/2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The 1% change in composition reduces the weight by HALF. How? Considering the case of Potatoes is the simplest method to visualise the solution. There are a hundred of them. Let’s use one painted potato to represent the 1% solid potato substance we started with. The water is represented by the remaining 99. The amount of solid stuff in the potatoes does not alter as they dry out. The water is the only thing that disappears. So, if we take away ONE unit of water, we’re left with 99 pounds of material, one of which is dry and the other 98 being water. So, let’s just perform some quick math here; 98.989899 % is 98.989899 %. Okay, that’s a lot of water.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/PotatoParadox/3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s just take out another pound of water, and we’ll have 97 of the 98 pounds here that are water. So we’ll go with 97 out of 98, which is 98.979591836 %. There’s simply too much water. There has to be a better way to do things.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/PotatoParadox/4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The issue is that this is decreasing extremely slowly since for every unit of water that evaporates, the total amount remaining decreases as well. Let’s take a look at the final result to see how many we need to remove. So, instead of 98 or 97, we need to cut our water supply to 49. What’s more, here’s why: So we’ve got 98 % water and 2% solids, but only one solid unit. Let’s change that 2 to a 1.&lt;/p&gt;

&lt;p&gt;So, we’ll divide 2 by 2 and get 1, and we’ll do the same thing with our 98, so we’ll divide that by 2 and get 49.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/PotatoParadox/5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So 49 plus 1 equals 50, and the solution is 50 pounds. So we have to get rid of a LOT of water potatoes. I’m not sure how many are left, but I believe there are 49 waters and one solid. And 98 % is equal to 49 divided by 50. It was a success! That is the solution.
The potato paradox comes into play every time there are two items and the concentration of one doubles. That requires the other one’s size to be reduced by half of the whole, whether it’s doubling from 1% or .00001%. Or 10%. Ask your friends to solve this problem and see what they say.&lt;/p&gt;

&lt;p&gt;Most of the time, our first response is to assume that not much has changed since 1% is so small. This isn’t something to be ashamed of – our brains evolved to compare quantities like this: there’s one wooly mammoth and there’s five of us or how much food do we need to keep the family surviving through the winter? Evaluating concentrations is more abstract and not usually a life-or-death issue that natural selection would play a role in shaping.&lt;/p&gt;

&lt;p&gt;Unlike other well-known paradoxes, such as time travel ones, the POTATO conundrum is a VERIDICAL paradox, meaning it has a TRUE solution that we can all agree on and verify, but it is nonetheless startling. So, the potato paradox is a type of paradox that isn’t based on misunderstandings, impossibilities, or speculation, but rather on a deep understanding of how the mind works.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">An Introduction To The Confusion Matrix</title><link href="http://localhost:4000/blog/Confusion-Matrix/" rel="alternate" type="text/html" title="An Introduction To The Confusion Matrix" /><published>2021-09-02T00:00:00+05:30</published><updated>2021-09-02T00:00:00+05:30</updated><id>http://localhost:4000/blog/Confusion-Matrix</id><content type="html" xml:base="http://localhost:4000/blog/Confusion-Matrix/">&lt;p&gt;A confusion matrix is a table of four different combinations of actual and the predicted values and is an important step in calculating the recall, precision, and accuracy of the values, as well as AUC-ROC curves.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/ConfusionMatrix/1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;●	The target variable is binary, that is, either Positive or Negative&lt;/p&gt;

&lt;p&gt;●	Actual values of the target variable are represented by the columns&lt;/p&gt;

&lt;p&gt;●	Predicted values of the target variable are shown in the rows of the matrix&lt;/p&gt;

&lt;p&gt;The matrix has been divided into four parts:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;True Positive&lt;/li&gt;
  &lt;li&gt;True Negative&lt;/li&gt;
  &lt;li&gt;False Positive&lt;/li&gt;
  &lt;li&gt;False Negative&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;True Positive (TP) &lt;/h2&gt;

&lt;p&gt;●	The predicted value matches the actual value&lt;/p&gt;

&lt;p&gt;●	The actual outcome was positive and the classifier predicted positive too&lt;/p&gt;

&lt;h2&gt;True Negative (TN) &lt;/h2&gt;

&lt;p&gt;●	The predicted value matches the actual value&lt;/p&gt;

&lt;p&gt;●	The actual outcome was negative and the model’s prediction was negative as well&lt;/p&gt;

&lt;h2&gt;False Positive (FP) – Type 1 error&lt;/h2&gt;

&lt;p&gt;●	The predicted value was falsely predicted&lt;/p&gt;

&lt;p&gt;●	The model predicted a positive value but the actual value was negative&lt;/p&gt;

&lt;p&gt;●	Also known as the Type 1 error&lt;/p&gt;

&lt;h2&gt;False Negative (FN) – Type 2 error&lt;/h2&gt;

&lt;p&gt;●	The predicted value was falsely predicted&lt;/p&gt;

&lt;p&gt;●	The model predicted a positive value but the actual outcome was positive&lt;/p&gt;

&lt;p&gt;●	Also known as the Type 2 error&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/ConfusionMatrix/2.png&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;How do we use the Matrix?&lt;/h2&gt;

&lt;p&gt;The confusion matrix helps in determining values that explain the results of the classifier through certain metrics to improve our understanding of its performance. These are:&lt;/p&gt;

&lt;h2&gt;Precision &lt;/h2&gt;
&lt;p&gt;Precision is the ratio of correct positive predictions to the total of all positive outcomes.&lt;/p&gt;

&lt;p&gt;It is also called Positive predictive value.&lt;/p&gt;

&lt;p&gt;Precision = TP/(TP+FP)&lt;/p&gt;

&lt;h2&gt;Recall&lt;/h2&gt;
&lt;p&gt;Recall is the ratio of the correct positive results to the total positive predictions.&lt;/p&gt;

&lt;p&gt;It is also called Sensitivity, Probability of Detection, True Positive Rate.&lt;/p&gt;

&lt;p&gt;Recall= TP/(TP+FN)&lt;/p&gt;

&lt;h2&gt;Accuracy&lt;/h2&gt;

&lt;p&gt;Accuracy is defined as the ratio of correct predictions by the total predictions.&lt;/p&gt;

&lt;p&gt;Accuracy = Correct Predictions/ Total Predictions&lt;/p&gt;

&lt;p&gt;In a confusion matrix, it can be derived using:&lt;/p&gt;

&lt;p&gt;Accuracy = (TP+TN)/(TP+TN+FP+FN)&lt;/p&gt;

&lt;p&gt;Accuracy is a handy metric for evaluation when all the classes are of equal importance. But this might not be the case if we are predicting if a patient has a fatal diagnosis or not. Here, False Positives are acceptable, but False Negatives are not.&lt;/p&gt;

&lt;h2&gt;ROC curve&lt;/h2&gt;

&lt;p&gt;A ROC curve (receiver operating characteristic curve) graphs the performance of a classification model at all classification thresholds.
(Using thresholds: Say, if you want to compute TPR and FPR for the threshold equal to 0.6, you apply the model to each example, get the score, and, if the score &amp;gt;=0.6, you predict the positive class; otherwise, the prediction is negative)&lt;/p&gt;

&lt;p&gt;It plots 2 parameters:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;True positive rate&lt;/b&gt; (Recall)= TP/(TP+FN)&lt;/p&gt;

&lt;p&gt;&lt;b&gt;False Positive rate&lt;/b&gt;= FP/(FP+TN)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/ConfusionMatrix/3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lowering the threshold predicts more items as positive, thus increasing both False Positives and True Positives in the outcome.&lt;/p&gt;

&lt;h2&gt;AUC&lt;/h2&gt;

&lt;p&gt;AUC stands for &lt;b&gt;Area under the ROC&lt;/b&gt; Curve. It provides an average measure of performance across all possible probability thresholds of the results.&lt;/p&gt;

&lt;p&gt;The higher the area under the ROC curve (AUC), the better the model and the more efficient the classifier. A perfect model would have an area of 1. Usually, if your model is efficient, you obtain a good performance class by selecting the value of the threshold whose TPR tends to 1 while FPR inches closer to 0.&lt;/p&gt;

&lt;h2&gt;An Example of the Confusion Matrix&lt;/h2&gt;

&lt;p&gt;Suppose we have 165 patients being tested for Covid-19, and have gotten the following results from the tests:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/ConfusionMatrix/1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These are the most important metrics that we derive from the matrix:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Accuracy:&lt;/b&gt; Overall, how often is the test correctly predicting the patient’s diagnosis?&lt;/p&gt;

&lt;p&gt;(TP+TN)/Total = (100+50)/165 = &lt;b&gt;0.91&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Misclassification Rate:&lt;/b&gt; Overall, how often is it wrong?&lt;/p&gt;

&lt;p&gt;(FP+FN)/total = (10+5)/165 = &lt;b&gt;0.09&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;It is also equal to (1-Accuracy), and is also known as the “Error Rate”.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;True Positive Rate:&lt;/b&gt; When it’s actually positive, how often does it predict positive?&lt;/p&gt;

&lt;p&gt;TP/(TP+FP) = 100/105 = &lt;b&gt;0.95&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;We’ve discussed this earlier, also known as “Sensitivity” or “Recall”.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;False Positive Rate:&lt;/b&gt; When it’s actually negative, how often does the test predict positive?&lt;/p&gt;

&lt;p&gt;FP/(TN+FN) = 10/60 = &lt;b&gt;0.17&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;True Negative Rate:&lt;/b&gt; When it’s actually negative, how often does it give a negative outcome?&lt;/p&gt;

&lt;p&gt;TN/(TN+FP) = 50/60 = &lt;b&gt;0.83&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;True Negative Rate is equal to (1-False Positive Rate), and is called “Specificity”.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Precision:&lt;/b&gt; When the outcome is positive, how often is it correct?&lt;/p&gt;

&lt;p&gt;TP/predicted yes = 100/110 = &lt;b&gt;0.91&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Prevalence:&lt;/b&gt; How often does the test predict positive?&lt;/p&gt;

&lt;p&gt;TP+FN/Total = 105/165 = &lt;b&gt;0.64&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;These metrics help us in understanding the results better as well as the efficiency of the classification.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The Confusion Matrix and the metrics derived from it are really helpful in analyzing the data as well as the algorithm used for classification, like the following.&lt;/p&gt;

&lt;p&gt;●	ROCs define the trade-off between the TPR and FPR for a predictive model using different probability thresholds.&lt;/p&gt;

&lt;p&gt;●	Precision-Recall curves summarize the trade-off between the TPR and the positive prediction efficiency for a  model using different probability thresholds.&lt;/p&gt;

&lt;p&gt;●	ROCs are suitable when the observations are balanced between each class, whereas precision-recall curves are more suitable when the data is imbalanced.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">A confusion matrix is a table of four different combinations of actual and the predicted values and is an important step in calculating the recall, precision, and accuracy of the values, as well as AUC-ROC curves.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Gradient Descent</title><link href="http://localhost:4000/blog/Gradient-Descent/" rel="alternate" type="text/html" title="Gradient Descent" /><published>2021-08-19T00:00:00+05:30</published><updated>2021-08-19T00:00:00+05:30</updated><id>http://localhost:4000/blog/Gradient-Descent</id><content type="html" xml:base="http://localhost:4000/blog/Gradient-Descent/">&lt;p&gt;&lt;img src=&quot;/blog/GradientDescent/9.png&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Optimization is an important part of our life. We all have limited resources and time and we wish to make the most of them. From utilizing resources effectively to solving problems for an organization – everything uses optimization. Optimization is required everywhere whether you are working with a real-life problem or building a product.&lt;/p&gt;

&lt;p&gt;Optimization means getting the optimal solution for your problem.&lt;/p&gt;

&lt;p&gt;Optimization starts with very simple and basic problems, but it can get very complex sometimes. For example, allocating a monthly household budget is a simple optimization problem. On the other hand, devising various strategies for any organization company can be very complex.&lt;/p&gt;

&lt;p&gt;Linear regression is a simple optimization problem. The representation is a linear equation that uses a specific set of input values/training data values (x) and a predicted output value/test set values (y).&lt;/p&gt;

&lt;p&gt;So in machine learning, we perform optimization on the training data and check its performance on newly defined test data.&lt;/p&gt;

&lt;p&gt;Many popular machine algorithms depend upon optimization techniques such as linear regression, k-nearest neighbors, neural networks, etc. The applications of optimization are limitless and are a widely researched topic in both academia and industries.&lt;/p&gt;

&lt;p&gt;Gradient Descent is the most commonly used optimization technique when dealing with machine learning.&lt;/p&gt;

&lt;h2&gt;What is Gradient Descent?&lt;/h2&gt;

&lt;p&gt;It is an optimization algorithm to reduce the cost of the function. We start with any random point on the function since we are unaware of the direction where we will obtain the most optimal solution and move in the &lt;b&gt;opposite direction&lt;/b&gt; of the &lt;b&gt;gradient of the function&lt;/b&gt; to obtain the &lt;b&gt;local/global minima.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/GradientDescent/7.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To understand the distinction between local and global minima, let’s take a look at the figure above. The global minimum is the least value of any function while a local minimum is the least value of a function in a certain neighborhood.&lt;/p&gt;

&lt;p&gt;To explain Gradient Descent, we will use the standard example of hill descending.&lt;/p&gt;

&lt;p&gt;Consider a valley on which you are standing. Now your task is to reach to the lowest point of the valley. A twist is that you are blindfolded and you have no visibility to see where you are heading to. So let’s understand what approach can be used for this problem&lt;/p&gt;

&lt;p&gt;The best way is to check the ground near you and observe where the terrain tends to descend or decrease. This will give you a good idea of what direction you must take your first step. If you follow the descending path, you would probably reach the bottom of the camp.&lt;/p&gt;

&lt;p&gt;To represent this graphically, let’s have a look at the below graph.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/GradientDescent/6.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let us now map this scenario in a mathematical formula.&lt;/p&gt;

&lt;p&gt;We have to start with some θ0  and θ1. We need to keep changing the parameters to reduce the cost function until we hopefully end up at a minimum.&lt;/p&gt;

&lt;p&gt;The algorithm of gradient descent can be written as follows&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/GradientDescent/5.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, on the y-axis, we have the cost function J(θ) along with the parameters θ1 and θ2 on the other two axes.&lt;/p&gt;

&lt;p&gt;Now there are some kinds of gradient descent algorithms that can be further classified as follows:
&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;● On the basis of data ingestion&lt;/b&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Full Batch Gradient Descent Algorithm&lt;/li&gt;
  &lt;li&gt;Stochastic Gradient Descent Algorithm&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For full batch gradient descent algorithms, we use the complete data along with all the parameters to compute the gradient, whereas, for stochastic algorithms, we take only a small sample of the data.
&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;● On the basis of differentiation techniques&lt;/b&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;First-order Differentiation&lt;/li&gt;
  &lt;li&gt;Second-order Differentiation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Gradient descent calculates gradient by differentiating the cost function. To take the derivative of the cost function, either first-order or second-order differentiation can be used.&lt;/p&gt;

&lt;h2&gt;Challenges in executing Gradient Descent&lt;/h2&gt;

&lt;p&gt;Gradient Descent is a binding technique that works in most cases. But there are various cases where gradient descent doesn’t work properly or fails to find an optimum value. The major reasons for its failure are as follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Data challenges&lt;/li&gt;
  &lt;li&gt;Gradient challenges&lt;/li&gt;
  &lt;li&gt;Implementation challenges&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;A. Data Challenges&lt;/b&gt;
&lt;br /&gt;
● Gradient Descent has a very low convergence rate and thus the answer is obtained in several iterations, consuming a lot of time and effort.
&lt;br /&gt;
● There is also a saddle point problem. This is a point in the data set where the gradient is zero and it is thus not an optimal point.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;B. Gradient Challenges&lt;/b&gt;
&lt;br /&gt;
● If the learning parameter becomes extremely large, the method of Gradient Descent can overshoot the minimum, it may fail to converge or even diverge.
&lt;br /&gt;
● One of the other problems of this approach is converging to a local minima can be quite slow. If there are multiple local minima present in the dataset, then there is no guarantee that the algorithm will detect the global minimum.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;C. Implementation Challenges&lt;/b&gt;
&lt;br /&gt;
● When implementing the algorithm of gradient descent, it is extremely important to calculate how many resources one would need. If the memory is too small, then the network would fail.
&lt;br /&gt;
● Also, it’s important to keep track of floating-point data values and hardware/software prerequisites.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The method of Gradient Descent can also be used for multiple regression when we have multiple features/parameters. To improve Gradient descent for multiple features, methods like Feature Scaling, Mean normalization, and Debugging can be used.
Gradient Descent can be used for both Linear and Logistic Regression machine learning algorithms. This method is a standard method of minimizing the cost function which works well for every machine learning algorithm.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Turing Test</title><link href="http://localhost:4000/blog/Turing-Test/" rel="alternate" type="text/html" title="The Turing Test" /><published>2021-08-05T00:00:00+05:30</published><updated>2021-08-05T00:00:00+05:30</updated><id>http://localhost:4000/blog/Turing-Test</id><content type="html" xml:base="http://localhost:4000/blog/Turing-Test/">&lt;p&gt;&lt;img src=&quot;/blog/TuringTest/Turing1.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;WHAT IS THE TURING TEST?&lt;/h2&gt;

&lt;p&gt;The Turing Test (originally known as an Imitation Game) is a method of inquiry into artificial intelligence (AI) for determining whether a computer is capable of thinking like a human being or not. The test is named after its creator Alan Turing, an English Computer Scientist, Theoretical Biologist, Mathematician, and Cryptanalyst.&lt;/p&gt;

&lt;p&gt;Turing proposed that a computer can be said to possess artificial intelligence if it can replicate human responses under particular conditions. The Original Turing Test involved three entities, each of which would be physically separated from the rest. One of the entities would be operated by a computer, while the other two would be operated by humans.&lt;/p&gt;

&lt;p&gt;During the test, one human is an interrogator, while the second human and the computer function as respondents or answer terminals. The questioner asks questions from the respondents within a particular subject area with a similar format and context. After a pre-set duration (usually 5 minutes), or a particular number of questions, the questioner is then asked to differentiate the human and the computer out of the two.&lt;/p&gt;

&lt;p&gt;The test is repeated a number of times. If the questioner cannot reliably differentiate between the conversation of the computer and human then the computer is considered to have passed the test because its answers are “just as human” as the human respondent.&lt;/p&gt;

&lt;h2&gt;THE WORKING OF THE TURING TEST&lt;/h2&gt;

&lt;p&gt;According to Turing, the main motive of the test is that a machine has to try and pretend to be a human, by answering questions put to it, and it will only pass if it is successful in convincing to be a human.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/TuringTest/Turing2.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The humans were restricted from giving away any of the personal information during the tests.&lt;/p&gt;

&lt;p&gt;In the tests conducted at the Royal Society in June 2014, there were six different sessions with five parallel imitation games at a time occurring during each session. A different judge was selected for each game, which meant there were five judges in each session. Each session had five rounds, with five parallel imitation games in each round. Each anonymous human was part of the five games in a session.  All five machines (the five different competition bots) took part, so every machine was involved in five games per session, hence 30 games in totality.&lt;/p&gt;

&lt;p&gt;In a particular session, a judge conducted five different tests. In their first test, they noticed a hidden human pitted against a hidden machine&lt;/p&gt;

&lt;p&gt;The second test conducted involved a different human against a different machine. And so on. It would continue until the judge had conducted all five tests in that session. At the end of each test, they were asked to tell each entity if they were able to differentiate between a machine and a human.&lt;/p&gt;

&lt;h2&gt;RESULTS&lt;/h2&gt;

&lt;p&gt;There were five machines involved in total in the tests and their success rates were:&lt;/p&gt;

&lt;p&gt;●	Eugene Goostman 33%&lt;/p&gt;

&lt;p&gt;●	Elbot 27%&lt;/p&gt;

&lt;p&gt;●	J. Fred 20%&lt;/p&gt;

&lt;p&gt;●	Ultra-Hal 13%&lt;/p&gt;

&lt;p&gt;●	Clever Bot 7%&lt;/p&gt;

&lt;p&gt;In each case, their success rate was in respect of judges identifying them as humans. Eugene Goostman was the program that was considered to have passed the test with a success rate of 33%.&lt;/p&gt;

&lt;h2&gt;EUGENE GOOSTMAN&lt;/h2&gt;

&lt;p&gt;Eugene Goostman is a chatbot programmed by three Russian programmers in Saint Petersburg in 2001. The program replicates a thirteen-year-old Ukrainian boy and is said to have been successful in passing the Turing test at an event organized by the University of Reading.&lt;/p&gt;

&lt;p&gt;According to Vladimir Veselov, one of the creators of the program, the choice of the age of Eugene Goostman was taken as thirteen because a thirteen-year-old is “neither too old to know a lot nor too young to know nothing”. This young age also leads to people ignoring any minor grammatical errors that the program might end up making.
The following is the transcript of a conversation between one of the Judges of the test, and two respondents, one of which is a human and the other is Eugene Goostman. We would like you to read both the transcripts and see whether you are able to distinguish between the human respondent and Eugene or not.&lt;/p&gt;

&lt;h2&gt;TRANSCRIPT 1:&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; Good afternoon&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; Good afternoon!&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; How many left hands do you have?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; The same as right hands, and how about you?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; The same of course. I love the scent of new-mown hay. How do you feel about scent?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; I find it exciting and refreshing! Does it make you feel anything?
Judge: Well it depends..Which smell do you really hate?&lt;/p&gt;

&lt;h2&gt;TRANSCRIPT 2:&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; Hello&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; Hello, I’m really glad to have the chance to chat with you! My guinea pig Bill sends his regards too!&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; Is Bill a male or a female?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; Ask Bill personally, please.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; Well I’d rather talk to you. My name is Jane and I am female. How about you? What’s your gender?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; I’m a male. A “guy”, I’d say.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; Pleased to meet you. What’s the weather like where you are?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; Let’s get on with our conversation!&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; Don’t you like talking about the weather?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; All these talks about weather is a waste of time&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; What would you like to discuss?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; I don’t know … Better tell me more about yourself! Where do you come from, by the way? Could you tell me about the place where you live?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; It’s a lovely place with two bedrooms and a great view over London. What can you see from your bedroom window?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; I’ve never been to London, but I’d really love to visit it! To see their … their … (damn, what do they have …) Oh yes – their Big-Ben!&lt;/p&gt;

&lt;p&gt;In this conversation, the respondent in Transcript 1 was a female human whereas the Respondent in Transcript 2 was the machine Eugene. The judge considered the respondent in Transcript 1 to be definitely a machine, awarding it only 20 out of 100 (a very poor score) for humanlike conversation. However, they marked the respondent in Transcript 2 i.e Eugene Goostman as unsure.&lt;/p&gt;

&lt;h2&gt;ALTERNATIVES TO THE TURING TEST&lt;/h2&gt;
&lt;p&gt;There were many alternatives to Turing Tests that were later developed. These alternatives include:&lt;/p&gt;

&lt;p&gt;●	The Marcus Test – A test in which a program can watch a visual show and is then interrogated by being asked meaningful questions about the show’s content.&lt;/p&gt;

&lt;p&gt;●	The Lovelace Test 2.0 – A test made to detect AI through its ability to create art.&lt;/p&gt;

&lt;p&gt;●	Winograd Schema Challenge – A test that asks multiple-choice questions in a particular format.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Logistic Regression And Surviving The Titanic</title><link href="http://localhost:4000/blog/Logistic-Regression/" rel="alternate" type="text/html" title="Logistic Regression And Surviving The Titanic" /><published>2021-07-22T00:00:00+05:30</published><updated>2021-07-22T00:00:00+05:30</updated><id>http://localhost:4000/blog/Logistic-Regression</id><content type="html" xml:base="http://localhost:4000/blog/Logistic-Regression/">&lt;p&gt;Logistic Regression, or the Logit Model, is one of the fundamental blocks of statistics and machine learning. It is used when the dependent variable is categorical, and is the go to method for binary classification (two class values). For example, the probability of win/lose or pass/fail an exam. Most popular use of logistic regression today is in determining if an email is spam or not.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic1.jpg&quot; /&gt;&lt;/p&gt;

&lt;h1&gt;How does it work?&lt;/h1&gt;
&lt;p&gt;At the core of the logistic regression is the logit function, also called the sigmoid function and was developed by statisticians to describe properties of population growth in ecology, biology and environment sciences. It’s an S-shaped curve that maps any real-valued number into a value between 0 and 1, not necessary at only those limits. The equation for Logistic Regression is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic2.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where y is the predicted output, B0 is the intercept and B1 is the coefficient for (x). It can be said that Logistic regression is a linear function. However, the predictions are morphed into classification using the logit function.
Example of Logistic Regression&lt;/p&gt;

&lt;p&gt;We can use an example to learn Logistic Regression better. Let’s say we have data that can be used to predict a person’s gender based on their height. Given a height of 150cm is the person male or female.&lt;/p&gt;

&lt;p&gt;Let’s say that the coefficients are b0 = -100 and b1 = 0.6. The above equation can be utilized to predict if a person is male given a height of 150cm.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;y = e^(b0 + b1&lt;em&gt;x) / (1 + e^(b0 + b1&lt;/em&gt;x))&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;y = e^(-100 + 0.6&lt;em&gt;150) / (1 + e^(-100 + 0.6&lt;/em&gt;x))&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;y = 0.00004539&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;The probability is so low that it can be used as 0, and certainly this person is not male.
Since, this is classification and we want a crisp answer, we can create bins for a complete classification of the values, for example:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;0 if p(male) &amp;lt; 0.5&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;1 if p(male) &amp;gt;= 0.5&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Logistic regression models are models that have a certain fixed number of parameters that depend on the number of input features, and they output categorical predictions, like for example if a cancer is malignant or not.&lt;/p&gt;

&lt;h1&gt;Types of Logistic Regression&lt;/h1&gt;

&lt;p&gt;&lt;b&gt;Binary Logistic Regression:&lt;/b&gt; The final response has only two possible outcomes. For example, either a student passes an exam or not.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Multinomial Logistic Regression:&lt;/b&gt; More than two possible outcomes, without any ordering. For example, predicting which of the election candidates wins among many.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Ordinal Logistic Regression:&lt;/b&gt; An ordered possibility of outcomes. For example, figuring out the movie rating from 1 to 5.&lt;/p&gt;

&lt;p&gt;However, in this article we’ll be focusing solely on the binary classification type as it is the most popular among the three. ‘&lt;/p&gt;

&lt;h1&gt;Surviving a Disaster and The Titanic Dataset&lt;/h1&gt;

&lt;p&gt;The most popular dataset on Kaggle, undoubtedly, is the Titanic Dataset. It can also be considered a rite of passage for aspiring data scientists learning classification models. And why not, the data is structured in a way that helps people learn the fundamentals of classification and logistic regression.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic3.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The dataset has the following variables (attributes) which are explained very well on Kaggle.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic4.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, we shall use the train.csv provided to train the model and predict the survival of a passenger based on the given variables.&lt;/p&gt;

&lt;h1&gt;Code &lt;/h1&gt;

&lt;h3&gt;Importing the dataset, and understanding the data: &lt;/h3&gt;

&lt;p&gt;import pandas as pd&lt;/p&gt;

&lt;p&gt;titanic = pd.read_csv(“train.csv”)&lt;/p&gt;

&lt;p&gt;titanic.shape&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic5.jpg&quot; /&gt;
&lt;img src=&quot;/blog/LogisticRegression/Logistic6.jpg&quot; /&gt;
&lt;img src=&quot;/blog/LogisticRegression/Logistic7.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Data Preprocessing &lt;/h3&gt;

&lt;p&gt;It is also worth noting that ‘Embarked’ has 3 classes C, Q, S which have to be converted into individual attributes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic8.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, we’ve used the ‘get_dummies’ function to create separate variables for each Embarked class. And we’ve joined the new dataframe with the original dataframe.&lt;/p&gt;

&lt;p&gt;We’ve created two dataframes X and y, which will be used for Logistic Regression and learning. And dropped multiple non-numeric attributes which have no effect on the survival of a passenger.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic9.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see that ‘Age’ has many null values, so we use ‘mean’ to impute null values.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic10.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Creating the Model&lt;/h3&gt;

&lt;p&gt;We’ll use ScikitLearn to create the Logistic Regression model, and split the dataset into 80% (used for training the model) and 20% (for testing the model).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic11.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After the model is created, it is necessary to check how well it has performed. The model score for testing is 0.754, which means that 75.4% of the time the model correctly predicts if a passenger has survived the disaster or not. We also figured out the intercept and coefficients (the array is made up of all the attributes used in the model).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic12.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, the correlation between the attributes and survival can be better understood with a visual.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic13.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This cell’s output is a heatmap that shows the correlation between all the attributes.
&lt;img src=&quot;/blog/LogisticRegression/Logistic14.jpg&quot; /&gt;&lt;/p&gt;

&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Logistic regression is one of the most exciting concepts in statistics and a powerful tool to classify data. However, one shortcoming of Logit functions is that they are not able to work well with outliers and leads to overfitting. Hence, we must try and remove outliers from the data provided to make the model more accurate.&lt;/p&gt;

&lt;p&gt;This was the most simple method that can be used to train a ML Logistic model. We can always use more sophisticated models for better prediction and classification using a more detailed analysis of the data and more complex feature engineering.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Logistic Regression, or the Logit Model, is one of the fundamental blocks of statistics and machine learning. It is used when the dependent variable is categorical, and is the go to method for binary classification (two class values). For example, the probability of win/lose or pass/fail an exam. Most popular use of logistic regression today is in determining if an email is spam or not.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Linear Discriminant Analysis</title><link href="http://localhost:4000/blog/Linear-Discriminant-Analysis/" rel="alternate" type="text/html" title="Linear Discriminant Analysis" /><published>2021-07-08T00:00:00+05:30</published><updated>2021-07-08T00:00:00+05:30</updated><id>http://localhost:4000/blog/Linear-Discriminant-Analysis</id><content type="html" xml:base="http://localhost:4000/blog/Linear-Discriminant-Analysis/">&lt;p&gt;&lt;img src=&quot;/blog/Linear-Discriminant-Analysis/LDA.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Imagine the world is struck by a devastating pandemic (even need to imagine?). The pandemic is taking the lives of people across the world and has put the world to a halt. People are eagerly waiting for a vaccine and scientists are working hard to make one. Now, making a vaccine isn’t an easy business - after doing a lot of research you find that different people react differently to the vaccine and a vaccine might have severe after-effects on a small group of people depending upon several differentiating factors among the people. How to classify these different people which have a specific effect after taking the vaccine based on a much fewer number of factors? That is, how to reduce the number of variables differentiating these people without losing information provided by other variables?&lt;/p&gt;

&lt;h2&gt;INTRODUCTION&lt;/h2&gt;

&lt;p&gt;Linear Discriminant Analysis is what we call a dimensionality reduction technique. Given all the factors that you have and all the information they contain, you try to squeeze that information into as few dimensions or variables as you can. 
Take a multiple regression model with 5 variables as an example. The model squeezes all the info in the factors into a single variable, i.e. the predicted y-value. It is an example of dimensionality reduction as it reduces multiple factors into a single variable without losing any substantial information.
Linear Discriminant Analysis does something very similar, but with a different objective. In regression, we are concerned with predicting the dependent variable with as much accuracy as possible. Whereas, LDA reduces the dimensions in such a way that it becomes easy to classify the dependent variable. In rough terms, it tries to predict a categorical dependent variable.
And it is this very feature of LDA, that makes it indispensable for the vaccine problem discussed in the beginning. But how exactly does LDA reduce the dimensions? Let’s find out.&lt;/p&gt;

&lt;h2&gt;INTUITION&lt;/h2&gt;

&lt;p&gt;Let’s start with a basic example having two independent variables X1 and X2. Using these two variables, we want to predict if the dot will be red or blue.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Linear-Discriminant-Analysis/1.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One way to reduce dimensions, in this case, is to project the values on either of the axes, i.e., you reduce the dimension by completely ignoring one of them. Needless to say, this is not an efficient way as we lose substantial amounts of data in the process. So, what we do is create a new axis using both the variables available to us and project the values onto that.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Linear-Discriminant-Analysis/2.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It can be clearly seen that it is now relatively easier to predict the color of the dots. Roughly speaking, those values in the lower half of the line are blue and other red. But how do we form this axis? In regression models, we form the axis (line of best fit) using the ordinary least squares method. Let’s see how the same is achieved in LDA.&lt;/p&gt;

&lt;h2&gt;FORMULA&lt;/h2&gt;

&lt;p&gt;1.) The required line is the one that maximizes the following amount.&lt;/p&gt;

&lt;p&gt;Here, μ represents the mean of the values in the respective categories (in our case, arithmetic mean for red and blue dots) and s squared represents the respective variations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Linear-Discriminant-Analysis/3.jpeg&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align:center; font-size:18px;&quot;&gt;via StatQuest&lt;/p&gt;

&lt;p&gt;2.) So basically, we try to separate the classes by maximizing the square of the distance between the means, and we try to improve the accuracy of the model by minimizing the “scatter” within individual categories.
One thing to keep in mind is that this is a basic formula where we only have two categories and two dimensions, to begin with. For more dimensions and categories, the formulas start expanding and become more complicated. But that is a story for another time…&lt;/p&gt;

&lt;h2&gt;CONCLUSION&lt;/h2&gt;

&lt;p&gt;Hope you got to know something about Linear Discriminant Analysis. It helps one separate different classes of objects, things, people, etc. based on their differentiators by reducing them. This can also be done by Principal Component Analysis covered by us &lt;a href=&quot;https://theanalyticsbay.com/blog/Principal-Component-Analysis/&quot; style=&quot;color:#a0f1ff&quot;&gt;&lt;u&gt;here&lt;/u&gt;&lt;/a&gt;, but Linear Discriminant Analysis focuses on maximum separability of the objects to be classified and hence is more accurate.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Conjoint Analysis</title><link href="http://localhost:4000/blog/Conjoint-Analysis/" rel="alternate" type="text/html" title="Conjoint Analysis" /><published>2021-06-17T00:00:00+05:30</published><updated>2021-06-17T00:00:00+05:30</updated><id>http://localhost:4000/blog/Conjoint-Analysis</id><content type="html" xml:base="http://localhost:4000/blog/Conjoint-Analysis/">&lt;p&gt;&lt;img src=&quot;/blog/Conjoint%20Analysis/Conjoint.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Conjoint analysis is the optimal market research approach for measuring the value that consumers place in features of a product or service.
It is used to determine the value consumers associate with different features of a product. In the real world, we make a lot of buying decisions and choose out of hundreds of varieties of a product available. These buying decisions are guided by the value we associate with various features of that product.
Conjoint analysis involves conducting surveys and recording their responses. These surveys should include various characteristics of a particular product. It may also involve associating numeric values or rankings to various features of the product. The analysis of the responses gives us the ability to peek into the mind of the target audience and see what they value the most. The information gathered acts as a decision-maker.
For example, if one go to buy a phone there are many factors one would like to consider. For simplicity let’s consider only 6 factors- price, screen size, looks, storage, battery and camera, which would impact our decision.
Now we survey 100 consumers about what would they prefer the most among these 6 features. The following are the results:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Conjoint%20Analysis/Consumer_Pref.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now using this data, a particular company can manufacture products keeping in mind the consumer preferences. However, conjoint analysis is a complex phenomenon it also involves series of permutations and combinations to determine the results. Using the above results, we can see that Battery and Screen size are the least looked at aspects while buying a phone and camera and price the most noticed aspect. Therefore, the company can save cost using average quality and specifications of battery and screen size and invest them in a good quality camera and reduce the price.&lt;/p&gt;

&lt;h3&gt;Types of Conjoint Analysis&lt;/h3&gt;

&lt;p&gt;There are three main types of conjoint analysis: Choice-based Conjoint (CBC) Analysis and Adaptive Conjoint Analysis (ACA) and Maxdiff Conjoint Analysis&lt;/p&gt;

&lt;h3&gt;Choice-based Conjoint (CBC) Analysis:&lt;/h3&gt;

&lt;p&gt;This type of conjoint analysis is the most popular because it asks consumers to imitate the real market’s purchasing behavior - which products they might choose, given specific criteria on price and features.
For example, each product or service has a specific set of defining characters. Some of these characters might be almost like one another or will differ. For example, you might present the respondents with the following choice:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Conjoint%20Analysis/Device.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The devices are almost similar, but device 2 has triple cameras with better configuration, and Device 1 has a higher battery power than Device 2. This helps in knowing the vital trade-off between the number of cameras and battery capacity based on analysis of received responses.&lt;/p&gt;

&lt;h3&gt;Adaptive conjoint analysis (ACA):&lt;/h3&gt;
&lt;p&gt;This type of conjoint analysis is often used in scenarios where the number of attributes/features exceeds what can be done in a choice-based scenario. ACA is suitable for product design and segmentation research, but not for determining the ideal price.
The adaptive conjoint analysis is a graded-pair comparison task, where the respondents are asked to assess their relative preferences between groups of attributes.  Each pair is evaluated thereafter on a predefined scale.&lt;/p&gt;

&lt;p&gt;For example, a respondent might be asked to choose between the following two concepts:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Conjoint%20Analysis/ACA.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The answers are used to determine the respondent’s part-worths of each of the attribute levels.  Once part-worths have been determined, the respondent’s overall preference for a given product can be estimated by summing the part-worths of each attribute level that describes that product.&lt;/p&gt;
&lt;h3&gt;Max-Diff Conjoint Analysis:&lt;/h3&gt;
&lt;p&gt;The max-Diff conjoint analysis shows a variety of packages to be selected under best/most preferred and worst/least preferred scenarios. Respondents can quickly indicate the best and worst items in a list, but struggle to decipher their feelings for the ‘middle ground’. Max-Diff is an easier task to undertake when consumers are well trained at making comparative judgments.
Max-Diff conjoint analysis is an ideal method when the decision task is to evaluate product choice. An experimental design is used to balance and properly represent the sets of items. Several methods can be taken with analyzing Max-Diff studies, including Hierarchical Bayes conjoint analysis to derive utility score estimates, best/worst counting analysis, and TURF analysis.&lt;/p&gt;

&lt;p&gt;Below is an example, involving a set of four attributes where the respondents can be asked to indicate the attributes that are the most/ least important to them:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Conjoint%20Analysis/MaxDiff.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Conclusion:&lt;/h3&gt;
&lt;p&gt;In conclusion, Conjoint Analysis helps in discovering the relative importance of the attributes of a product to the consumers. It is a marketing tool that is gaining momentum and is being used by product developers all over the world.&lt;/p&gt;

&lt;p&gt;Hopefully, this blog has enlightened about Conjoint Analysis and it’s different types.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Conjoint analysis is the optimal market research approach for measuring the value that consumers place in features of a product or service. It is used to determine the value consumers associate with different features of a product. In the real world, we make a lot of buying decisions and choose out of hundreds of varieties of a product available. These buying decisions are guided by the value we associate with various features of that product. Conjoint analysis involves conducting surveys and recording their responses. These surveys should include various characteristics of a particular product. It may also involve associating numeric values or rankings to various features of the product. The analysis of the responses gives us the ability to peek into the mind of the target audience and see what they value the most. The information gathered acts as a decision-maker. For example, if one go to buy a phone there are many factors one would like to consider. For simplicity let’s consider only 6 factors- price, screen size, looks, storage, battery and camera, which would impact our decision. Now we survey 100 consumers about what would they prefer the most among these 6 features. The following are the results:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Fibonacci and The Golden Ratio</title><link href="http://localhost:4000/blog/Fibonacci/" rel="alternate" type="text/html" title="Fibonacci and The Golden Ratio" /><published>2021-06-02T00:00:00+05:30</published><updated>2021-06-02T00:00:00+05:30</updated><id>http://localhost:4000/blog/Fibonacci</id><content type="html" xml:base="http://localhost:4000/blog/Fibonacci/">&lt;p&gt;&lt;img src=&quot;/blog/Fibonacci/Cover.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Maths can be fun, right? In this article we’ll be going through one of the more popular as well as enthralling concepts from the world of numbers: Fibonacci Numbers.&lt;/p&gt;

&lt;p&gt;Any person who has ever studied maths will be acquainted with the Fibonacci numbers or the Fibonacci sequence, introduced by Leonardo of Pisa in his 1202 book Liber Abaci. Well, to sum up, the Fibonacci sequence is made of numbers where each number is a sum of two preceding numbers in the sequence, beginning from 0 and 1. Thus, the first fifteen numbers in the sequence are:&lt;/p&gt;

&lt;p&gt;0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377…. and so on.&lt;/p&gt;

&lt;p&gt;Here, the 6th term, 5 is a sum of 2 and 3, 89 results from the addition of 34 and 55, and 233 from 89 and 144. The sequence obviously stretches to infinity. However, an interesting observation arises from the Fibonacci sequence, when we divide any number in the series from the previous number, we increasingly tend towards the ratio 1.618, as seen in the image below. This is the Golden Ratio, also known as Phi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Fibonacci/1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Golden Ratio also manifests in the following spiral:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Fibonacci/2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Image Credit: theincredibletruths.com&lt;/p&gt;

&lt;p&gt;The appearance and applications of Fibonacci numbers and the Golden Ratio are plenty and baffling, ranging from nature and biology to algorithms and even the stock market. Here, we’ll discuss some of these intriguing aspects of the two.
Fibonacci in Nature, Space and Biology&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Fibonacci/3.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Image Credit: theincredibletruths.com&lt;/p&gt;

&lt;p&gt;As evident from the above image, petals of many flowers like roses, sunflowers, and lilies follow a pattern very similar to the one formed by the Golden Ratio. This phenomenon is also found in biology, especially human anatomy.&lt;/p&gt;

&lt;p&gt;There are many instances of the Golden Ratio in the human body, and many people even estimate the number to be as high as 300! Like, the length of your palm to your arm is approximately 1.618. The spiral also appears quite frequently in many works of art, particularly in those of Leonardo Da Vinci.&lt;/p&gt;

&lt;p&gt;Outer space also encompasses the Golden Ratio in many forms and methods, like the spiral of a galaxy. Or that the ratio of diameter of Saturn’s rings to the diameter of the planet itself is very close to the Golden Ratio.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Fibonacci/4.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Image Credit: unimelb.edu.au&lt;/p&gt;

&lt;p&gt;A more acute example of the Golden Ratio in space would be found in our Solar System itself. The period it takes for many planets to revolve around the sun on their elliptical paths corresponds very closely to the Golden Ratio (and the use of exponents).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Fibonacci/5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fibonacci Retracements
One application of the Fibonacci series appears in the stock market, specifically in technical analysis of securities. Consistency is a prominent feature of the series, and it is also visible when we divide one term with its succeeding numbers in the series.&lt;/p&gt;

&lt;p&gt;For example,
Dividing one number by the following number, 13/21 = 0.618
If we skip over one number in the series, we get 13/34 = 0.382
Skipping over two numbers gets us 13/55 = 0.236&lt;/p&gt;

&lt;p&gt;This holds true for all numbers in the series, and when expressed as percentages they are 61.8%, 38.2% and 23.6% which form the levels of Fibonacci Retracements. But what exactly is Fibonacci analysis or retracement? Whenever a stock moves in any direction, upwards or downwards, usually it tends to retract before its next movement. For example, if the price of a security moves from Rs 100 to Rs 150, it’s expected that the price will retrace to Rs 120 before moving upwards again to a higher level. The Fibonacci Ratios, i.e 61.8%, 38.2% and 23.6% helps traders determine the level of retracement, and also serve as indicators to enter a new position or exit a loss-making one.&lt;/p&gt;

&lt;p&gt;Let’s understand Fibonacci Retracement through the help of the TCS share price over the last 6 months.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Fibonacci/6.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Image Credit: moneycontrol.com&lt;/p&gt;

&lt;p&gt;We can calculate the retracement levels manually, but most technical analysis tools enable us to draw Fibonacci levels automatically. Fibonacci levels are drawn when the stock is in an uptrend or downtrend, between a trough and a peak. Here, the price on 2nd Nov, i.e, Rs 2592 has been taken as the trough, and point A (Rs 3353) is the peak. Notice how the stock movements correspond with the Fibonacci levels before it reaches the peak. From there, the price retreats to point B, which is 38.2% below the peak, then rises and again retraces to C 61.8% reduction. It moves onto D the 38.2% and then moves towards the peak E. From there it retraces to F and this cycle continues.&lt;/p&gt;

&lt;p&gt;So, what does it mean for the trader? How do the retracement levels affect the trading strategies? This chart has multiple implications:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;If a trader has a significant position at point A and sees that the stock price is falling, they can place the stop-loss at point B because the stock can fall further if it crosses that level, and it did. This function serves to cut down his losses.&lt;/li&gt;
  &lt;li&gt;At point C when the stock starts rallying and a trader misses this opportunity, they can enter into the market at point D, because the stock crosses the retracement line and the price is expected to rise further.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thus, the retracement levels enable us to determine targets and stop-loss for our trades and contribute to the construction of support and resistance lines, one of the fundamental concepts of technical analysis. However, Fibonacci Retracement shouldn’t be the only factor in determining trades, but it acts as an indicator to support trading strategies and should be taken with a grain of salt.
Conclusion
The Fibonacci number has vast applications, in many many fields. Some of the are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It is used in the algorithm for a polyphase merge sort, which divides the list in the proportion of the Golden Ratio. It is also used for one-dimensional searching algorithms.&lt;/li&gt;
  &lt;li&gt;Random number generators also use a form of the Fibonacci sequence.&lt;/li&gt;
  &lt;li&gt;Used in Planning Poker, a game used to estimate the duration of software development.&lt;/li&gt;
  &lt;li&gt;Fibonacci series plays a key role in the formation of the Brock-Mirman economic growth model.&lt;/li&gt;
  &lt;li&gt;Mario Merz, a 20th-century artist, used the Golden Circle in his artworks in the 1970s.&lt;/li&gt;
  &lt;li&gt;Fibonacci sequence features prominently in musical composition, popularised by Joseph Schillinger.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The purpose of the article was to establish the fact that Fibonacci sequence isn’t some trivial mathematical concept spoken of casually; but that its applications are everywhere, not always prominent but subtle in nature.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>