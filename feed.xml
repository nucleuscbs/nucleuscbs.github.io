<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-11-29T16:44:55+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">The Analytics Bay</title><subtitle>Always Analyzing</subtitle><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><entry><title type="html">Ethics in Data Science</title><link href="http://localhost:4000/blog/Ethics-in-Data-Science/" rel="alternate" type="text/html" title="Ethics in Data Science" /><published>2021-11-29T00:00:00+05:30</published><updated>2021-11-29T00:00:00+05:30</updated><id>http://localhost:4000/blog/Ethics-in-Data-Science</id><content type="html" xml:base="http://localhost:4000/blog/Ethics-in-Data-Science/">&lt;p&gt;&lt;img src=&quot;/blog/EthicsDS.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;“With Great power comes greater responsibility,” as the saying goes. In today’s society, data is becoming as valuable as gold to large corporations. Where does data power come from, and what are the responsibilities for data scientists, in an era when all the big organisations compete to collect and preserve data and gather even more from other sources to get better insights into where data power comes from and what are the responsibilities for data scientists?&lt;/p&gt;

&lt;p&gt;Let’s start with a discussion about power. The economist published the following headline in May 2017: “The world’s most precious resource is no longer oil, but data.” The comparison of “data as the new oil” dates back to 2006, when Clive Humbey of Tesco in the United Kingdom stated that data is the new oil. It’s precious, but it can’t be utilised unless it’s polished, which is where you come in. Data is a really valuable resource, but it’s up to you to extract the value. Data has a lot of potential, and it’s up to you to realise it. As a data scientist, you hold a strong and privileged position, and your abilities are in great demand. Most individuals do not have access to education in these areas for a number of reasons, including the difficulty of teaching statistics and programming, as well as a scarcity of teachers.&lt;/p&gt;

&lt;p&gt;To do it, getting exposed to and encouraged to pursue skills in data science is rare, especially among certain groups such as women or rural populations; a master’s degree in data science is expensive and thus prohibitive for many, and we lack widespread statistical literacy to make the path to becoming a data scientist easier. Quantitative methods are frequently valued in Western society over observational reports, lived experience, or even rigorous qualitative analyses. This has historical roots in misogyny and colonialism when only a small percentage of people were thought to be intelligent enough to pursue logical subjects like math and statistics. Of course, science is important to society, and quantitative methods may teach us a lot, but they are overrated, even though many statistical conclusions are demonstrably false.&lt;/p&gt;

&lt;p&gt;Remember the big data myths of Boyd and Crawford? That way, you can respect authority and power just because you are a data scientist. He has a variety of talents, from programming to visualization to communication. Let’s talk about responsibilities. When you think about data science, you might think of a business model that optimizes advertising revenue. While enterprise data science is used in all possible areas, from marketing to medicine, transportation to waste management, data scientists may find it close to implementing real-world work. Models and analysis ultimately affect real life. This is a decision you make because it’s convenient and tidy, or because you don’t completely know what the model is doing. Yes, this will happen to you and will do a lot of damage.&lt;/p&gt;

&lt;p&gt;As a data scientist, it is our responsibility to think critically about algorithm design and, in the event of ambiguity, to communicate the algorithm’s role to the general public. This is known as context-sensitive data science, and it attempts to assist rather than replace the design of the job. Because the model and analysis are handy and neat, or because we don’t totally understand what the model is doing, they will eventually influence the judgments we make. Yes, it will be really harmful to you. Let’s look at an example of a not-so-obvious error in an algorithm that snowballs into unjustly affecting a large number of people risk assessment scores for determining whether or not someone is likely to commit a crime.This isn’t science fiction this is real life risk assessment scores which will be used in the criminal justice system today because people tend to see numbers and statistics as more objective.&lt;/p&gt;

&lt;p&gt;Our job as a data scientist is to  think critically about algorithm design and teach the general public how algorithms work. If in doubt, ask model stakeholders to interfere with the design, be friendly, curious, and remain critical.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Cool Python Libraries</title><link href="http://localhost:4000/blog/Cool-Python-Libraries/" rel="alternate" type="text/html" title="Cool Python Libraries" /><published>2021-11-11T00:00:00+05:30</published><updated>2021-11-11T00:00:00+05:30</updated><id>http://localhost:4000/blog/Cool-Python-Libraries</id><content type="html" xml:base="http://localhost:4000/blog/Cool-Python-Libraries/">&lt;p&gt;&lt;img src=&quot;/blog/Python-Libraries/1.png&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;Features of Python&lt;/h3&gt;

&lt;p&gt;&lt;b&gt;1) Portable:&lt;/b&gt; Python can run on a wide variety of platforms i.e. you can write your script locally on your system and distribute it to other machines with the help of an interpreter.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;2) Scalable:&lt;/b&gt; Python is scalable, that is, it can be  implemented for complex tasks, that includes compiling large amounts of data and executing ML algorithms.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;3) Databases:&lt;/b&gt; Python maintains a different list of databases like SQLite, Oracle, MySQL, PostgreSQL,etc.&lt;/p&gt;

&lt;p&gt;There are some interesting libraries in Python. In this article, some of them are discussed which will come in handy no matter if you are a beginner or a professional.&lt;/p&gt;

&lt;h3&gt;1. Emoji&lt;/h3&gt;

&lt;p&gt;Emojis have become a fancy way of expression that can enhance and comprehend the mood of simple boring texts. Now, the same can be used in Python programs too. Yes, really! You now have a magic code to use emojis in python. For this, emoji module needs to be installed.
Use the following command in terminal:&lt;/p&gt;

&lt;p&gt;pip install emoji&lt;/p&gt;

&lt;p&gt;To upgrade to the latest packages of emojis, use the following command:&lt;/p&gt;

&lt;p&gt;pip install emoji –upgrade
&lt;img src=&quot;/blog/Python-Libraries/2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can use the emoji cheat sheet to find your favorite emoji.&lt;/p&gt;

&lt;p&gt;encode() function can also be used from emojis module to convert unicode to emojis
&lt;img src=&quot;/blog/Python-Libraries/3.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;2. Wikipedia&lt;/h3&gt;

&lt;p&gt;With this module, we can now import the entire Wikipedia! Yes, We can now import Wikipedia in Python using the Wikipedia module. 
Install it as:&lt;/p&gt;

&lt;p&gt;pip install wikipedia
&lt;img src=&quot;/blog/Python-Libraries/4.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;3. Antigravity&lt;/h3&gt;

&lt;p&gt;It’s basically used in Google App Engines. It was added to Google App Engines just as a source to amuse and entertain the users.
Install it with:&lt;/p&gt;

&lt;p&gt;pip install antigravity&lt;/p&gt;

&lt;p&gt;import antigravity&lt;/p&gt;

&lt;p&gt;This opens up a page in your web browser which contains a comical abstract of Python. Woah!!&lt;/p&gt;

&lt;p&gt;This can be done as follows:
&lt;img src=&quot;/blog/Python-Libraries/5.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;4. Urllib&lt;/h3&gt;

&lt;p&gt;Urllib library is the URL (Uniform Resource Locators) handling library for python that is useful in fetching URLs using a variety of different protocols. For this, it makes use of urlopen function. Urllib is a package that collects several libraries for working with URLs, such as:&lt;/p&gt;

&lt;p&gt;● urllib.request for opening and reading.&lt;/p&gt;

&lt;p&gt;● urllib.parse for parsing URLs&lt;/p&gt;

&lt;p&gt;● urllib.error for the exceptions raised&lt;/p&gt;

&lt;p&gt;● urllib.robotparser for parsing robot.txt files&lt;/p&gt;

&lt;p&gt;You can also see the coding of the website by using read() function:,,
&lt;img src=&quot;/blog/Python-Libraries/6.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;5. Turtle&lt;/h3&gt;

&lt;p&gt;So in python, a turtle can be imported but don’t worry it’s not slow. Turtle is a Python library that is used to draw. It has various applications. This library is built-in with Python so we  need not install it.
&lt;img src=&quot;/blog/Python-Libraries/7.png&quot; /&gt;
&lt;img src=&quot;/blog/Python-Libraries/8.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;6. Gramformer&lt;/h3&gt;

&lt;p&gt;Yet another impressive and interesting module is Gramformer, which  is based on generative models helping us to correct the grammar in the sentences. This library has three tools which have a detector,0 a highlighter, and a corrector. The detector identifies if the text has incorrect grammar or not. The highlighter marks the faulty parts of speech and the corrector fixes the errors. One limitation of Gramformer is that it isn’t suitable for long paragraphs as it works only at a sentence level and has a limit for 64 length sentences.&lt;/p&gt;

&lt;p&gt;Installation and instantiatiation of  Gramformer
&lt;img src=&quot;/blog/Python-Libraries/9.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;An example that can demonstrate the above process is giving sample text for correction under gf.correct
&lt;img src=&quot;/blog/Python-Libraries/10.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the above output, we can see it corrects grammar and even spelling mistakes. 
&lt;img src=&quot;/blog/Python-Libraries/11.png&quot; /&gt;
&lt;img src=&quot;/blog/Python-Libraries/12.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;This article covers all the cool Python libraries and is extremely helpful in making complex projects. These libraries are easy to use and install. They even offer a variety of modules and libraries by which coding becomes extremely easy. Thus, Python offers an advantage to access these libraries with utmost ease and make the coding fun learning and seamless.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Features of Python</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Getting Started with Support Vector Machine</title><link href="http://localhost:4000/blog/Support-Vector-Machine/" rel="alternate" type="text/html" title="Getting Started with Support Vector Machine" /><published>2021-10-29T00:00:00+05:30</published><updated>2021-10-29T00:00:00+05:30</updated><id>http://localhost:4000/blog/Support-Vector-Machine</id><content type="html" xml:base="http://localhost:4000/blog/Support-Vector-Machine/">&lt;p&gt;Every individual interested in data analysis and data science usually goes through a stage, where they are astonished by the staggering vastness of the field. The plethora of tools and methods, algorithms and concepts overwhelms us as we move on from the typical methods of analysis and predictions. At The Analytics Bay we try to break down new concepts for easier understanding, and in this article we focus on Support Vector Machine which is commonly known as SVM.&lt;/p&gt;

&lt;p&gt;A very popular analogy to understand the difference between SVM and other models using Regression is: Regression is a sword which is used bluntly and is applicable for large but simpler data. SVM is a carefully crafted knife and provides precision in predictions that other algorithms cannot. SVM is built to work on smaller, but highly complex data used in the creation of stronger, more efficient models.&lt;/p&gt;

&lt;h3&gt;So, what is SVM?&lt;/h3&gt;

&lt;p&gt;Support Vector Machine, or SVM, is a supervised ML algorithm useful for both regression and classification problems. However, it is primarily used for classification. In SVM, each data point is plotted in n-dimensional space (n representing the number of features available in the data), the coordinates of the point being the value of the feature. Classification is performed when we construct a hyper-plane to differentiate between the two classes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/SVM/1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, multiple planes could be created to classify the data points. The most efficient plane is the one with the maximum margin, that is the maximum distance from both the data points. This is called the optimal hyperplane.&lt;/p&gt;

&lt;p&gt;But what about datasets with more than two features? Hyperplanes are just boundaries between the two classes, to separate the two data points. If input has two features, the plane is a line. But if it is 3, the hyperplane is a two-dimensional plane. As the number of features increase, the hyperplane becomes more complex and difficult to visualize.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/SVM/2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we over simplify things, the construction of a linear hyperplane between two classes is quite easy. However, it leads to another urgent dilemma: Do we need to prove this feature manually for the creation of a hyperplane? No, the SVM  algorithm utilizes a brilliant method called the kernel trick. The SVM kernel is a function that takes low dimensional input space and transforms it to a higher dimensional space i.e. it converts non separable problem to separable problem. The kernels are mostly used to solve non-linear separation problems. To put into simpler terms, we leave it to the SVM model and its kerns to perform complex data transformations and come up with a process to separate the various classes using the outputs and labels defined by the user.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/SVM/3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, SVM uses a circular plane to classify the data points. SVM uses an additional feature using the formula &lt;b&gt;z= x^2+y^2&lt;/b&gt; to come up with more appropriate data points. Plotting x axis and z axis would be something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/SVM/4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Therefore, we understand that SVM is not a one-trick pony, but can adapt itself to multiple complex forms of data for better classification.&lt;/p&gt;

&lt;h3&gt;Merits and Demerits of using SVM&lt;/h3&gt;

&lt;h3&gt;Merits&lt;/h3&gt;

&lt;p&gt;1) SVM works perfectly for data that has a clear margin of separation.&lt;/p&gt;

&lt;p&gt;2) Works well with multi-dimensional data.&lt;/p&gt;

&lt;p&gt;3) It uses a smaller set of data to determine the decision function (i.e support vectors) and hence is memory effective.&lt;/p&gt;

&lt;h3&gt;Demerits&lt;/h3&gt;

&lt;p&gt;1) SVM can be time consuming, hence is not the most suited for large data sets.&lt;/p&gt;

&lt;p&gt;2) The efficiency decreases when we have overlapping data points, i.e. the data is noisy.&lt;/p&gt;

&lt;p&gt;3) It doesn’t give the probability estimates, which have to be calculated using another, rather expensive method.&lt;/p&gt;

&lt;h3&gt;SVM in Python&lt;/h3&gt;

&lt;p&gt;Here, we’ll look at how to implement SVM in Python on the Iris dataset using the scikit-learn package.&lt;/p&gt;

&lt;p&gt;&lt;i&gt;import numpy as np&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;import matplotlib.pyplot as plt&lt;/p&gt;

&lt;p&gt;from sklearn import svm, datasets&lt;/p&gt;

&lt;p&gt;&lt;b&gt;import some data to play with&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;iris = datasets.load_iris()&lt;/p&gt;

&lt;p&gt;X = iris.data[:, :2] # we only take the first two features. We could&lt;/p&gt;

&lt;p&gt;&lt;b&gt;avoid this ugly slicing by using a two-dim dataset&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;y = iris.target&lt;/p&gt;

&lt;p&gt;&lt;b&gt;we create an instance of SVM and fit out data. We do not scale our data since we want to plot the support vectors&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;C = 1.0 # SVM regularization parameter&lt;/p&gt;

&lt;p&gt;svc = svm.SVC(kernel=’linear’, C=1,gamma=0).fit(X, y)&amp;lt;/i&amp;gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/SVM/5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Plotting the kernel gives us this result, where a linear hyperplane is used to separate the data points. We can even use ‘rbf’ or ‘poly’ for non linear hyperplanes, however there is a risk of overfitting.&lt;/p&gt;

&lt;p&gt;&lt;i&gt;svc = svm.SVC(kernel=’rbf’, C=1,gamma=0).fit(X, y)&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/SVM/6.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The SVM library has multiple parameters that determine the construction of the planes, and many of them can be altered according to the dataset. Do give this a try and play around with them to better understand Support Vector Machines.&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Thus, we can conclude that Support Vector Machine is an amazing tool for a data scientist. In this article, we understood the conceptual working of the SVM and looked at how it segregates and classifies data points.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Every individual interested in data analysis and data science usually goes through a stage, where they are astonished by the staggering vastness of the field. The plethora of tools and methods, algorithms and concepts overwhelms us as we move on from the typical methods of analysis and predictions. At The Analytics Bay we try to break down new concepts for easier understanding, and in this article we focus on Support Vector Machine which is commonly known as SVM.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Law of Large Numbers</title><link href="http://localhost:4000/blog/Law-of-Large-Numbers/" rel="alternate" type="text/html" title="Law of Large Numbers" /><published>2021-10-14T00:00:00+05:30</published><updated>2021-10-14T00:00:00+05:30</updated><id>http://localhost:4000/blog/Law-of-Large-Numbers</id><content type="html" xml:base="http://localhost:4000/blog/Law-of-Large-Numbers/">&lt;p&gt;&lt;img src=&quot;/blog/2021-10-14-Law-of-Large-Numbers/Cover.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Law of Large Numbers is a concept of probability used in statistics which states that as the size of a sample grows or rises, its mean gets closer and closer to the average or mean of the whole population. This law in the financial context has a whole different connotation or inference, which is related to the growth rate.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2021-10-14-Law-of-Large-Numbers/1.jpeg&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;IN STATISTICS&lt;/h2&gt;
&lt;p&gt;The large numbers theorem in statistics proves that if the same study or experiment is repeated independently a great number of times, the average of the results of the experiments or trials will be close to the expected value. The average or mean of results becomes closer to the expected value as the frequency of trials increases.
The law of large numbers is an essential concept in statistics because it states that even random trials with a large number of trials may return stable, accurate, and long-term results. It is essential to understand that the average of the results of the experiment repeated a small number of times might give a substantially different value from that of the expected value. However, each additional trial increases the precision or accuracy of the average result.&lt;/p&gt;

&lt;h2&gt;COIN FLIPPING EXAMPLE&lt;/h2&gt;
&lt;p&gt;Now, let’s look at coin flips. This is a Bernoulli Trial as there are primarily two outcomes, heads and tails. The data are binary and follow the binomial distribution as provided by a proportion of events. For this scenario, an event as heads in the coin toss is defined. A coin toss is considered to be one trial. The law of large numbers proposes that when the frequency of trials increases, the proportion will converge and eventually coincide with the expected value of 0.50 (a head or a tail having a probability of 0.50 in each flip).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2021-10-14-Law-of-Large-Numbers/2.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The sample proportion becomes more stable and accurate. It converges with the expected probability value of 0.50 as the sample size is increased.&lt;/p&gt;

&lt;h2&gt;IN BUSINESS AND FINANCE TERMS&lt;/h2&gt;

&lt;p&gt;In business, the term “law of large numbers” is majorly used in relation to growth rates, in terms of a percentage. It suggests that, as a business grows, the percentage rate of growth becomes increasingly strenuous to maintain.&lt;/p&gt;

&lt;p&gt;The law of large numbers does not mean that a given sample or group of successive samples will always reflect the true and accurate population characteristics, true for especially small samples or a small number of results taken into consideration. It can also be inferred that if a given sample or series of samples drifts away from the true population mean or expected value, the law of large numbers does not strongly or firmly guarantee that successive samples will move the observed average toward the population mean (as provided by the Gambler’s Fallacy).
Basically, the concept of LLN in finance tells, that as a business grows, it gets hard to maintain the previous growth rates.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Practical Example&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2021-10-14-Law-of-Large-Numbers/3.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s assume there are two companies namely, ABC Ltd. and XYZ Ltd. The market capitalization of the companies ABC and XYZ are $1 million and $100 million respectively. Let’s further assume the growth of ABC is 50% for the current year, it is attainable as it will grow further by $500,000.
But the same is not possible for the company XYZ as it will have to grow by $50 million to attain the 50% growth rate point. Thus, as company XYZ Ltd. will continue to expand, the growth rate of the company will fall or decline.&lt;/p&gt;

&lt;h2&gt;TYPES OF LAW OF LARGE NUMBERS&lt;/h2&gt;

&lt;p&gt;There are two main parts of the law of large numbers. They are: the weak law and the strong law of large numbers. The difference between the two is mostly theoretical.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;THE WEAK LAW:&lt;/b&gt;
The mean of a sample gets nearer to, i.e., converges the population mean as the sample size grows bigger. This law is known as the Weak Law of Large Numbers or the Bienaymé–T Chebyshev Inequality.
In other words, The Weak Law of Large Numbers proves that the sample average value converges in probability towards the expected value of the population.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;THE STRONG LAW:&lt;/b&gt;
The strong law of large numbers (also called Kolmogorov’s law) tells that the sample average converges almost surely with the expected value of the population.&lt;/p&gt;

&lt;p&gt;The Strong Law of Large Numbers tells that when the sample size grows infinite times the average mean will converge to the one for sure.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;THE DIFFERENCE&lt;/b&gt; - Weak law of large numbers provides that it is a probability that the sample average will converge towards the expected value whereas Strong law of large numbers shows almost sure convergence. Weak law has a probability tending to 1 whereas Strong law has a probability equal to 1.&lt;/p&gt;

&lt;h2&gt;CONCLUSION&lt;/h2&gt;

&lt;p&gt;The law of large numbers plays an essential role because it guarantees accurate results from the averages of random events. For instance, while playing roulette, a casino may lose a sum of money in a single spin of the wheel, though its earnings will get closer to a predictable average value over a large number of spins. Any winning or losing streak by a player will certainly be overcome by the parameters of the game. It is integral to remember that the law of large numbers only applies when a large number of observations are taken into consideration. This principle does not work when a small number of observations is considered. Greater frequency of observations will eventually let the averages coincide with the expected value and that a streak of one value will immediately be balanced and compensated by the others.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Gambler’s Ruin</title><link href="http://localhost:4000/blog/Gamblers-Ruin/" rel="alternate" type="text/html" title="The Gambler’s Ruin" /><published>2021-09-30T00:00:00+05:30</published><updated>2021-09-30T00:00:00+05:30</updated><id>http://localhost:4000/blog/Gamblers-Ruin</id><content type="html" xml:base="http://localhost:4000/blog/Gamblers-Ruin/">&lt;p&gt;&lt;img src=&quot;/blog/2021-09-30-GamblersRuin/Cover.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Imagine that a reluctant gambler is dragged to a casino by his friends. He’s very conservative when it comes to gambling so he only takes $50 in the casino to gamble with. Since he doesn’t know a whole lot about gambling, he decides to play roulette. The gambler places a very simple bet of $25 on red. So with every spin, if red occurs he will win $25 but if black occurs he will lose $25. Therefore the odds of winning or losing are almost 50% each. A thing to notice is that in casinos, the odds are always slightly uneven because if they were even the casino would never make money. Casinos set up games that are slanted in their favor and so are the payouts. The gambler sets up some simple rules -  he decides that he will quit playing when he has either 0 money left or he is up by $25 i.e has $75. We can model this entire process as a Markov Chain and examine its long-term behavior. A Markov Chain is a model that helps in defining a sequence of all possible events where the probability of each event depends only on the state attained in the previous event. We realize that at any point in time, the only relevant information is the amount of money the gambler has available. How he got to that amount in the past is irrelevant. Firstly, we can set up a transition diagram to represent the possible evolution of this game:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2021-09-30-GamblersRuin/1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are 4 states in this transition diagram. The gambler is going to come into the casino with $50. There is a possibility that he loses all his money and goes broke (gets ruined) or he can end up winning up to $75 and then he stops and calls it a night. These two are the endpoints of our transition diagram.  Since he is betting $25 dollars at a time there is also a state that he has $25 left with him. Now at each step, if he actually has the money to make a bet, the chance of winning or losing is 0.5 so for example if he has $25 and he makes a bet the chance of winning and going to $50 is 0.5. Let’s go ahead and set up a matrix using this transition diagram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2021-09-30-GamblersRuin/2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the two ends we have what are called Absorbing States. An absorbing state is a state in which once you enter, there is no leaving. It’s almost like a black hole. Once the gambler is broke (or ruined), he’s always broke and once he wins $75 he quits and therefore always has that $75. So once he enters these absorbing states, he never leaves that state. In our transition matrix, on the left hand side we have the state we’re coming from and on the top we have the state we’re going to so you can see the one there in the top left that just means if he’s broke now he’ll be broke next time and on the lower right that just means if he had $75 he will always have $75 because he stopped playing and then the point five probabilities show the various transition stages so for example the probability of going from the $25 state to the broke state is 0.5. This matrix showcases the exact same information that is present in the transition diagram in matrix form.&lt;/p&gt;

&lt;p&gt;What do we find out when we analyze this matrix and look at some long-run probabilities of this game? Let’s start by taking the transition matrix 2 steps into the future.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2021-09-30-GamblersRuin/3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looking at the second row second column we get a 0.25 probability. This means that if our gambler starts with $25 when he walks into the casino and plays this game the probability of having the $25 in his pocket two spins of the game from now is 0.25 or 25%. We can even take this further and see what we would expect 10 spins into the game:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2021-09-30-GamblersRuin/4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we interpret the above matrix, we can see that after 10 spins of the wheel if he walks in with $25 the probability that he will be broke 10 spins from now is 0.667 but if we go down to the next row we see that if he comes in with $50 the chance of him being broke after 10 spins is 0.333. So we can see that the probability of being broke or ending up with $75 is dependent on the amount of money he started with as well.&lt;/p&gt;

&lt;p&gt;If we run this matrix into the future, we see that the probability of being broke if he comes in with $25 dollars is ⅔ but is ⅓ if he comes with $50. Similarly, the probability of having $75 if he comes in with $25 is 2/3 and is 2/3 if he comes with $50.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2021-09-30-GamblersRuin/5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It might be difficult to believe that, given a fair game, the probability that someone will win their desired amount or get ruined is determined by their initial wealth. Using Markov chains, we can determine the same probabilities between any sequences of games using the transition matrix. This concept has specific relevance towards gambling, however it is also used in various mathematical theorems with wide applications in probability and statistics.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Potato Paradox</title><link href="http://localhost:4000/blog/Potato-Paradox/" rel="alternate" type="text/html" title="The Potato Paradox" /><published>2021-09-16T00:00:00+05:30</published><updated>2021-09-16T00:00:00+05:30</updated><id>http://localhost:4000/blog/Potato-Paradox</id><content type="html" xml:base="http://localhost:4000/blog/Potato-Paradox/">&lt;p&gt;&lt;img src=&quot;/blog/PotatoParadox/1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Potato, one of the most commonly used vegetables in cooking, which we’ve long connected with for making us fat or being our favourite carbohydrate, actually has a perplexing paradox built into it. But potatoes have never been contradictory, have they? Wrong! THE POTATO PARADOX IS HERE.&lt;/p&gt;

&lt;p&gt;Assume you have 100 pounds of potatoes, and 99 % of their weight is water. The remaining 1% of their weight is made up of carbohydrates, proteins, pectins, and minerals. They dry out a little after being left out overnight. They’re only 98 % water when you wake up, thus just 2% of it is solid. So, how much does your sack of potatoes now weigh? Your sack of potatoes would now weigh 50 pounds after this minor adjustment. So, what causes this? Is it true that the potato went on a complete carb-free diet in one night? Maybe not, because they’d been killed doing it. Clearly not. Let’s delve into the math now to better grasp this.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/PotatoParadox/2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The 1% change in composition reduces the weight by HALF. How? Considering the case of Potatoes is the simplest method to visualise the solution. There are a hundred of them. Let’s use one painted potato to represent the 1% solid potato substance we started with. The water is represented by the remaining 99. The amount of solid stuff in the potatoes does not alter as they dry out. The water is the only thing that disappears. So, if we take away ONE unit of water, we’re left with 99 pounds of material, one of which is dry and the other 98 being water. So, let’s just perform some quick math here; 98.989899 % is 98.989899 %. Okay, that’s a lot of water.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/PotatoParadox/3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s just take out another pound of water, and we’ll have 97 of the 98 pounds here that are water. So we’ll go with 97 out of 98, which is 98.979591836 %. There’s simply too much water. There has to be a better way to do things.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/PotatoParadox/4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The issue is that this is decreasing extremely slowly since for every unit of water that evaporates, the total amount remaining decreases as well. Let’s take a look at the final result to see how many we need to remove. So, instead of 98 or 97, we need to cut our water supply to 49. What’s more, here’s why: So we’ve got 98 % water and 2% solids, but only one solid unit. Let’s change that 2 to a 1.&lt;/p&gt;

&lt;p&gt;So, we’ll divide 2 by 2 and get 1, and we’ll do the same thing with our 98, so we’ll divide that by 2 and get 49.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/PotatoParadox/5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So 49 plus 1 equals 50, and the solution is 50 pounds. So we have to get rid of a LOT of water potatoes. I’m not sure how many are left, but I believe there are 49 waters and one solid. And 98 % is equal to 49 divided by 50. It was a success! That is the solution.
The potato paradox comes into play every time there are two items and the concentration of one doubles. That requires the other one’s size to be reduced by half of the whole, whether it’s doubling from 1% or .00001%. Or 10%. Ask your friends to solve this problem and see what they say.&lt;/p&gt;

&lt;p&gt;Most of the time, our first response is to assume that not much has changed since 1% is so small. This isn’t something to be ashamed of – our brains evolved to compare quantities like this: there’s one wooly mammoth and there’s five of us or how much food do we need to keep the family surviving through the winter? Evaluating concentrations is more abstract and not usually a life-or-death issue that natural selection would play a role in shaping.&lt;/p&gt;

&lt;p&gt;Unlike other well-known paradoxes, such as time travel ones, the POTATO conundrum is a VERIDICAL paradox, meaning it has a TRUE solution that we can all agree on and verify, but it is nonetheless startling. So, the potato paradox is a type of paradox that isn’t based on misunderstandings, impossibilities, or speculation, but rather on a deep understanding of how the mind works.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">An Introduction To The Confusion Matrix</title><link href="http://localhost:4000/blog/Confusion-Matrix/" rel="alternate" type="text/html" title="An Introduction To The Confusion Matrix" /><published>2021-09-02T00:00:00+05:30</published><updated>2021-09-02T00:00:00+05:30</updated><id>http://localhost:4000/blog/Confusion-Matrix</id><content type="html" xml:base="http://localhost:4000/blog/Confusion-Matrix/">&lt;p&gt;A confusion matrix is a table of four different combinations of actual and the predicted values and is an important step in calculating the recall, precision, and accuracy of the values, as well as AUC-ROC curves.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/ConfusionMatrix/1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;●	The target variable is binary, that is, either Positive or Negative&lt;/p&gt;

&lt;p&gt;●	Actual values of the target variable are represented by the columns&lt;/p&gt;

&lt;p&gt;●	Predicted values of the target variable are shown in the rows of the matrix&lt;/p&gt;

&lt;p&gt;The matrix has been divided into four parts:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;True Positive&lt;/li&gt;
  &lt;li&gt;True Negative&lt;/li&gt;
  &lt;li&gt;False Positive&lt;/li&gt;
  &lt;li&gt;False Negative&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;True Positive (TP) &lt;/h2&gt;

&lt;p&gt;●	The predicted value matches the actual value&lt;/p&gt;

&lt;p&gt;●	The actual outcome was positive and the classifier predicted positive too&lt;/p&gt;

&lt;h2&gt;True Negative (TN) &lt;/h2&gt;

&lt;p&gt;●	The predicted value matches the actual value&lt;/p&gt;

&lt;p&gt;●	The actual outcome was negative and the model’s prediction was negative as well&lt;/p&gt;

&lt;h2&gt;False Positive (FP) – Type 1 error&lt;/h2&gt;

&lt;p&gt;●	The predicted value was falsely predicted&lt;/p&gt;

&lt;p&gt;●	The model predicted a positive value but the actual value was negative&lt;/p&gt;

&lt;p&gt;●	Also known as the Type 1 error&lt;/p&gt;

&lt;h2&gt;False Negative (FN) – Type 2 error&lt;/h2&gt;

&lt;p&gt;●	The predicted value was falsely predicted&lt;/p&gt;

&lt;p&gt;●	The model predicted a positive value but the actual outcome was positive&lt;/p&gt;

&lt;p&gt;●	Also known as the Type 2 error&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/ConfusionMatrix/2.png&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;How do we use the Matrix?&lt;/h2&gt;

&lt;p&gt;The confusion matrix helps in determining values that explain the results of the classifier through certain metrics to improve our understanding of its performance. These are:&lt;/p&gt;

&lt;h2&gt;Precision &lt;/h2&gt;
&lt;p&gt;Precision is the ratio of correct positive predictions to the total of all positive outcomes.&lt;/p&gt;

&lt;p&gt;It is also called Positive predictive value.&lt;/p&gt;

&lt;p&gt;Precision = TP/(TP+FP)&lt;/p&gt;

&lt;h2&gt;Recall&lt;/h2&gt;
&lt;p&gt;Recall is the ratio of the correct positive results to the total positive predictions.&lt;/p&gt;

&lt;p&gt;It is also called Sensitivity, Probability of Detection, True Positive Rate.&lt;/p&gt;

&lt;p&gt;Recall= TP/(TP+FN)&lt;/p&gt;

&lt;h2&gt;Accuracy&lt;/h2&gt;

&lt;p&gt;Accuracy is defined as the ratio of correct predictions by the total predictions.&lt;/p&gt;

&lt;p&gt;Accuracy = Correct Predictions/ Total Predictions&lt;/p&gt;

&lt;p&gt;In a confusion matrix, it can be derived using:&lt;/p&gt;

&lt;p&gt;Accuracy = (TP+TN)/(TP+TN+FP+FN)&lt;/p&gt;

&lt;p&gt;Accuracy is a handy metric for evaluation when all the classes are of equal importance. But this might not be the case if we are predicting if a patient has a fatal diagnosis or not. Here, False Positives are acceptable, but False Negatives are not.&lt;/p&gt;

&lt;h2&gt;ROC curve&lt;/h2&gt;

&lt;p&gt;A ROC curve (receiver operating characteristic curve) graphs the performance of a classification model at all classification thresholds.
(Using thresholds: Say, if you want to compute TPR and FPR for the threshold equal to 0.6, you apply the model to each example, get the score, and, if the score &amp;gt;=0.6, you predict the positive class; otherwise, the prediction is negative)&lt;/p&gt;

&lt;p&gt;It plots 2 parameters:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;True positive rate&lt;/b&gt; (Recall)= TP/(TP+FN)&lt;/p&gt;

&lt;p&gt;&lt;b&gt;False Positive rate&lt;/b&gt;= FP/(FP+TN)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/ConfusionMatrix/3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lowering the threshold predicts more items as positive, thus increasing both False Positives and True Positives in the outcome.&lt;/p&gt;

&lt;h2&gt;AUC&lt;/h2&gt;

&lt;p&gt;AUC stands for &lt;b&gt;Area under the ROC&lt;/b&gt; Curve. It provides an average measure of performance across all possible probability thresholds of the results.&lt;/p&gt;

&lt;p&gt;The higher the area under the ROC curve (AUC), the better the model and the more efficient the classifier. A perfect model would have an area of 1. Usually, if your model is efficient, you obtain a good performance class by selecting the value of the threshold whose TPR tends to 1 while FPR inches closer to 0.&lt;/p&gt;

&lt;h2&gt;An Example of the Confusion Matrix&lt;/h2&gt;

&lt;p&gt;Suppose we have 165 patients being tested for Covid-19, and have gotten the following results from the tests:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/ConfusionMatrix/1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These are the most important metrics that we derive from the matrix:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Accuracy:&lt;/b&gt; Overall, how often is the test correctly predicting the patient’s diagnosis?&lt;/p&gt;

&lt;p&gt;(TP+TN)/Total = (100+50)/165 = &lt;b&gt;0.91&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Misclassification Rate:&lt;/b&gt; Overall, how often is it wrong?&lt;/p&gt;

&lt;p&gt;(FP+FN)/total = (10+5)/165 = &lt;b&gt;0.09&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;It is also equal to (1-Accuracy), and is also known as the “Error Rate”.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;True Positive Rate:&lt;/b&gt; When it’s actually positive, how often does it predict positive?&lt;/p&gt;

&lt;p&gt;TP/(TP+FP) = 100/105 = &lt;b&gt;0.95&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;We’ve discussed this earlier, also known as “Sensitivity” or “Recall”.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;False Positive Rate:&lt;/b&gt; When it’s actually negative, how often does the test predict positive?&lt;/p&gt;

&lt;p&gt;FP/(TN+FN) = 10/60 = &lt;b&gt;0.17&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;True Negative Rate:&lt;/b&gt; When it’s actually negative, how often does it give a negative outcome?&lt;/p&gt;

&lt;p&gt;TN/(TN+FP) = 50/60 = &lt;b&gt;0.83&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;True Negative Rate is equal to (1-False Positive Rate), and is called “Specificity”.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Precision:&lt;/b&gt; When the outcome is positive, how often is it correct?&lt;/p&gt;

&lt;p&gt;TP/predicted yes = 100/110 = &lt;b&gt;0.91&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Prevalence:&lt;/b&gt; How often does the test predict positive?&lt;/p&gt;

&lt;p&gt;TP+FN/Total = 105/165 = &lt;b&gt;0.64&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;These metrics help us in understanding the results better as well as the efficiency of the classification.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The Confusion Matrix and the metrics derived from it are really helpful in analyzing the data as well as the algorithm used for classification, like the following.&lt;/p&gt;

&lt;p&gt;●	ROCs define the trade-off between the TPR and FPR for a predictive model using different probability thresholds.&lt;/p&gt;

&lt;p&gt;●	Precision-Recall curves summarize the trade-off between the TPR and the positive prediction efficiency for a  model using different probability thresholds.&lt;/p&gt;

&lt;p&gt;●	ROCs are suitable when the observations are balanced between each class, whereas precision-recall curves are more suitable when the data is imbalanced.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">A confusion matrix is a table of four different combinations of actual and the predicted values and is an important step in calculating the recall, precision, and accuracy of the values, as well as AUC-ROC curves.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Gradient Descent</title><link href="http://localhost:4000/blog/Gradient-Descent/" rel="alternate" type="text/html" title="Gradient Descent" /><published>2021-08-19T00:00:00+05:30</published><updated>2021-08-19T00:00:00+05:30</updated><id>http://localhost:4000/blog/Gradient-Descent</id><content type="html" xml:base="http://localhost:4000/blog/Gradient-Descent/">&lt;p&gt;&lt;img src=&quot;/blog/GradientDescent/9.png&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Optimization is an important part of our life. We all have limited resources and time and we wish to make the most of them. From utilizing resources effectively to solving problems for an organization – everything uses optimization. Optimization is required everywhere whether you are working with a real-life problem or building a product.&lt;/p&gt;

&lt;p&gt;Optimization means getting the optimal solution for your problem.&lt;/p&gt;

&lt;p&gt;Optimization starts with very simple and basic problems, but it can get very complex sometimes. For example, allocating a monthly household budget is a simple optimization problem. On the other hand, devising various strategies for any organization company can be very complex.&lt;/p&gt;

&lt;p&gt;Linear regression is a simple optimization problem. The representation is a linear equation that uses a specific set of input values/training data values (x) and a predicted output value/test set values (y).&lt;/p&gt;

&lt;p&gt;So in machine learning, we perform optimization on the training data and check its performance on newly defined test data.&lt;/p&gt;

&lt;p&gt;Many popular machine algorithms depend upon optimization techniques such as linear regression, k-nearest neighbors, neural networks, etc. The applications of optimization are limitless and are a widely researched topic in both academia and industries.&lt;/p&gt;

&lt;p&gt;Gradient Descent is the most commonly used optimization technique when dealing with machine learning.&lt;/p&gt;

&lt;h2&gt;What is Gradient Descent?&lt;/h2&gt;

&lt;p&gt;It is an optimization algorithm to reduce the cost of the function. We start with any random point on the function since we are unaware of the direction where we will obtain the most optimal solution and move in the &lt;b&gt;opposite direction&lt;/b&gt; of the &lt;b&gt;gradient of the function&lt;/b&gt; to obtain the &lt;b&gt;local/global minima.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/GradientDescent/7.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To understand the distinction between local and global minima, let’s take a look at the figure above. The global minimum is the least value of any function while a local minimum is the least value of a function in a certain neighborhood.&lt;/p&gt;

&lt;p&gt;To explain Gradient Descent, we will use the standard example of hill descending.&lt;/p&gt;

&lt;p&gt;Consider a valley on which you are standing. Now your task is to reach to the lowest point of the valley. A twist is that you are blindfolded and you have no visibility to see where you are heading to. So let’s understand what approach can be used for this problem&lt;/p&gt;

&lt;p&gt;The best way is to check the ground near you and observe where the terrain tends to descend or decrease. This will give you a good idea of what direction you must take your first step. If you follow the descending path, you would probably reach the bottom of the camp.&lt;/p&gt;

&lt;p&gt;To represent this graphically, let’s have a look at the below graph.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/GradientDescent/6.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let us now map this scenario in a mathematical formula.&lt;/p&gt;

&lt;p&gt;We have to start with some θ0  and θ1. We need to keep changing the parameters to reduce the cost function until we hopefully end up at a minimum.&lt;/p&gt;

&lt;p&gt;The algorithm of gradient descent can be written as follows&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/GradientDescent/5.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, on the y-axis, we have the cost function J(θ) along with the parameters θ1 and θ2 on the other two axes.&lt;/p&gt;

&lt;p&gt;Now there are some kinds of gradient descent algorithms that can be further classified as follows:
&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;● On the basis of data ingestion&lt;/b&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Full Batch Gradient Descent Algorithm&lt;/li&gt;
  &lt;li&gt;Stochastic Gradient Descent Algorithm&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For full batch gradient descent algorithms, we use the complete data along with all the parameters to compute the gradient, whereas, for stochastic algorithms, we take only a small sample of the data.
&lt;br /&gt;
&lt;br /&gt;
&lt;b&gt;● On the basis of differentiation techniques&lt;/b&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;First-order Differentiation&lt;/li&gt;
  &lt;li&gt;Second-order Differentiation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Gradient descent calculates gradient by differentiating the cost function. To take the derivative of the cost function, either first-order or second-order differentiation can be used.&lt;/p&gt;

&lt;h2&gt;Challenges in executing Gradient Descent&lt;/h2&gt;

&lt;p&gt;Gradient Descent is a binding technique that works in most cases. But there are various cases where gradient descent doesn’t work properly or fails to find an optimum value. The major reasons for its failure are as follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Data challenges&lt;/li&gt;
  &lt;li&gt;Gradient challenges&lt;/li&gt;
  &lt;li&gt;Implementation challenges&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt;A. Data Challenges&lt;/b&gt;
&lt;br /&gt;
● Gradient Descent has a very low convergence rate and thus the answer is obtained in several iterations, consuming a lot of time and effort.
&lt;br /&gt;
● There is also a saddle point problem. This is a point in the data set where the gradient is zero and it is thus not an optimal point.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;B. Gradient Challenges&lt;/b&gt;
&lt;br /&gt;
● If the learning parameter becomes extremely large, the method of Gradient Descent can overshoot the minimum, it may fail to converge or even diverge.
&lt;br /&gt;
● One of the other problems of this approach is converging to a local minima can be quite slow. If there are multiple local minima present in the dataset, then there is no guarantee that the algorithm will detect the global minimum.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;C. Implementation Challenges&lt;/b&gt;
&lt;br /&gt;
● When implementing the algorithm of gradient descent, it is extremely important to calculate how many resources one would need. If the memory is too small, then the network would fail.
&lt;br /&gt;
● Also, it’s important to keep track of floating-point data values and hardware/software prerequisites.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The method of Gradient Descent can also be used for multiple regression when we have multiple features/parameters. To improve Gradient descent for multiple features, methods like Feature Scaling, Mean normalization, and Debugging can be used.
Gradient Descent can be used for both Linear and Logistic Regression machine learning algorithms. This method is a standard method of minimizing the cost function which works well for every machine learning algorithm.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Turing Test</title><link href="http://localhost:4000/blog/Turing-Test/" rel="alternate" type="text/html" title="The Turing Test" /><published>2021-08-05T00:00:00+05:30</published><updated>2021-08-05T00:00:00+05:30</updated><id>http://localhost:4000/blog/Turing-Test</id><content type="html" xml:base="http://localhost:4000/blog/Turing-Test/">&lt;p&gt;&lt;img src=&quot;/blog/TuringTest/Turing1.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;WHAT IS THE TURING TEST?&lt;/h2&gt;

&lt;p&gt;The Turing Test (originally known as an Imitation Game) is a method of inquiry into artificial intelligence (AI) for determining whether a computer is capable of thinking like a human being or not. The test is named after its creator Alan Turing, an English Computer Scientist, Theoretical Biologist, Mathematician, and Cryptanalyst.&lt;/p&gt;

&lt;p&gt;Turing proposed that a computer can be said to possess artificial intelligence if it can replicate human responses under particular conditions. The Original Turing Test involved three entities, each of which would be physically separated from the rest. One of the entities would be operated by a computer, while the other two would be operated by humans.&lt;/p&gt;

&lt;p&gt;During the test, one human is an interrogator, while the second human and the computer function as respondents or answer terminals. The questioner asks questions from the respondents within a particular subject area with a similar format and context. After a pre-set duration (usually 5 minutes), or a particular number of questions, the questioner is then asked to differentiate the human and the computer out of the two.&lt;/p&gt;

&lt;p&gt;The test is repeated a number of times. If the questioner cannot reliably differentiate between the conversation of the computer and human then the computer is considered to have passed the test because its answers are “just as human” as the human respondent.&lt;/p&gt;

&lt;h2&gt;THE WORKING OF THE TURING TEST&lt;/h2&gt;

&lt;p&gt;According to Turing, the main motive of the test is that a machine has to try and pretend to be a human, by answering questions put to it, and it will only pass if it is successful in convincing to be a human.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/TuringTest/Turing2.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The humans were restricted from giving away any of the personal information during the tests.&lt;/p&gt;

&lt;p&gt;In the tests conducted at the Royal Society in June 2014, there were six different sessions with five parallel imitation games at a time occurring during each session. A different judge was selected for each game, which meant there were five judges in each session. Each session had five rounds, with five parallel imitation games in each round. Each anonymous human was part of the five games in a session.  All five machines (the five different competition bots) took part, so every machine was involved in five games per session, hence 30 games in totality.&lt;/p&gt;

&lt;p&gt;In a particular session, a judge conducted five different tests. In their first test, they noticed a hidden human pitted against a hidden machine&lt;/p&gt;

&lt;p&gt;The second test conducted involved a different human against a different machine. And so on. It would continue until the judge had conducted all five tests in that session. At the end of each test, they were asked to tell each entity if they were able to differentiate between a machine and a human.&lt;/p&gt;

&lt;h2&gt;RESULTS&lt;/h2&gt;

&lt;p&gt;There were five machines involved in total in the tests and their success rates were:&lt;/p&gt;

&lt;p&gt;●	Eugene Goostman 33%&lt;/p&gt;

&lt;p&gt;●	Elbot 27%&lt;/p&gt;

&lt;p&gt;●	J. Fred 20%&lt;/p&gt;

&lt;p&gt;●	Ultra-Hal 13%&lt;/p&gt;

&lt;p&gt;●	Clever Bot 7%&lt;/p&gt;

&lt;p&gt;In each case, their success rate was in respect of judges identifying them as humans. Eugene Goostman was the program that was considered to have passed the test with a success rate of 33%.&lt;/p&gt;

&lt;h2&gt;EUGENE GOOSTMAN&lt;/h2&gt;

&lt;p&gt;Eugene Goostman is a chatbot programmed by three Russian programmers in Saint Petersburg in 2001. The program replicates a thirteen-year-old Ukrainian boy and is said to have been successful in passing the Turing test at an event organized by the University of Reading.&lt;/p&gt;

&lt;p&gt;According to Vladimir Veselov, one of the creators of the program, the choice of the age of Eugene Goostman was taken as thirteen because a thirteen-year-old is “neither too old to know a lot nor too young to know nothing”. This young age also leads to people ignoring any minor grammatical errors that the program might end up making.
The following is the transcript of a conversation between one of the Judges of the test, and two respondents, one of which is a human and the other is Eugene Goostman. We would like you to read both the transcripts and see whether you are able to distinguish between the human respondent and Eugene or not.&lt;/p&gt;

&lt;h2&gt;TRANSCRIPT 1:&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; Good afternoon&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; Good afternoon!&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; How many left hands do you have?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; The same as right hands, and how about you?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; The same of course. I love the scent of new-mown hay. How do you feel about scent?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; I find it exciting and refreshing! Does it make you feel anything?
Judge: Well it depends..Which smell do you really hate?&lt;/p&gt;

&lt;h2&gt;TRANSCRIPT 2:&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; Hello&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; Hello, I’m really glad to have the chance to chat with you! My guinea pig Bill sends his regards too!&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; Is Bill a male or a female?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; Ask Bill personally, please.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; Well I’d rather talk to you. My name is Jane and I am female. How about you? What’s your gender?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; I’m a male. A “guy”, I’d say.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; Pleased to meet you. What’s the weather like where you are?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; Let’s get on with our conversation!&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; Don’t you like talking about the weather?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; All these talks about weather is a waste of time&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; What would you like to discuss?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; I don’t know … Better tell me more about yourself! Where do you come from, by the way? Could you tell me about the place where you live?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Judge:&lt;/b&gt; It’s a lovely place with two bedrooms and a great view over London. What can you see from your bedroom window?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Respondent:&lt;/b&gt; I’ve never been to London, but I’d really love to visit it! To see their … their … (damn, what do they have …) Oh yes – their Big-Ben!&lt;/p&gt;

&lt;p&gt;In this conversation, the respondent in Transcript 1 was a female human whereas the Respondent in Transcript 2 was the machine Eugene. The judge considered the respondent in Transcript 1 to be definitely a machine, awarding it only 20 out of 100 (a very poor score) for humanlike conversation. However, they marked the respondent in Transcript 2 i.e Eugene Goostman as unsure.&lt;/p&gt;

&lt;h2&gt;ALTERNATIVES TO THE TURING TEST&lt;/h2&gt;
&lt;p&gt;There were many alternatives to Turing Tests that were later developed. These alternatives include:&lt;/p&gt;

&lt;p&gt;●	The Marcus Test – A test in which a program can watch a visual show and is then interrogated by being asked meaningful questions about the show’s content.&lt;/p&gt;

&lt;p&gt;●	The Lovelace Test 2.0 – A test made to detect AI through its ability to create art.&lt;/p&gt;

&lt;p&gt;●	Winograd Schema Challenge – A test that asks multiple-choice questions in a particular format.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Logistic Regression And Surviving The Titanic</title><link href="http://localhost:4000/blog/Logistic-Regression/" rel="alternate" type="text/html" title="Logistic Regression And Surviving The Titanic" /><published>2021-07-22T00:00:00+05:30</published><updated>2021-07-22T00:00:00+05:30</updated><id>http://localhost:4000/blog/Logistic-Regression</id><content type="html" xml:base="http://localhost:4000/blog/Logistic-Regression/">&lt;p&gt;Logistic Regression, or the Logit Model, is one of the fundamental blocks of statistics and machine learning. It is used when the dependent variable is categorical, and is the go to method for binary classification (two class values). For example, the probability of win/lose or pass/fail an exam. Most popular use of logistic regression today is in determining if an email is spam or not.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic1.jpg&quot; /&gt;&lt;/p&gt;

&lt;h1&gt;How does it work?&lt;/h1&gt;
&lt;p&gt;At the core of the logistic regression is the logit function, also called the sigmoid function and was developed by statisticians to describe properties of population growth in ecology, biology and environment sciences. It’s an S-shaped curve that maps any real-valued number into a value between 0 and 1, not necessary at only those limits. The equation for Logistic Regression is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic2.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where y is the predicted output, B0 is the intercept and B1 is the coefficient for (x). It can be said that Logistic regression is a linear function. However, the predictions are morphed into classification using the logit function.
Example of Logistic Regression&lt;/p&gt;

&lt;p&gt;We can use an example to learn Logistic Regression better. Let’s say we have data that can be used to predict a person’s gender based on their height. Given a height of 150cm is the person male or female.&lt;/p&gt;

&lt;p&gt;Let’s say that the coefficients are b0 = -100 and b1 = 0.6. The above equation can be utilized to predict if a person is male given a height of 150cm.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;y = e^(b0 + b1&lt;em&gt;x) / (1 + e^(b0 + b1&lt;/em&gt;x))&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;y = e^(-100 + 0.6&lt;em&gt;150) / (1 + e^(-100 + 0.6&lt;/em&gt;x))&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;y = 0.00004539&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;The probability is so low that it can be used as 0, and certainly this person is not male.
Since, this is classification and we want a crisp answer, we can create bins for a complete classification of the values, for example:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;0 if p(male) &amp;lt; 0.5&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;1 if p(male) &amp;gt;= 0.5&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Logistic regression models are models that have a certain fixed number of parameters that depend on the number of input features, and they output categorical predictions, like for example if a cancer is malignant or not.&lt;/p&gt;

&lt;h1&gt;Types of Logistic Regression&lt;/h1&gt;

&lt;p&gt;&lt;b&gt;Binary Logistic Regression:&lt;/b&gt; The final response has only two possible outcomes. For example, either a student passes an exam or not.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Multinomial Logistic Regression:&lt;/b&gt; More than two possible outcomes, without any ordering. For example, predicting which of the election candidates wins among many.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Ordinal Logistic Regression:&lt;/b&gt; An ordered possibility of outcomes. For example, figuring out the movie rating from 1 to 5.&lt;/p&gt;

&lt;p&gt;However, in this article we’ll be focusing solely on the binary classification type as it is the most popular among the three. ‘&lt;/p&gt;

&lt;h1&gt;Surviving a Disaster and The Titanic Dataset&lt;/h1&gt;

&lt;p&gt;The most popular dataset on Kaggle, undoubtedly, is the Titanic Dataset. It can also be considered a rite of passage for aspiring data scientists learning classification models. And why not, the data is structured in a way that helps people learn the fundamentals of classification and logistic regression.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic3.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The dataset has the following variables (attributes) which are explained very well on Kaggle.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic4.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, we shall use the train.csv provided to train the model and predict the survival of a passenger based on the given variables.&lt;/p&gt;

&lt;h1&gt;Code &lt;/h1&gt;

&lt;h3&gt;Importing the dataset, and understanding the data: &lt;/h3&gt;

&lt;p&gt;import pandas as pd&lt;/p&gt;

&lt;p&gt;titanic = pd.read_csv(“train.csv”)&lt;/p&gt;

&lt;p&gt;titanic.shape&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic5.jpg&quot; /&gt;
&lt;img src=&quot;/blog/LogisticRegression/Logistic6.jpg&quot; /&gt;
&lt;img src=&quot;/blog/LogisticRegression/Logistic7.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Data Preprocessing &lt;/h3&gt;

&lt;p&gt;It is also worth noting that ‘Embarked’ has 3 classes C, Q, S which have to be converted into individual attributes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic8.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, we’ve used the ‘get_dummies’ function to create separate variables for each Embarked class. And we’ve joined the new dataframe with the original dataframe.&lt;/p&gt;

&lt;p&gt;We’ve created two dataframes X and y, which will be used for Logistic Regression and learning. And dropped multiple non-numeric attributes which have no effect on the survival of a passenger.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic9.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see that ‘Age’ has many null values, so we use ‘mean’ to impute null values.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic10.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Creating the Model&lt;/h3&gt;

&lt;p&gt;We’ll use ScikitLearn to create the Logistic Regression model, and split the dataset into 80% (used for training the model) and 20% (for testing the model).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic11.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After the model is created, it is necessary to check how well it has performed. The model score for testing is 0.754, which means that 75.4% of the time the model correctly predicts if a passenger has survived the disaster or not. We also figured out the intercept and coefficients (the array is made up of all the attributes used in the model).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic12.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, the correlation between the attributes and survival can be better understood with a visual.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/LogisticRegression/Logistic13.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This cell’s output is a heatmap that shows the correlation between all the attributes.
&lt;img src=&quot;/blog/LogisticRegression/Logistic14.jpg&quot; /&gt;&lt;/p&gt;

&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Logistic regression is one of the most exciting concepts in statistics and a powerful tool to classify data. However, one shortcoming of Logit functions is that they are not able to work well with outliers and leads to overfitting. Hence, we must try and remove outliers from the data provided to make the model more accurate.&lt;/p&gt;

&lt;p&gt;This was the most simple method that can be used to train a ML Logistic model. We can always use more sophisticated models for better prediction and classification using a more detailed analysis of the data and more complex feature engineering.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Logistic Regression, or the Logit Model, is one of the fundamental blocks of statistics and machine learning. It is used when the dependent variable is categorical, and is the go to method for binary classification (two class values). For example, the probability of win/lose or pass/fail an exam. Most popular use of logistic regression today is in determining if an email is spam or not.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>