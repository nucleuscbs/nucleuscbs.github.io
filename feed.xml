<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-05-05T15:22:32+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">The Analytics Bay</title><subtitle>Always Analyzing</subtitle><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><entry><title type="html">The Birthday Problem</title><link href="http://localhost:4000/blog/Birthday-Problem/" rel="alternate" type="text/html" title="The Birthday Problem" /><published>2021-05-05T00:00:00+05:30</published><updated>2021-05-05T00:00:00+05:30</updated><id>http://localhost:4000/blog/Birthday-Problem</id><content type="html" xml:base="http://localhost:4000/blog/Birthday-Problem/">&lt;p&gt;&lt;img src=&quot;/blog/BirthdayProblem/BirthdayProblem%20(1).png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Alternatively, how many people must be there in a room such that the probability of two people sharing the same birthday is pretty high, 50% to 90%? Quite intriguing questions, right? Think about the problem before reading further on. Assume that there are no twins in the room, the no. of days in the year is 365 (sorry people who have birthdays on 29th Feb) and the distribution of birthdays throughout the year is uniform (Actually it’s the worst case, the results will be even better in real-world scenarios because the birthdays are more closely distributed in the real-world than the ideal world of uniform distribution).&lt;/p&gt;

&lt;p&gt;The Math&lt;/p&gt;

&lt;p&gt;Let’s turn to math for an answer because our intuition can be wrong (wasn’t yours?) but math can’t be wrong. We as humans are a bit selfish (weren’t you thinking about your birthday matching with someone else, rather than any two people having the same birthday). Combinatorics tells us that there are nC2 pairs possible when we take any 2 people from the total of n people in the room.&lt;/p&gt;

&lt;p&gt;For these calculations, we’ll make a few assumptions. First, we’ll disregard leap years. It simplifies the math without having too great an effect on the results. Second, we assume that birthdays are uniformly distributed throughout the year and have an equal probability of occurring (However, that is not usually the case. Studies have shown that more people are born in the first half of the year in India than the second, especially from April to June). Taking uniform distribution gives a truer mathematical approximation as the probability of two people sharing the same birthday is the least in case of uniform distribution.&lt;/p&gt;

&lt;p&gt;We’ll start with one person, and then add people in one at a time to illustrate how the calculations work. It is simpler to find the probability that no one shares a birthday. We’ll then take that probability and subtract it from one to derive the probability that at least two people share a birthday because these two are complements of each other.&lt;/p&gt;

&lt;p&gt;Using, P(A) = 1 - P(A’),&lt;/p&gt;

&lt;p&gt;Probability of at least one same birthday = 1 – Probability of no same birthday&lt;/p&gt;

&lt;p&gt;For the first person, there are no birthdays already reserved, which means that there is a 365/365 chance that there is not a shared birthday. That makes sense since we have only one person.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/BirthdayProblem/BirthdayProblem%20(2).png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Adding a second person to the mix gives us an interesting result. The first person covers one possible birthday, so the second person has a 364/365 chance of not sharing the same day. To find the probability of no match, we’ll multiply the probabilities of the first two people and subtract from one to calculate the probability of them sharing the same birthday.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/BirthdayProblem/BirthdayProblem%20(3).png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When the third person comes, the previous two people already cover two dates. So, the third person has a probability of 363/365 for not sharing a birthday.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/BirthdayProblem/BirthdayProblem%20(4).png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The pattern for how to calculate the probability for a given number of people must be quite visible now, though no worries if you couldn’t figure it out. Here’s the general form of the equation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/BirthdayProblem/BirthdayProblem%20(5).png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By assessing the probabilities, the solution to the Birthday Problem is that you need a group of 23 people to have a 50.73% chance of people sharing a birthday! Most people expect the group to be considerably larger than that (what was you guess?). The chart also depicts that a group of 57 has a probability of 0.99. It’s practically a guarantee that in a group of 57 people, two will share a birthday.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/BirthdayProblem/BirthdayProblem%20(6).png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(Credit: statisticsbyjim.com)&lt;/p&gt;

&lt;p&gt;Explaining the small size of the group.&lt;/p&gt;

&lt;p&gt;People consider the answer to the Birthday Problem difficult to believe: How can it be so small? However, the answer is correct, proven by probability and statistics. When thinking of the problem, people usually consider themselves. However, it could be any two people in the group. If we pair each person with another, this leads to the formation of a large number of pairs and therefore increases the possibility of the twin-birthday pair. The number of different pairs or unique combinations formed for 23 people (probability of 0.5) is 253.&lt;/p&gt;

&lt;p&gt;=&amp;gt; nC2 = 23C2 = 253&lt;/p&gt;

&lt;p&gt;As seen earlier, each pair’s probability of sharing a birthday is 0.0027, but for 253 pairs that number changes drastically.&lt;/p&gt;

&lt;p&gt;Similarly, when there are 57 people when the probability is ~1, you have 1596 pairs. Now, we would find it absurd if none of the pairs share a birthday.&lt;/p&gt;

&lt;p&gt;=&amp;gt; nC2 = 57C2 = 1596&lt;/p&gt;

&lt;p&gt;Conclusion&lt;/p&gt;

&lt;p&gt;We conclude that intuition doesn’t work in such problems because we humans are pretty bad at visualizing nonlinear functions and grossly underestimate the number of pairs formed from n number of people. Mathematics and statistics hence come to our rescue and prove to be pretty useful in such situations. So next time whenever you are in a room with 50 people make sure to find your birthday match.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Behavioral Analytics</title><link href="http://localhost:4000/blog/Behavioral-Analytics/" rel="alternate" type="text/html" title="Behavioral Analytics" /><published>2021-04-21T00:00:00+05:30</published><updated>2021-04-21T00:00:00+05:30</updated><id>http://localhost:4000/blog/Behavioral-Analytics</id><content type="html" xml:base="http://localhost:4000/blog/Behavioral-Analytics/">&lt;p&gt;&lt;img src=&quot;/blog/BehavioralAnalytics/feature.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The number of clicks you made, the amount of time you spent on each item, the type of items you spend your time on, etc. is stored. This massive volume of data is captured and analyzed by companies. You must have noticed, when you spend time on apps like Instagram, if you started to look at a lot of cute dog pictures on Instagram, your feed will automatically start having more and more dog content. This is because Instagram has been capturing your behavior and analyzing it to provide more personalized content for you. This process of analyzing user behavior is called behavioral analytics.
Incorporating behavioral analytics into your operations can be a little intimidating, both in terms of implementation and expense. However, according to a report by McKinsey, organizations that use customer data to produce behavioral insights outperform their peers by 85 percent in sales growth and more than 25 percent in gross margin.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/BehavioralAnalytics/11.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We usually see behavioral analytics being used by companies regularly (but often don’t realize it):&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Netflix:
Netflix provides its recommendations through a complex algorithm that takes into consideration the preferences of the customer who is watching and also the shows which were watched by people with similar preferences:
&lt;img src=&quot;/blog/BehavioralAnalytics/16.png&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Analyzing user behavior like this has helped Netflix increase the lifetime of their customers and helped in making their content more personalized for them. Netflix executives estimated that this analysis saves the company $1 Billion a year.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Amazon:
Amazon gives product recommendations through a complex machine learning algorithm that combines behavioral data such as:
● A user’s purchase history
● Items in their cart
● Items they’ve liked and rated
● What other customers have viewed and purchased
This algorithm is estimated to be responsible for around 35% of Amazon’s total revenue.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/blog/BehavioralAnalytics/13.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What are Behavioral Analysis Tools?&lt;/p&gt;

&lt;p&gt;There are three main behavioral analysis tools involved in building a picture of your customer journey: segmentation analysis, funnel analysis, and cohort analysis.&lt;/p&gt;

&lt;p&gt;SEGMENTATION ANALYSIS&lt;/p&gt;

&lt;p&gt;The study of customers divided into smaller groups to understand specific characteristics such as their behavior, age, income, and personality is known as segmentation analysis. When a company is marketing a smaller segment of consumers, it is easier for them to advertise since each advertisement can be highly tailored and precise to the features of each group.&lt;/p&gt;

&lt;p&gt;FUNNEL ANALYSIS&lt;/p&gt;

&lt;p&gt;Funnel analysis is a method of evaluating the steps taken to achieve a certain outcome on a website, as well as the number of users who complete each step. Funnel analysis helps you spot where users are leaving your website, so you can optimize the problem area and increase conversion rates.
To analyze a funnel, you have to find:
● User Conversion rates
● User Drop-off rates
&lt;img src=&quot;/blog/BehavioralAnalytics/14.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;COHORT ANALYSIS&lt;/p&gt;

&lt;p&gt;Cohort analysis breaks the data into a data set into related groups before analysis. These groups, or cohorts, usually share some common characteristics. Cohort analysis allows a company to find patterns throughout the life-cycle of a customer. By seeing these patterns, a company can adapt its service to those specific cohorts.&lt;/p&gt;

&lt;p&gt;The 2 most common cohort types are:&lt;/p&gt;

&lt;p&gt;● Acquisition Cohorts: divides users by when they signed up first for your product. For app users, you might break down cohorts by the day, the week, or the month they launched an app, and track daily weekly, or monthly cohorts.&lt;/p&gt;

&lt;p&gt;● Behavioral Cohorts: divides users by their behavior in your app within a given period. These could be any number of actions that a user can perform – App Install, App Uninstall, Transaction, or any combination of these events.&lt;/p&gt;

&lt;p&gt;Benefits of Behavioral Analytics&lt;/p&gt;

&lt;p&gt;Behavioral analytics is critical for increasing conversion, commitment, and retention at a company. Every member of a team should be able to obtain the actionable knowledge they need to answer their questions and exploit data in ways that didn’t seem possible before with the right behavioral analytics tool.&lt;/p&gt;

&lt;p&gt;According to the results, a large percentage of users use a particular e-commerce platform after searching “Thai food” on Google. Most of the users spent time on the homepage and went to the “Asian Food” tab and end up buying nothing. Examining each of these incidents as a single data point fails to shows what is going in consumers mind and not able to analyze why consumer isn’t buying the product&lt;/p&gt;

&lt;p&gt;Both web traffic and page views are viewed as a timeline of related events that did not result in orders in behavioral analytics. Since the majority of users left after seeing the “Asian Food” page, there might be a discrepancy between what they’re looking for on Google and what the “Asian Food” page reveals. Knowing this, a glance at the “Asian Food” page shows that Thai food is not prominently displayed, leading people to assume it is not available, even though it is.&lt;/p&gt;

&lt;p&gt;Let’s look at a few examples of how behavioral analytics may be used:
A travel company decides to use its website to monitor consumer events to improve its marketing efforts. A consumer might have looked at visiting a particular destination with a specific airline but abandoned the process before making an order. The business sends an email to the prospect in response to this material. To inspire a booking, the email may contain a travel discount or bid, as well as any valuable information on the destination of interest.&lt;/p&gt;

&lt;p&gt;Another scenario may be that a car dealership sends out an email campaign with a PDF attachment providing information about a variety of vehicles. They can detect user events using behavioral analytics. They will see who opened the attachment, how long they left the PDF open, how much they got into it, and where they spent the most time. The data is then sent to the sales team, which is now in a stronger position to initiate talks with prospects. Prospects become more involved in discussions, and sales representatives may provide more accurate information and tailored deals.&lt;/p&gt;

&lt;p&gt;Here are a couple of the benefits of behavioral data analytics:
● Customized advertising and marketing campaigns
● Heightened customer interaction and subsequent fulfillment
● Better customer relations, and
● Eventually more sales&lt;/p&gt;

&lt;p&gt;To conclude, a company should use behavioral analytics to help understand its target audience’s expectations and preferences. In today’s world, not understanding it means resorting to a scattershot campaign, which does not work.&lt;/p&gt;

&lt;p&gt;Criticism of Behavioral Analytics&lt;/p&gt;

&lt;p&gt;Behavioral analytics raises substantial privacy questions because it necessitates the processing and aggregation of vast volumes of personal data, including extremely confidential data (such as sexual identity or sexual interests, health conditions, and location), which is then exchanged between hundreds of parties interested in targeted ads.
Starting in 2015, Amazon joined Google and other tech giants in launching in-home voice devices that are expected to become a gold mine of behavioral insights for off-line life, just as the activities on their pages are a repository of data for your online life. Some people think this is invasive and unnecessarily informative to data providers and the government, but when they buy something, they are de facto subscribing to the terms.&lt;/p&gt;

&lt;p&gt;Various Products and Websites
&lt;img src=&quot;/blog/BehavioralAnalytics/15.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Amplitude, Indicative, and Mixpanel are common behavioral analytics providers, and each has its own set of advantages and disadvantages.&lt;/p&gt;

&lt;p&gt;If a company is looking for the right behavioral analytics supplier, it’s critical to take your time to thoroughly study each tool. Some companies have a free trial period, which will help you get a clearer understanding of how the tools function and whether the product is right for your market and research needs.
It’s important to check out behavioral intelligence resources that can help you to&lt;/p&gt;

&lt;p&gt;● Optimize behavior through multiple paths and isolate the efficient ones
● Diagnose and remove unwanted steps for customers
● Focus on key behaviors that result in higher total customer value
● Use targeted customer segments (cohorts) to inform and launch campaigns
● Isolate and aim users at risk of churn ahead of time
● Develop at-a-glance dashboards that can be shared with teams and executives&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Quantitative Forecasting</title><link href="http://localhost:4000/blog/Quantitative-Forecasting/" rel="alternate" type="text/html" title="Quantitative Forecasting" /><published>2021-04-14T00:00:00+05:30</published><updated>2021-04-14T00:00:00+05:30</updated><id>http://localhost:4000/blog/Quantitative-Forecasting</id><content type="html" xml:base="http://localhost:4000/blog/Quantitative-Forecasting/">&lt;p&gt;&lt;img src=&quot;/blog/Quantitative_Forecasting/feature.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What is Forecasting?&lt;/p&gt;

&lt;p&gt;Forecasting in analytics carries the same meaning as in the English language i.e., predicting values and outcomes. As the term Quantitative suggests, we rely on mathematical data or numbers to forecast. And exactly how do we do that? What methods do we use? That’s the focus of this blog.&lt;/p&gt;

&lt;p&gt;Before delving any deeper, let’s understand the importance of forecasting and why exactly do we need to understand the methods used for the same. Forecasting is valuable as it gives us the ability to make informed business decisions and develop data-driven strategies.&lt;/p&gt;

&lt;p&gt;Forecasting quantitatively i.e., using data analysis to predict future trends, advances and changes helps us make calculated and prudent decisions. Most financial and operational decisions are made based on current market conditions and predictions on how the future looks, for which we naturally need to forecast. You can’t predict uncertainties, but forecasting helps you to be proactive rather than reactive when faced with dire situations.&lt;/p&gt;

&lt;p&gt;The models we use to forecast outcomes quantitatively can be broadly classified into two categories:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Associative models
Associative models are where we identify and analyze a causal relationship between the given variables. For that we require Pattern. A pattern between the outcome and the factors is what enables us to understand the model and extend it to predict the results for other data values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Time series models
So, what do we mean by a time series? Let’s just break it in simpler terms. It refers to any series that represents data in a chronological order. These models examine the past data patterns and predict the future outcomes based on underlying patterns which we identify in the given data.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ASSOCIATIVE MODELS&lt;/p&gt;

&lt;p&gt;Linear Regression is perhaps the most famous tool used for the same at the basic level. It basically identifies the pattern and states it in the form of a linear equation. Another thing it calculates is the P-Value, which is nothing but the coefficient of correlation between the dependent and independent variables.&lt;/p&gt;

&lt;p&gt;As an example, let’s see if we can use a student’s reading capability to predict his acumen in mathematics. (Refer to the first 150 data entries in the dataset available at https://www.kaggle.com/spscientist/students-performance-in-exams).&lt;/p&gt;

&lt;p&gt;First, we calculate the correlation (Multiple R) between reading score and the maths score, which comes out to be 0.867. This gives us the R-Squared as 0.751.&lt;/p&gt;

&lt;p&gt;What R-Squared tells us is that how much of the variation seen in y-variable can be explained by the given x-variable. In this case, it is 0.751 i.e., around 75% of the variation in maths score can be explained by the variation in Reading score. Now this does not mean that the change is x-variable causes the change in y-variable. This has to do with the fact that correlation does not necessarily mean causality.&lt;/p&gt;

&lt;p&gt;R-Squared as 0.751 is significant, which means that variables are linearly related to each other and the data is a good fit for linear regression. This can be visually seen in the following graph as the data points seem to neatly arrange themselves in a straight line (linear manner).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Quantitative_Forecasting/3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now that we have established that the relationship exists, let’s analyze the linear equation received through the regression analysis. This is the equation of the line of the best fit (see the previous graph):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Quantitative_Forecasting/2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This means that an increase of 10 marks in reading score leads to an increase of around 9 marks in the math score. How do we interpret this? Logically, reading and mathematics are two polar opposite skills with seemingly no connection to each other. One possible explanation is that there might be a third factor (such as Parental Education level) which enables these students to be good at both the skills. This leaves the data open to further analysis.&lt;/p&gt;

&lt;p&gt;One last factor we must take into account is the confidence we should have in the given results. It could be possible that the relationship we observed between the values was merely a coincidence. What is the chance that if we were to take some random values instead of the given dataset, we would get the same (or even stronger) relationship?&lt;/p&gt;

&lt;p&gt;For that we turn to p-value. For the time being, we need not concern ourselves with its calculation. In this case the p-value is 1.42E-46, which is nearly zero. It means, that there is nearly 0% chance that we will be able to replicate the results by taking some random values. So, we can be confident of the relationship established by the regression analysis. Traditionally speaking, a p-value of less than 0.05 is a good indicator that the result established is not just a mere coincidence.&lt;/p&gt;

&lt;p&gt;In Statistical Lingo, we have something called a Null Hypothesis, which generally means that result obtained is not special and is just a fluke. p-value is the evidence in the favour of Null Hypothesis. So higher the p-value, the less evidence we have in favour of the Null Hypothesis, and more confidence we have in the analysis.&lt;/p&gt;

&lt;p&gt;But what if the data arranged itself into a shape other than a straight line? That’s when we turn to other more advanced forms of regressions viz: Polynomial Regression, Logistic Regression, among others. But again, this is a story for another time.&lt;/p&gt;

&lt;p&gt;TIME SERIES MODELS&lt;/p&gt;

&lt;p&gt;Perhaps the most famous example for a data set in this category is the stock price. A good way to predict the future prices is to analyze past trends.&lt;/p&gt;

&lt;p&gt;For that, first, we must account for extreme short-term fluctuations by making the curve smoother to focus on the long-term trends. Moving Average does the same, as can be seen in the following chart.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Quantitative_Forecasting/1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In Moving Average, each data entry is equated to the average of previous N entries, where N is a natural number whose magnitude depends upon the level of smoothness required.&lt;/p&gt;

&lt;p&gt;Once we have the smooth curve, we can then use other more complex and advanced analytic tools to predict the prices. We have various types of time series models and methods to choose from based on the intended future like trend projections, simple mean, and exponential smoothing among many.&lt;/p&gt;

&lt;p&gt;But once again, that is a story for another time…&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Principal Component Analysis</title><link href="http://localhost:4000/blog/Principal-Component-Analysis/" rel="alternate" type="text/html" title="Principal Component Analysis" /><published>2020-12-16T00:00:00+05:30</published><updated>2020-12-16T00:00:00+05:30</updated><id>http://localhost:4000/blog/Principal-Component-Analysis</id><content type="html" xml:base="http://localhost:4000/blog/Principal-Component-Analysis/">&lt;p&gt;&lt;img src=&quot;/blog/pca/1.png&quot; /&gt;
Let’s say we want to predict the impact of COVID-19 on Stock Markets and General Business Activities for different countries. For this we have tons of information like the opening and closing price, lowest and highest price, correlation of different indices over the world, inflation rate, unemployment rate, days of lockdown, cases reported, and so on… This sort of problem has an overwhelming number of variables, and someone who has worked earlier with a lot of variables knows what all obstructions this may present. The question is, “How do we take all of the variables collected and focus on only few of them?” In terms of Analytics, we simply want to “Reduce the Dimension Space.” By Dimensionality Reduction, we have fewer variable relationship to consider which makes data easy to explore and visualize to get the desired outcomes.&lt;/p&gt;

&lt;p&gt;Principal Component Analysis is nothing but a Dimensionality Reduction technique to reduce the feature space of large data-sets with numerous variables by transforming a large data-set into a smaller one but still with maximum information so to make analysing data much easier and quicker for machine learning algorithms and tuning techniques without unrelated variables to process. PCA helps in Noise Filtering, Visualization, Feature Extraction, Stock Market Predictions, Gene Data Analysis and many more. Another fascination property of PCA is that it is both a technique of Feature Extraction as well as a medium to perform Feature Extraction, as it combines the input variables in such a way that it drops the extraneous variables while keeping the more important ones.
Now since What, Why and When parts have been answered, let’s move to the most important question, that is How?&lt;/p&gt;

&lt;p&gt;&lt;b&gt; HOW DOES PCA WORK? &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Let’s look at the step by step how Principal Component Analysis works-&lt;/p&gt;

&lt;p&gt;&lt;b&gt; 1. STANDARDIZATION &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;The purpose of standardising the dataset is to ensure that each of the variable contributes equally to the analysis without any bias. It becomes essential to do standardization before PCA as variables are the core of the PCA and the analysis is pretty much sensitive regarding the variance of initial variables. That is if there are variables with large differences between the initial variables, the variable with larger range will dominate over the smaller ranges.
For example, consider two variables, Distance within city and distance between cities. One of the variables is measured in meters while other in Kilometers. Now if the data is not standardised than distance of 500 meters would be considered as greater than distance of 5KM, which isn’t true. Therefore, transforming the data to comparable scales can prevent this problem. Mathematically, it is done by subtracting the mean and dividing by the standard deviation for each value of each variable.
&lt;i&gt; z = value – mean/(standard deviation) &lt;/i&gt;&lt;/p&gt;

&lt;p&gt;All the variables will be transformed to the same scale once the standardization is completed.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; 2. COVARIENCE MATRIX COMPUTATION &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/pca/2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Often variables are correlated in such a way that they contain redundant information. Thus, to understand the relationship between the variables and identify such correlations, we compute the covariance matrix. The covariance matrix is an n×n symmetric matrix, where n is the number of dimensions. For example, for a 5-dimensional data set with 4 variables a, b, c, d and e, the covariance matrix is a 5×5 matrix of the from-&lt;/p&gt;

&lt;p&gt;Since covariance of a Variable with itself is nothing but its variance [Cov(a,a) = V(a)] therefore the diagonal elements in the covariance matrix are the variance of each variable.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; 3. COMPUTING THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Eigenvectors and eigenvalues are the linear algebra concepts that we are required to process from the covariance matrix to decide the “principal components” of the data. Principal components are the new variables that are constructed as a result of combinations done in a manner that the new components are uncorrelated and most of the information within the initial variables are compressed into the first components. For example, there is 10-dimensional data that provides 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on as shown in the plot below-&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/pca/3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Computing the eigenvectors and eigenvalues to organising the principal components in this manner helps if dimensionalty reduction without losing much information. Statistically, Principal Components represent the direction of data that explains the maximum amount of variance.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; 4. FEATURE VECTOR &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Feature Vector is the first visible step of dimensionality reduction with help of Principal Component Analysis. It is a a matrix of the columns of those eigenvectors that we decide to keep say k, where k &amp;lt; n, that is k is always less than the n, which was the number of initial Eigenvectors. The decision of selection is based on the eigenvalues, i.e. components with lesser significance are discared.&lt;/p&gt;

&lt;p&gt;For Example, suppose we create a feature vector by discarding one of the eigenvector as it was carrying only 2% information and therefore even though dimension now becomes k=n-1, we still have the necessary 98% of information carried by the other eigenvectors.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; 5. LAST STEP - RECAST THE DATA ALONG THE PRINCIPAL COMPONENTS AXES &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;In all the previous steps, excluding Standardisation, we didn’t make any change with the data and just selected the principal components and form the feature vector. The purpose of this step is to use the feature vector and reorient the data from the axis of the original variables (n axis) to the ones represented by the principal components, k axis (hence, the name Principal Components Analysis)&lt;/p&gt;

&lt;p&gt;This can be achieved by multiplying the transpose of the original data set by the transpose of the feature vector which we formed with the help of principal components.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; FinalData = Feature_VectorT * Standardized_Original_Data &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Here’s an example from setosa.io where we transform 5 data points using PCA-&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/pca/4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, we hope that you must have understood the concept of this not so explored technique in data science and statistics as we answered the final question of how PCA works. Hit the like button, if you liked our post; and subscribe our blog to be connected to us, as many more wonderful topics are underway -)&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Let’s say we want to predict the impact of COVID-19 on Stock Markets and General Business Activities for different countries. For this we have tons of information like the opening and closing price, lowest and highest price, correlation of different indices over the world, inflation rate, unemployment rate, days of lockdown, cases reported, and so on… This sort of problem has an overwhelming number of variables, and someone who has worked earlier with a lot of variables knows what all obstructions this may present. The question is, “How do we take all of the variables collected and focus on only few of them?” In terms of Analytics, we simply want to “Reduce the Dimension Space.” By Dimensionality Reduction, we have fewer variable relationship to consider which makes data easy to explore and visualize to get the desired outcomes.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Autocorrelation</title><link href="http://localhost:4000/blog/Autocorrelation/" rel="alternate" type="text/html" title="Autocorrelation" /><published>2020-12-02T00:00:00+05:30</published><updated>2020-12-02T00:00:00+05:30</updated><id>http://localhost:4000/blog/Autocorrelation</id><content type="html" xml:base="http://localhost:4000/blog/Autocorrelation/">&lt;p&gt;Autocorrelation shows the degree of similarity between a given time series and a lagged version of itself over successive time intervals. Lagged version of time series is simply the past period values of the concerned variable. This seemingly simple concept has wide application in fields of mathematics, statistics, and sciences. It is used in measuring optical spectra, short duration light pulses, in GPS system, portfolio and return optimization and much more. Unit root processes, trend-stationary processes, autoregressive processes, and moving average processes are specific forms of processes with autocorrelation.&lt;br /&gt;
The resulting values ranges between -1 to 1, like the traditional correlation statistic. An autocorrelation of +1 represents a perfect positive correlation (an increase seen in one time series leads to a proportionate increase in the other time series). An autocorrelation of negative 1, on the other hand, represents perfect negative correlation (an increase seen in one time series results in a proportionate decrease in the other time series). But it must be remembered as words of caution that autocorrelation measures linear relationships. So even if the autocorrelation is minuscule, there may still be a nonlinear relationship between a time series and a lagged version of itself.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Reasons of autocorrelation &lt;/b&gt;
Some major reasons of autocorrelation include-&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Carryover of effect- Carry over effect of variables in data causes autocorrelation. For ex- Data on monthly expenditure of a family in a particular city, preceding month expenditure will influence the expenditure of next month. This is called carry over effect.&lt;/li&gt;
  &lt;li&gt;Mis-specification of form of relationship in a model also causes autocorrelation. For ex- a study assumes that explanatory and study variables have linear relationship but they have logarithmic or exponential relation.&lt;/li&gt;
  &lt;li&gt;Measurement error- Errors may creep in due to difference between observed and true value of variable, data collection issues etc.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt; Problems caused by autocorrelation &lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Inefficient (but unbiased) ordinary least square estimate which fails to achieve smallest variance and any forecast based on them is hence inefficient. Efficient estimator gives most information about a sample and inefficient estimator may perform well but requires larger sample sizes to do so.&lt;/li&gt;
  &lt;li&gt;Exaggerated goodness of fit for a time series with positive serial correlation and an independent variable that grows over time.&lt;/li&gt;
  &lt;li&gt;Leads to overly optimistic view of R2.&lt;/li&gt;
  &lt;li&gt;Large variance in predictions based on the model and narrow confidence interval. The T statistics and F value are often not reliable.&lt;/li&gt;
  &lt;li&gt;It increases the occurrence of false negative for significant regression coefficient. Regression coefficients appear significant when they are not.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt; Graphical approach through MS-Excel &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Autocorrelation/1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Plotting the data with its lag clearly shows us there exist a relationship between the two, because the plot denotes a nearly definite pattern. This high relation is also denoted with correlation coefficient of 0.83.&lt;/p&gt;

&lt;p&gt;But things are not always this simple. Such high value of correlation of a data with its past values would have made everything so predictable and easy eliminating the need of complex statistical analysis and modeling. Let’s take another example-&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Autocorrelation/2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The plot shows data points paired with their lagged values are scattered without any pattern or direction. This is again confirmed with a low correlation coefficient.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Example 3 &lt;/b&gt;- For finding autocorrelation of a series with different lags on excel we simply create a table with suitable lags and find the correlation using CORREL() function. We can plot the various correlations so obtain to have a better picture of the case.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Autocorrelation/3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Autocorrelation in R &lt;/b&gt;
Finding autocorrelation is not always that simple because we test it with many lags. Repeating the same process with huge data sets becomes a tedious task in MS-Excel. Not to worry, we are again there for your rescue. Let’s try it out on R.
Documentation-
acf(x, lag.max = NULL,
type = c(“correlation”, “covariance”, “partial”),
plot = TRUE, na.action = na.fail, demean = TRUE, …)&lt;/p&gt;

&lt;p&gt;x&amp;lt;- Time series data
lag.max&amp;lt;- Number of lags upta which correlation needs to be calculated
type&amp;lt;- Character string giving the type of acf to be computed. Allowed values are”correlation” (the default), “covariance” or “partial”
plot&amp;lt;- True or False based on whether you want plot or not&lt;/p&gt;

&lt;p&gt;&lt;b&gt; What can you do? &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Removing autocorrelation involves making changing to the data, improvising the model and other tests and modifications. In layman language, for positive serial correlation one can add lags of dependent or independent variable in the model. For negative serial correlation one must check that none of the variables in over-differenced and for seasonal correlation one can add seasonal dummy variable to the model. It itself is a wide concept and we will surely cover it in detail in one of our subsequent blogs.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Example &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Mr. X is a trader. While monitoring stock of ABD ltd he realized that usually whenever stock price rallies on Monday it is followed by decrease in price by the next day and a increase in price on Wednesday. Amused to know this, he tries to prove his observations quantitatively. He takes the daily returns of the share price and run autocorrelation on it with lag 1 to lag 10. He finds that returns one day prior have a negative autocorrelation of -0.74, while the returns two days prior have a positive autocorrelation of +0.83. Past returns seem to influence future returns. Therefore, Mr.X can adjust his position to take advantage of the autocorrelation and resulting momentum by selling the share the next day after a rally and buy them on the following day again. While doing this he never forgets that the chances of error remains high because what was right yesterday may prove wrong tomorrow and he continuously monitors other factors that govern the stock price while repeatedly checking the relevance of auto-correlation at appropriate intervals.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Conclusion &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Autocorrelation is a wide concept. If discovered it can help us to detect patterns in our data, find seasonality and other interesting insights. If ignored it may distort our results. Just a basic awareness about its presence will help us improve our models and analysis. And I hope that we were successful in introducing and explaining the concept to you. Practice the methods we shared, explore new ones, and don’t forget to share your views and findings with us.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Autocorrelation shows the degree of similarity between a given time series and a lagged version of itself over successive time intervals. Lagged version of time series is simply the past period values of the concerned variable. This seemingly simple concept has wide application in fields of mathematics, statistics, and sciences. It is used in measuring optical spectra, short duration light pulses, in GPS system, portfolio and return optimization and much more. Unit root processes, trend-stationary processes, autoregressive processes, and moving average processes are specific forms of processes with autocorrelation. The resulting values ranges between -1 to 1, like the traditional correlation statistic. An autocorrelation of +1 represents a perfect positive correlation (an increase seen in one time series leads to a proportionate increase in the other time series). An autocorrelation of negative 1, on the other hand, represents perfect negative correlation (an increase seen in one time series results in a proportionate decrease in the other time series). But it must be remembered as words of caution that autocorrelation measures linear relationships. So even if the autocorrelation is minuscule, there may still be a nonlinear relationship between a time series and a lagged version of itself.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Random Forest</title><link href="http://localhost:4000/blog/Random-Forest/" rel="alternate" type="text/html" title="Random Forest" /><published>2020-10-27T00:00:00+05:30</published><updated>2020-10-27T00:00:00+05:30</updated><id>http://localhost:4000/blog/Random-Forest</id><content type="html" xml:base="http://localhost:4000/blog/Random-Forest/">&lt;p&gt;&lt;img src=&quot;/blog/RF/rf.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Random Forest is a versatile machine learning method capable of performing both the regression and classification tasks. It is a type of the ensemble learning method, in which a group of weak models combine to form a powerful model.&lt;/p&gt;

&lt;p&gt;It can be properly defined as &lt;b&gt; a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset. &lt;/b&gt;
Instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; The Algorithm of Random Forest &lt;/b&gt;
Random forest is like the bootstrapping algorithm with Decision tree (CART) model. Let’s say, we have 1000 observations in the complete population with 10 variables. Random forest tries to build multiple CART (decision tree) models with different samples and different initial variables. For instance, it would take a random sample of 100 observations and 5 randomly chosen initial variables to build a CART model. It will repeat the process, say 10 times, and then make a final prediction on each observation. The final prediction is the function of each prediction. This final prediction can simply be the mean of each of the predictions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/RF/1_each of the predictions.png&quot; /&gt;
&lt;b&gt; The Working Process of Random Forest can be explained in the below steps- &lt;/b&gt;
Step 1 - Firstly, select random K data points from the training set.
Step 2 - Now, build the decision trees associated with the selected data points (Subsets).
Step 3 - Choose the number N for the decision trees that you want to build.
Step 4 - Repeat Steps 1 &amp;amp; 2.
Step 5 - For new data points, find out the predictions of each decision tree, and assign the new data points to the category that wins the majority votes.
Random forest prediction pseudo-code-
To perform predictions; the trained random forest algorithm uses the below pseudo-code.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Take the test features and use the rules of each randomly created decision tree to predict the outcome and stores the predicted outcome (target)&lt;/li&gt;
  &lt;li&gt;Calculate number of votes for each predicted target.&lt;/li&gt;
  &lt;li&gt;Always consider the high voted predicted target as the final prediction from the random forest algorithm.
Now, we do understand that, some of the words or terms used above must have gone above your head; that’s no problem at all; we will understand the concept with the help of a real life example as well as a case study.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Random Forest Algorithm; A Basic Real Life Example.
Suppose, Saksham wants to go to different places that he may like for his vacation, and he asks his friend for advice. His friend will ask which places he has been to already, and whether he likes the places that he’s visited. Based on Saksham’s answers, his friend starts to give the recommendation. Here, his friend forms the decision tree.
Saksham wants to ask more friends for advice because he thinks only one friend cannot help him make an accurate decision. So, his other friends also ask him some random questions, and finally, provide an answer. He will consider the place with the most votes as his vacation decision.
His friends asked him some questions and gave the recommendation of the best place based on the answers. The friends created the rules based on the answers and used the rules to find the answer that matched the rules. Saksham’s friends also randomly asked him different questions and gave answers, which for they are the votes for the place. At the end, the place with the highest votes is the one he will select to go. This is the typical Random Forest algorithm approach.
Now, we move on to a serious and intuitive case study to actually understand why the concept of Random Forest is so useful, and how it is used practically.&lt;/p&gt;

&lt;p&gt;Case Study on Usage of Random Forest in ML
Following is the distribution of Annual income GINI Coefficients across different countries-&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/RF/2_Gini.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Mexico has a population of 118 Million. Let’s assume that, the algorithm Random forest picks up 10k observations with only one variable (for simplicity) to build each Decision tree (CART model). In total, we are looking at 5 CART models being built with different variables. In a real life problem, you will have more number of population samples and different combinations of input variables.
Salary bands-
Band 1 - Below $40,000
Band 2 - $40,000 – 150,000
Band 3 - More than $150,000
Following are the outputs of the 5 different CART models.
CART (Decision Tree) 1- Variable Age
&lt;img src=&quot;/blog/RF/3_Cart1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CART 2 - Variable Gender
&lt;img src=&quot;/blog/RF/4_Cart2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CART 3 - Variable Education
&lt;img src=&quot;/blog/RF/5_Cart3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CART 4 - Variable Residence
&lt;img src=&quot;/blog/RF/6_Cart4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CART 5 - Variable Industry
&lt;img src=&quot;/blog/RF/7_Cart_5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using these 5 Decision Tree models (CART models), we need to come up with single set of probability to belong to each of the salary classes. For simplicity, we will just take the mean of probabilities in this case study. Other than simple mean, we will also consider the vote method to come up with the final prediction. To come up with the final prediction let’s locate the following profile in each of the CART models-&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Age- 35 years,&lt;/li&gt;
  &lt;li&gt;Gender- Male,&lt;/li&gt;
  &lt;li&gt;Highest Educational Qualification- Diploma holder,&lt;/li&gt;
  &lt;li&gt;Industry- Manufacturing,&lt;/li&gt;
  &lt;li&gt;Residence- Metro
For each of these Decision tree (CART models), following is the distribution across salary bands-&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/blog/RF/8_finalcart.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The final probability is simply the average of the probability in the same salary bands in different CART models. As we can see from this analysis, that there is 70% chance of the individual falling in class 1 (less than $40,000) and around 24% chance of the individual falling in class 2.
Final Inference
Random forest gives much more accurate predictions when compared to simple Decision Tree (CART) or regression models in many scenarios. These cases generally have high number of predictive variables and huge sample size. This is because it captures the variance of several input variables at the same time and enables high number of observations to participate in the prediction.&lt;/p&gt;

&lt;p&gt;So, we hope that you must have understood the concept of ‘Random Forest’ in Machine Learning through the blog. Do write your precious feedback, and feel free to ask any doubt related.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Business Intelligence Project</title><link href="http://localhost:4000/blog/The-Business-Intelligence-Project/" rel="alternate" type="text/html" title="The Business Intelligence Project" /><published>2020-09-08T00:00:00+05:30</published><updated>2020-09-08T00:00:00+05:30</updated><id>http://localhost:4000/blog/The-Business-Intelligence-Project</id><content type="html" xml:base="http://localhost:4000/blog/The-Business-Intelligence-Project/">&lt;p&gt;Initially, BI used to be the work of just IT professionals, who would supply the whole organisation with the reports for decision-making, however, now it is used at every stage of management by the managers (primarily due to the development in data science sphere and the rise of self- service BI tools).&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Ways in which BI can help organisations make smart, data- driven decisions-&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Tracking performance&lt;/li&gt;
  &lt;li&gt;Analysing Key Performance Indicators (KPI)&lt;/li&gt;
  &lt;li&gt;Analysing the market share and consumer behaviour&lt;/li&gt;
  &lt;li&gt;Optimizing business operations using historical data&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt; Business Intelligence Tools &lt;/b&gt;
&lt;br /&gt;
Business intelligence tools are nothing but software that are used to analyse trends and extract insights out of the data in order to make tactical and strategic business decisions.
There are many Business Intelligence tools like SAS BI, MicroStrategy, Datapine, Domo, etc that help in carrying out the necessary tasks but two of the most powerful and widely used tools in Business Intelligence on which this blog is also dedicated are Power BI and Tableau.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Power BI&lt;/b&gt;
&lt;br /&gt;
Power BI provides a summary of the data in the form of reports and dashboards while connecting with every data source across different. It makes data assessment, sharing scalable dashboards, embedded visualizations, interactive reports and various another feature which we will see further in the blog. It is amazing at importing visualizations with easy-to-use and user-friendly interfaces like Excel, etc. Power BI is simple for using that provides a full overview of your business performance.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Tableau&lt;/b&gt;
&lt;br /&gt;
While there are numerous intuitive business intelligence tools, Tableau uses data discovery and its interpretation for all types of the business user. It is sometimes quoted as the most user-friendly data science tool available. Being simple to handle, any user without much of a stretch can perform reading or writing data related tasks along with analysing key data insights, while creating visualizations and innovative reports, and finally sharing insights throughout the enterprise with help of dashboards and stories. Tableau is good at connecting to any data source with a drag-and-drop interface that is easy-to-use and makes transferring data simple. However, unlike Power BI it is weak at combining with different data sources for analysis.&lt;/p&gt;

&lt;p&gt;INSIGHTS&lt;/p&gt;

&lt;p&gt;As we have always said, hands-on real-time experience on analytics software are any day better than the theoretical concepts and is also our core strength, so to compare how the visualisations would look in Power BI and Tableau we took a same dataset and created a dashboard. We undertook a project under the supervision of our esteemed alumnus, &lt;u&gt;Mr. Vikrant Sharma&lt;/u&gt; (Analyst, InMobi) and &lt;u&gt;Mr. Shoury Anand&lt;/u&gt;(IIM Lucknow, PGP’22). This included a study on COVID-19 scenario where we took 5 datasets which contained data for number of COVID cases worldwide and in India. Also, to study what impact the current pandemic has on the indices we took time series data for S&amp;amp;P Global, FTSE 100 and Dow Jones along with Gold and Crude Oil prices starting from 1st January 2020 to 31st July 2020. To study how GDP has fared in the past years, we took Real GDP for 180 countries for past 13 years. Not surprisingly, the visualizations said for itself all the data which we collected and wanted to concluded our results. Finally, both the dashboards looked pretty amazing and gave us a hard time to choose one over another. So the question in on our readers which visualisation did they find better!&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Tableau Dashboard&lt;/b&gt;
&lt;img src=&quot;/blog/tableau/Tableau Dashboard.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Power BI Dashboard&lt;/b&gt; 
&lt;img src=&quot;/blog/tableau/PowerBI Dashboard.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;
&lt;br /&gt;
We have conducted a survey of sample size of 100 to gather and access the opinion of college students on Business Intelligence tools namely Power BI and Tableau.
The respondents had to answer various questions; like on what parameters would they favour one software from the other, rating these softwares on User- friendliness and attractiveness, their personal opinions/ experiences among others.
Most of the students replied on the same lines and the results matched our findings/ expectations. Tableau was found to be the favourable of the two.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Some of the opinions-&lt;/b&gt;
&lt;i&gt;“The Power BI Student Version has a lot of limitations and thus though it is capable to a large extent, cannot be used to its full capacity. Tableau has some limited features as compared to Power BI w.r.t graph styles, colour available, etc. Overall, Both the tools are very powerful for Data Viz”&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;“I feel that use of visualizations created in Tableau should be restricted to Tableau itself. Any presentation that requires only and only visualizations with little to none supporting text can be presented through Tableau’s story mode. For PowerPoint presentations, I like to connect excel sheets with it for charts, as it makes it easy for me to change the formatting of the chart within PowerPoint and any change in figures can also be speedily done.
And about dashboards in presentations, I don’t think that is a good choice because the number of visualizations in a dashboard might distract an audience, and it is difficult to demonstrate its dynamic features on a presentation.”&lt;/p&gt;

&lt;p&gt;&lt;i&gt;I have had a better experience working with Power BI over tableau due to its user-friendly nature and a relatively easier visualisation capability.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Result of the survey were as follows-&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/tableau/1.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;
&lt;img src=&quot;/blog/tableau/2.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BUSINESS INTELLIGENCE TOOLS FROM A STUDENTS’ PERSPECTIVE
Microsoft’s Power BI is a suite of data visualization and business analytic tools. It offers tools to easily analyse, transform and visualize data pipelines, including the ability to build reusable models. The software enables users to integrate their apps, to deliver reports along with real-time dashboards.
Tableau is a visualization tool that helps businesses transform their data into insights that can lead to action. The tool makes it easy to connect data in almost any format from almost any source. Interactive dashboard with visual analytics can be created with simple dragging and dropping, and data transformed in graphs, maps, charts, and other visualizations.&lt;/p&gt;

&lt;p&gt;Data Visualizations tools are necessary when it comes to creating a visual representation of analytics and sharing insights with other.
And as Management students, we faced difficulty initially in choosing the best tool for making reports and dashboards, because both Power BI and Tableau were terrific and had a lot to offer. So, to make life simpler and easier, we’ll be comparing these two on a range of parameters-&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Cost&lt;/b&gt;
Tableau is expensive than Power BI when it comes to buying the complete version which usually is bought by the businesses and working professionals. Tableau’s annual price ranges around $1000 while Power BI’s annual price ranges around $100. Even though there is a great price difference in the full version, however if we compare only the free version Tableau has an upper hand. This is because many useful features were not included in Power BI’s free desktop version and this certainly impaired us during this project. We also had to face many hassles when it came to editing the shared files.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Data Analysis&lt;/b&gt;
Power BI should be the choice if someone wants to have an in- depth analysis of the data. It offers DAX (Data Analysis Expressions) which is a delight to work with. It is designed to work with tables and relational database, creating meaningful relationships between various data sources.
Tableau has in-built features like data blending and drill-down, which one can use to determine the variations, data patterns and for further data analysis.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Data Visualization&lt;/b&gt;
There is no one best tool when it comes to Data Visualization as both Power BI and Tableau has some really powerful visuals to offer. While talking about simplicity, surely Tableau creates fascinating dashboards through simple drag- and- drop, and complex calculations can also be made with the help of simple line of codes. It offers various types of visualizations such as Heat maps, Treemaps, Scatter Plots etc. One can also create ‘Word clouds’ and ‘Bubble charts’ in Tableau.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;On the other hand, Power BI has loads of visualisations which help in depth analysis. Power BI boasts of a wide variety of visualizations, such as R script visuals and Python visuals as well. These visuals can be created in Power BI Desktop and then published online.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Performance&lt;/b&gt;
The Clock Rate in Power BI is faster than that in Tableau, it loads data sets faster than the latter, plus it saves the files in a compressed manner and takes lower disk space. One can also publish their Power BI Desktop reports online and thus can have an easy access.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;User community&lt;/b&gt;
Both these tools offer great customer support, in terms of services and learning material. However, Tableau may have an upper edge in community support due to its huge user base and awareness in general.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;User Interface&lt;/b&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Tableau has an edge over Power BI when it comes to the user interface. It has an intuitive user interface along with versatile features. Its Tool Tip is more efficient than Power BI’s and provides elaborate details.
Power BI’s user interface is no less than that of Tableau’s but it can be a little intimidating for a novice.&lt;/p&gt;

&lt;p&gt;After comparing the 2 tools on various parameters and from our personal experience, we felt that Tableau is the better of the two when it comes to creating Dashboards and visualizations.&lt;/p&gt;

&lt;p&gt;Conclusion-
Tableau remains the choice of BI Tool for students from non-technical background like us. However, the professional reports and perfect visualisations by Power BI would certainly add a feather in your cap in terms of both skillset and employability.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;&lt;u&gt;Authored by&lt;/u&gt;&lt;/b&gt;: Chaitanya Gupta and Ritik Garg&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;&lt;u&gt;Co-Authored by&lt;/u&gt;&lt;/b&gt;: Mr Shoury Anand and Mr Vikrant Sharma
&lt;img src=&quot;/blog/tableau/3.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;
&lt;img src=&quot;/blog/tableau/4.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;
&lt;img src=&quot;/blog/tableau/5.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Initially, BI used to be the work of just IT professionals, who would supply the whole organisation with the reports for decision-making, however, now it is used at every stage of management by the managers (primarily due to the development in data science sphere and the rise of self- service BI tools).</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Decoding APIs?</title><link href="http://localhost:4000/blog/API-Basics/" rel="alternate" type="text/html" title="Decoding APIs?" /><published>2020-09-08T00:00:00+05:30</published><updated>2020-09-08T00:00:00+05:30</updated><id>http://localhost:4000/blog/API-Basics</id><content type="html" xml:base="http://localhost:4000/blog/API-Basics/">&lt;p&gt;What is an API?&lt;/p&gt;

&lt;p&gt;Application Program Interface. Wait you didn’t ask for its expansion, you wanted to know what an API actually meant and what it is used for; but sadly, this what you get to know once you google for an API and then you get extremely confused. As always, we’re here to help.&lt;/p&gt;

&lt;p&gt;An API, enables developers to integrate one app with another. They are the interfaces provided by servers that you can use to, among others, retrieve and send data using code.&lt;/p&gt;

&lt;p&gt;A simpler explanation?&lt;/p&gt;

&lt;p&gt;Let’s assume you wanted to ask Github to send you all the user details for your study. Even though you wanted the data for the study, Github cannot give you login credentials (email, password, etc.) of other users in your hands. But Github can do one thing, it can provide you with a portal/interface from where you could fetch limited user data (like only no. of commits made, no. of repositories, etc.). This interface is called an API. An API enables developers (like you ;)) to integrate one app (your python code) with another (like Github). They are the interfaces provided by servers that you can use to, among others, retrieve and send data using code.&lt;/p&gt;

&lt;p&gt;Why use APIs?&lt;/p&gt;

&lt;p&gt;APIs can help you fetch real-time data for your application in a well-structured way. They help in automating things that might have taken months if you did them manually using your web browser.&lt;/p&gt;

&lt;p&gt;How do APIs send data?&lt;/p&gt;

&lt;p&gt;Most of the APIs send data in JSON (JavaScript Object Notation). JSON is a lightweight format for storing and transporting data. It looks like a python dictionary or a python array. Hence, all the manipulation you did on python dictionaries and arrays come in handy here. It looks like:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;err&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;analytics&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bay&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
	&lt;span class=&quot;err&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Organization&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Nuclues&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
	&lt;span class=&quot;err&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;helpful&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Status Codes&lt;/p&gt;

&lt;p&gt;Before we deep dive into using APIs, we need to know how the server will respond to our requests. When a request fails, we can have different reasons for the same; maybe the server wasn’t functioning properly or we requested for something strange. All of this information is communicated using HTTP status codes, which are 3-digit numbers divided into categories.&lt;/p&gt;

&lt;p&gt;Informational responses (100–199),&lt;/p&gt;

&lt;p&gt;Successful responses (200–299), =&amp;gt;This is what we aim for&lt;/p&gt;

&lt;p&gt;Redirects (300–399),&lt;/p&gt;

&lt;p&gt;Client errors (400–499) =&amp;gt; The errors from client side (due to us)&lt;/p&gt;

&lt;p&gt;and Server errors (500–599) =&amp;gt; The errors from server side&lt;/p&gt;

&lt;p&gt;What is a request?&lt;/p&gt;

&lt;p&gt;You would encounter three major terminologies that sum up to make a request:&lt;/p&gt;

&lt;p&gt;Endpoints =&amp;gt; This is usually a url from which you need to fetch data&lt;/p&gt;

&lt;p&gt;Methods =&amp;gt; either GET, PUT, POST, or DELETE.&lt;/p&gt;

&lt;p&gt;Headers =&amp;gt; this contains information regarding authentication.&lt;/p&gt;

&lt;p&gt;When are we going to actually work with APIs?&lt;/p&gt;

&lt;p&gt;Now, Let’s fetch data from covid19api.com. First, we need to find the right endpoint. You can do so by looking up the API documentation of the same https://documenter.getpostman.com/view/10808728/SzS8rjbc?version=latest#7934d316-f751-4914-9909-39f1901caeb8&lt;/p&gt;

&lt;p&gt;We would be using an inbuilt library json (to convert json response to python object) and install the requests library (to use its methods to make requests for data)&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;		&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;installing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;json&lt;/span&gt;		&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;importing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;requests&lt;/span&gt;		&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;importing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;api_base_url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;https&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;covid19api&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;com&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dayone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;country&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;india&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confirmed&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;	&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endpoint&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt;	&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SOME&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KEY&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;	&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;use&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;only&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;you&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;need&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;API&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Content-Type'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'application/json'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Authorization'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Bearer {0}'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endpoint&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;protected&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_base_url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;		&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;json_response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'utf-8'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;		&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loads&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;convert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;python&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s Print json_response to see what we got
print(json_response)&lt;/p&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Cases':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'City':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'CityCode':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Country':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'India'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'CountryCode':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'IN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Date':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2020-01-30&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Z'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Lat':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;20.59&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Lon':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;78.96&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Province':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Status':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'confirmed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Cases':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'City':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'CityCode':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Country':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'India'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'CountryCode':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'IN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Date':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2020-01-31&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Z'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Lat':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;20.59&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Lon':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;78.96&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Province':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Status':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'confirmed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Cases':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;456183&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'City':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'CityCode':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Country':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'India'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'CountryCode':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'IN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Date':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2020-06-23&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Z'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Lat':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;20.59&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Lon':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;78.96&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Province':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Status':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'confirmed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Cases':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;473105&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'City':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'CityCode':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Country':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'India'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'CountryCode':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'IN'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Date':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2020-06-24&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Z'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Lat':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;20.59&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Lon':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;78.96&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Province':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'Status':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'confirmed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You can see we received an array of dictionaries. Now we need to iterate between the array to access data (keys and values) inside dictionary.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;Dates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Cases&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json_response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;Cases&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Cases'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;Dates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s plot the Dates with the no. of cases.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;12.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cases&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Close'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'0.95'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/blog/api.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And we told you! API’s did it. They can do much more wonders when used the right way for the right projects. You can even create one (but that’s a blog for another day). Hope you have a good understanding of the use case of this beautiful piece of technology in data science.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">What is an API?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ML CHALLENGE - CHURN MODELLING</title><link href="http://localhost:4000/blog/ML-CHALLENGE-CHURN-MODELLING/" rel="alternate" type="text/html" title="ML CHALLENGE - CHURN MODELLING" /><published>2020-07-28T00:00:00+05:30</published><updated>2020-07-28T00:00:00+05:30</updated><id>http://localhost:4000/blog/ML-CHALLENGE-CHURN-MODELLING</id><content type="html" xml:base="http://localhost:4000/blog/ML-CHALLENGE-CHURN-MODELLING/">&lt;p&gt;&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/ML CHALLENGE – CHURN MODEL.jpg&quot; alt=&quot;ML Challenge&quot; /&gt;
The topics covered in the series were,&lt;/p&gt;

&lt;p&gt;All about Machine Learning
Random Forest
Logistic Regression
K-Nearest Neighbours
Support Vector Machines (SVMs)
Clustering
Association rule learning with Apriori
Deep Learning and AI
To provide hands on practice so that our followers can now learn while practicing we opened a Machine Learning Challenge based on Churn Modelling. An important area of operation of banks is Churn Analysis. Churn Modelling or Analysis is the evaluation of the customer loss rate in order to reduce it. Banks are intrigued to identify segments of such customer as the cost for associating with a new customer is usually higher than retaining the old one.&lt;/p&gt;

&lt;p&gt;For this challenge, a dataset was provided wherein, a Bank is witnessing some unusual churn rates and thus they want to predict whether a customer will leave the bank or not, based on the data provided.&lt;/p&gt;

&lt;p&gt;The winner of the Challenge, Soham Mukherjee, majorly performed following 4 steps to solve this Challenge-&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Exploratory Data Analysis&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data Pre-Processing&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data Modelling&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Boosting Techniques&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Exploratory Data Analysis (EDA)
Imagine your friends decide to go out for dinner to a restaurant you have never heard of before. Being a foodie, you will straight away find yourself perplexed with several questions. As a rule of thumb, you would go online to check for menu, reviews, and ratings. Furthermore, you will dig out details on the experience of people from the restaurant.&lt;/p&gt;

&lt;p&gt;Whatever research you will undertake before finally booking your table for the dinner is nothing but what Data Scientists in their language call “Exploratory Data Analysis”&lt;/p&gt;

&lt;p&gt;EDA refers to the analytical procedure of performing preliminary investigations on data in order to discover patterns, to detect abnormalities in data, to test hypothesis and to check assumptions with the support of insights on synopsis and graphical representations of data provided.&lt;/p&gt;

&lt;p&gt;In this case, our Winner performed multiple EDA techniques including checking for skewness, visualizing with help of histogram plots, and checking the relationship between variables. Further distribution of dependent variable was depicted using a pie chart.&lt;/p&gt;

&lt;p&gt;Data is always first available in human readable form due to which the data is labelled with words. However, Label Encoding is applied to convert the words into numeric values and make the data in machine-readable form. Therefore, the categorical variables (Geography, Gender) were first encoded with the help of following code-
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/1.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once data was verified for duplications, the participant checked the skewness of the variables and then further illustrated the frequency-variable chart for each variable as shown below-
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/2.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then, some detailed visualizations were prepared with help of Pair Plots. One of the most significant analysis is to check the relationship between the two variables. To accomplish that so that further decisions could be made, a heat map was plotted that illustrating the correlation between the variables-
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/3.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since Correlation of X with itself is always 1, we get a diagonal which divides data into two triangles.&lt;/p&gt;

&lt;p&gt;Now that Exploratory Data Analysis has been performed and required results fetched, by going into the intricacies of data with help of certain visualizations, the next step would be Data Pre-processing to make data fit for Machine Learning Modelling.&lt;/p&gt;

&lt;p&gt;Data Pre-processing
This stage is one of the most significant steps in Machine Learning and no data scientist can ever skip this step in his/her analysis.&lt;/p&gt;

&lt;p&gt;To define in simple words, Data Preprocessing is a technique to convert the raw data which has been gathered from numerous sources into cleaner and more meaningful pieces of information. Datasets are incredibly massive and usually contain unnecessary data as well. That vast amount of information is also heterogeneous by nature, which means that they don’t share the same structure and thus there is also a need to standardize the data first. Data Preprocessing technically deals with issues such as inconsistent data, missing values, insufficient data, imbalance data, etc.&lt;/p&gt;

&lt;p&gt;In the winner’s entry, special attention was given to this step and he removed the “Balance” variable as it would decrease accuracy. It was observed from the correlation matrix between all the variables that Balance variable can reduce the accuracy of model as it was less related to the target and other variables.  Further to handle the imbalance and inconsistent data following two techniques were performed-&lt;/p&gt;

&lt;p&gt;SMOTE for Handling Imbalanced Data- 
Data imbalance usually reflects an unequal distribution of classes within a dataset and a major issue associated is that there are very few examples of the minority class in data for a model to effectively learn and predict. Therefore, to overcome this issue, one way is to oversample the examples in the minority class. The same can be achieved by replicating examples from the minority class in the training dataset before fitting a model. This can balance the class distribution without providing any additional information to the model. An improvement over replicating examples from the minority class is to synthesize new examples from the minority class.&lt;/p&gt;

&lt;p&gt;For the same, the most widely used approach to synthesizing new examples is called the Synthetic Minority Oversampling Technique (SMOTE). SMOTE first selects a minority class instance at random and finds its k-nearest minority class neighbours. These synthetic training records are generated by randomly selecting one or more of the k-nearest neighbours for each example in the minority class. After the oversampling process, the data is reconstructed and several classification models can be applied for the processed data.
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/4.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Standardization of Inconsistent Data-
The purpose of standardising the dataset is to ensure that each of the variables contributes equally to the analysis without any bias. It becomes essential to do standardization before applying Machine Learning Models as variables are the core of the model and the analysis is pretty much sensitive regarding the variance of initial variables. That is if there are variables with large differences between the initial variables, the variable with larger range will dominate over the smaller ranges.&lt;/p&gt;

&lt;p&gt;For example, consider two variables, distance within city and distance between cities. One of the variables is measured in meters while the other in Kilometres. Now if the data is not standardised then distance of 500 meters would be considered as greater than distance of 5KM, which isn’t true. Therefore, transforming the data to comparable scales can prevent this problem.&lt;/p&gt;

&lt;p&gt;Mathematically, it is done by subtracting the mean and dividing by the standard deviation for each value of each variable.&lt;/p&gt;

&lt;p&gt;z = (value – mean)/ (standard deviation)&lt;/p&gt;

&lt;p&gt;All the variables will be transformed to the same scale once the standardization is completed. Finally, the data is divided into training and test sets for further steps.&lt;/p&gt;

&lt;p&gt;Data Modelling
This is a technique to analyse data and acquiring certain results from it. There are various data modelling techniques which can be used such as Random Forest Regression, Logistic Regression, K Nearest Neighbours etc. All these techniques can be used to fit in the dataset and then predict the required outcome. For this purpose, dataset is divided into two categories-&lt;/p&gt;

&lt;p&gt;Train set- For fitting the model.
Test set- To predict the required outcome.
The various data modelling techniques used in this case are-&lt;/p&gt;

&lt;p&gt;Logistic Regression
Random Forest Classifier
Decision Tree Classifier
Support Vector Machines
K nearest neighbours
Here is the code and confusion matrix for the Random Forest Classifier;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/5.png&quot; alt=&quot;ML Challenge&quot; /&gt;
Heatmap from the seaborn library has been used in the Confusion Matrix for better understanding-&lt;/p&gt;

&lt;p&gt;The confusion matrix shows that among all positive predictions, 84.4% of the predictions were true positives and it was correctly predicted that a customer will churn. Rest 15.6% were false positives which means that it was wrongly predicted that these customers will churn.&lt;/p&gt;

&lt;p&gt;Similarly, out of the negative predictions, 83.8% were true negatives while rest 16.2% were false negatives and they were wrongly predicted.
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/6.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Boosting Techniques-
Boosting techniques helps to convert weak learners into strong learners. The various boosting techniques used in the case are-&lt;/p&gt;

&lt;p&gt;Gradient Boosting Classifier
Ada boost Classifier
XG Boost
Here is a sample code for Gradient Boosting Classifier
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/7.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The winner has used certain techniques to increase the accuracy of the model in the XG Boost technique-&lt;/p&gt;

&lt;p&gt;Random search has been used to find out the best parameters with the help of a Param grid. It searches among a given set of values to find out parameters with highest accuracy.
Stratified-K-Fold shuffles the data and then split the dataset into n-splits (specified number of splits) Then it uses each part as a test set.
When applying machine learning models, one usually does data pre-processing (as explained above), feature engineering, feature extraction and, feature selection. After this, he/ she selects the best algorithm and tuning of the parameters is done in order to obtain the best results.&lt;/p&gt;

&lt;p&gt;AutoML is a series of concepts and techniques used to automate these processes. It reduces the bias and errors that occur when a human being is designing the machine learning models. An organization can also reduce the cost of hiring many experts by applying AutoML in their data pipeline. The winner achieved the best accuracy with the help of Stacked Ensemble (Auto ML).&lt;/p&gt;

&lt;p&gt;We hope that you understood the framework used in order to solve this Challenge. Also, if the training time of model was increased from 5 minutes to probably a couple of hours, the model, the accuracy would have been around a remarkable 94-96%. In case of any queries, feel free to reach out to us.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">The topics covered in the series were,</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Data Visualization in R using ggplot2 (PART 2)</title><link href="http://localhost:4000/blog/Data-Visualization-in-R-Part2/" rel="alternate" type="text/html" title="Data Visualization in R using ggplot2 (PART 2)" /><published>2020-07-14T00:00:00+05:30</published><updated>2020-07-14T00:00:00+05:30</updated><id>http://localhost:4000/blog/Data-Visualization-in-R-Part2</id><content type="html" xml:base="http://localhost:4000/blog/Data-Visualization-in-R-Part2/">&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Featured.jpg&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
PLOT 11 - JITTER
&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot11 jitter.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
So, what’s a jitter? Jitter is a random value that is assigned to the variable so they don’t fall on top of each other.&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y= price,x=cut,color=cut))  + geom_jitter(size=1.2, alpha= 0.5)&lt;/p&gt;

&lt;p&gt;PLOT 12 - Box Plot&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y= price,x=cut,color=cut))  + geom_boxplot(size=1.2)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/2.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Box pot is a method to show data through their quartiles. The line in middle shows the median of the range. This gives us a hint about skewness and some other characteristics of the data.&lt;/p&gt;

&lt;p&gt;The dots out of the box represent outliers. It is a useful tool to quickly represent outliers.&lt;/p&gt;

&lt;p&gt;Here it can be observed that median of the premium quality cuts is higher, this sounds very reasonable because the premium quality does have high prices.&lt;/p&gt;

&lt;p&gt;PLOT 13 -  Histogram&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, color= “Blue”, fill= “White”)
&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/3.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The histogram represents the price of a diamond and the no. of diamonds in each price range. Binwidth here represents the width of each bar in the histograms. Please note that here colour represents the outline colour of the bar and fill represents the inside. A key take away from here is that almost more than 15000 diamonds lie in the range of 1000$-2000$. But remember, what we’ve tried now is setting i.e assigning one colour to the whole plot. Let’s try mapping -&lt;/p&gt;

&lt;p&gt;PLOT 14 -  Mapping Histogram&lt;/p&gt;

&lt;p&gt;Trying out mapping -&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, aes( fill= clarity))
&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/4-Plot15-Adding Color.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now we have the categorization based on clarity in each price range but the graph looks distorted and cluttered in. Let’s try to add a borderline to see a better view.&lt;/p&gt;

&lt;p&gt;PLOT 15 -  Adding Color&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, aes( fill= clarity),color=”Black”)&lt;/p&gt;

&lt;p&gt;The chart now looks good and is understandable. Let’s move on to explore facets now -&lt;/p&gt;

&lt;p&gt;Facets –&lt;/p&gt;

&lt;p&gt;Facets are used to create subdivision. For example - we want to see the price of each diamond concerning its cut independently. We will use facets to divide the diamonds according to their cuts. The code for creating distinct plots in one bar is – facet_grid(Rows~Columns).&lt;/p&gt;

&lt;p&gt;Replace rows for dividing charts in the form of rows and columns for the same. Let’s try it out -&lt;/p&gt;

&lt;p&gt;PLOT 16 -  Scatter Plot (Facet Version)&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(x = carat ,y=price)) + geom_point() + facet_grid(clarity~.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot-16-Scatter Plot.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
Here, we have a relation between price and carat of each category of clarity. Each category of clarity has a different scatter plot in the form of rows. We would like you to try the same thing in the column form of facet_grid. Code would appear something like this -&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(x = carat ,y=price)) + geom_point() + facet_grid(~Clarity)&lt;/p&gt;

&lt;p&gt;PLOT 17 -&lt;/p&gt;

&lt;p&gt;Now we’ll use both row and column&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(x = carat ,y=price)) + geom_point() + facet_grid(clarity~cut)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot 17.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The chart now is depicting relationship between carat and price on the basis of the quality of cut as well as clarity.Let’s refine our chart to make it visually appealing.&lt;/p&gt;

&lt;p&gt;PLOT 18 -  Refining Visualization&lt;/p&gt;

&lt;p&gt;Code -  ggplot(data=diamond, aes(x = carat ,y=price)) + geom_point(aes(color= cut),size=0.5,alpha=0.5) + facet_grid(clarity~cut)+geom_smooth()&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot 18 Refining viz.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ve reduced the size of the dots to make it easy for us to interpret. Smoother is added to identify trends, if there’s any. It can be seen from the chart that diamonds with I1 clarity and very good cut has almost equal price range as the premium cut. 
The cheapest diamonds are here seen to be of VVS2 category with fair cut, which makes sense.
Coordinates –&lt;/p&gt;

&lt;p&gt;Co-ordinates deal with adjusting or zooming in and out of your coordinates. Let’s zoom in to one of the previously made visualizations.&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, aes( fill= clarity),color=”Black”) + coord_cartesian(ylim = c(0,10000))&lt;/p&gt;

&lt;p&gt;PLOT 19 -&lt;/p&gt;

&lt;p&gt;The y-axis is zoomed in from (0-10,000), there is also another method of limiting your axes. You can similarly limit the x-axis as well. However, that method sometimes cuts your data and is not advisable. The code is given here, we would suggest you to try it yourself and identify this limitation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot 19.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, aes( fill= clarity),color=”Black”) + ylim(0,10000)&lt;/p&gt;

&lt;p&gt;PLOT 20 -&lt;/p&gt;

&lt;p&gt;Let’s now try zooming in both the axes -&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, aes( fill= clarity),color=”Black”) +&lt;/p&gt;

&lt;p&gt;coord_cartesian(ylim = c(0,10000),xlim = c(0,10000))&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot 20.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ve successfully zoomed in both the axes, we believe that is all there is tell about axes. Let’s move on to themes.&lt;/p&gt;

&lt;p&gt;Themes –&lt;/p&gt;

&lt;p&gt;Theme is all about formatting your axes labels, adjusting and positioning legend. This should’ve been done to every chart but you can of course go and apply this to every chart.&lt;/p&gt;

&lt;p&gt;Let’s start with one of our most basic plots and start giving it the elements of themes.&lt;/p&gt;

&lt;p&gt;Starting with labels and titles -&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y=price,x=carat, color= cut)) + geom_point(alpha=0.5) + xlab(“Diamond carat”) + ylab(“Price”) + ggtitle(“Relationship between Price and Carat”)&lt;/p&gt;

&lt;p&gt;PLOT 21 -  Giving label&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot 21 giving labe_.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
The chart looks more descriptive, but there is still a lot of scope for improvement. Let’s try to change the font of labels.&lt;/p&gt;

&lt;p&gt;PLOT 22 -  Adjusting Label&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y=price,x=carat, color= cut)) + geom_point(alpha=0.5) + xlab(“Diamond carat”) + ylab(“Price”) +&lt;/p&gt;

&lt;p&gt;ggtitle(“Relationship between Price and Carat”) +&lt;/p&gt;

&lt;p&gt;theme(axis.title.x = element_text(color = “Dark Blue”, size = 30),&lt;/p&gt;

&lt;p&gt;axis.title.y = element_text(color = “Dark Blue”, size = 30),&lt;/p&gt;

&lt;p&gt;axis.text.x = element_text(size=20), axis.text.y = element_text(size=20))&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot22 Adjust label.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
Finally, Lets adjust labels and get done with it.&lt;/p&gt;

&lt;p&gt;PLOT 23 -  Final Touch&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y=price,x=carat, color= cut)) + geom_point(alpha=0.5) + xlab(“Diamond carat”) + ylab(“Price”) +  ggtitle(“Relationship between Price and Carat”) +&lt;/p&gt;

&lt;p&gt;theme(axis.title.x = element_text(color = “Dark Blue”, size = 20),axis.title.y = element_text(color = “Dark Blue”, size = 20),axis.text.x = element_text(size=20),axis.text.y = element_text(size=20),legend.title = element_text(size = 15),legend.text  = element_text(size = 15),legend.position = c(1,1),legend.justification = c(1,1), plot.title = element_text(color=”Dark Blue”,size=20,family = “Courier”))&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot23 final touch.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
Finally, we’re done with everything. I would suggest you to try things on your own and experiment things in order to get a better view yourself.&lt;/p&gt;

&lt;p&gt;Before proceeding towards visualization, it is highly suggested to be aware about all the elements of your data set and what is to be achieved from it.&lt;/p&gt;

&lt;p&gt;GGPLOT2, in it has even more fascinating and useful tools. We suggest our readers to explore more of it and learn more about it.&lt;/p&gt;

&lt;p&gt;The use of visualization techniques differ according to the purpose it serves but the underlying idea essentially remains the same. We hope that the basic idea and concept of visualization is clear to all of you. So, play around with these visualizations and do write to us in case of any queries. All the best!!!&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">PLOT 11 - JITTER So, what’s a jitter? Jitter is a random value that is assigned to the variable so they don’t fall on top of each other.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>