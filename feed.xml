<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-09-08T19:37:54+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">The Analytics Bay</title><subtitle>Always Analyzing</subtitle><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><entry><title type="html">The Business Intelligence Project</title><link href="http://localhost:4000/blog/The-Business-Intelligence-Project/" rel="alternate" type="text/html" title="The Business Intelligence Project" /><published>2020-09-08T00:00:00+05:30</published><updated>2020-09-08T00:00:00+05:30</updated><id>http://localhost:4000/blog/The-Business-Intelligence-Project</id><content type="html" xml:base="http://localhost:4000/blog/The-Business-Intelligence-Project/">&lt;p&gt;Initially, BI used to be the work of just IT professionals, who would supply the whole organisation with the reports for decision-making, however, now it is used at every stage of management by the managers (primarily due to the development in data science sphere and the rise of self- service BI tools).&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Ways in which BI can help organisations make smart, data- driven decisions-&lt;/b&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Tracking performance&lt;/li&gt;
  &lt;li&gt;Analysing Key Performance Indicators (KPI)&lt;/li&gt;
  &lt;li&gt;Analysing the market share and consumer behaviour&lt;/li&gt;
  &lt;li&gt;Optimizing business operations using historical data&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;b&gt; Business Intelligence Tools &lt;/b&gt;
&lt;br /&gt;
Business intelligence tools are nothing but software that are used to analyse trends and extract insights out of the data in order to make tactical and strategic business decisions.
There are many Business Intelligence tools like SAS BI, MicroStrategy, Datapine, Domo, etc that help in carrying out the necessary tasks but two of the most powerful and widely used tools in Business Intelligence on which this blog is also dedicated are Power BI and Tableau.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Power BI&lt;/b&gt;
&lt;br /&gt;
Power BI provides a summary of the data in the form of reports and dashboards while connecting with every data source across different. It makes data assessment, sharing scalable dashboards, embedded visualizations, interactive reports and various another feature which we will see further in the blog. It is amazing at importing visualizations with easy-to-use and user-friendly interfaces like Excel, etc. Power BI is simple for using that provides a full overview of your business performance.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Tableau&lt;/b&gt;
&lt;br /&gt;
While there are numerous intuitive business intelligence tools, Tableau uses data discovery and its interpretation for all types of the business user. It is sometimes quoted as the most user-friendly data science tool available. Being simple to handle, any user without much of a stretch can perform reading or writing data related tasks along with analysing key data insights, while creating visualizations and innovative reports, and finally sharing insights throughout the enterprise with help of dashboards and stories. Tableau is good at connecting to any data source with a drag-and-drop interface that is easy-to-use and makes transferring data simple. However, unlike Power BI it is weak at combining with different data sources for analysis.&lt;/p&gt;

&lt;p&gt;INSIGHTS&lt;/p&gt;

&lt;p&gt;As we have always said, hands-on real-time experience on analytics software are any day better than the theoretical concepts and is also our core strength, so to compare how the visualisations would look in Power BI and Tableau we took a same dataset and created a dashboard. We undertook a project under the supervision of our esteemed alumnus, &lt;u&gt;Mr. Vikrant Sharma&lt;/u&gt; (Analyst, InMobi) and &lt;u&gt;Mr. Shoury Anand&lt;/u&gt;(IIM Lucknow, PGP’22). This included a study on COVID-19 scenario where we took 5 datasets which contained data for number of COVID cases worldwide and in India. Also, to study what impact the current pandemic has on the indices we took time series data for S&amp;amp;P Global, FTSE 100 and Dow Jones along with Gold and Crude Oil prices starting from 1st January 2020 to 31st July 2020. To study how GDP has fared in the past years, we took Real GDP for 180 countries for past 13 years. Not surprisingly, the visualizations said for itself all the data which we collected and wanted to concluded our results. Finally, both the dashboards looked pretty amazing and gave us a hard time to choose one over another. So the question in on our readers which visualisation did they find better!&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Tableau Dashboard&lt;/b&gt;
&lt;img src=&quot;/blog/tableau/Tableau Dashboard.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Power BI Dashboard&lt;/b&gt; 
&lt;img src=&quot;/blog/tableau/PowerBI Dashboard.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;
&lt;br /&gt;
We have conducted a survey of sample size of 100 to gather and access the opinion of college students on Business Intelligence tools namely Power BI and Tableau. 
The respondents had to answer various questions; like on what parameters would they favour one software from the other, rating these softwares on User- friendliness and attractiveness, their personal opinions/ experiences among others.
Most of the students replied on the same lines and the results matched our findings/ expectations. Tableau was found to be the favourable of the two.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Some of the opinions-&lt;/b&gt;
&lt;i&gt;“The Power BI Student Version has a lot of limitations and thus though it is capable to a large extent, cannot be used to its full capacity. Tableau has some limited features as compared to Power BI w.r.t graph styles, colour available, etc. Overall, Both the tools are very powerful for Data Viz”&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;“I feel that use of visualizations created in Tableau should be restricted to Tableau itself. Any presentation that requires only and only visualizations with little to none supporting text can be presented through Tableau’s story mode. For PowerPoint presentations, I like to connect excel sheets with it for charts, as it makes it easy for me to change the formatting of the chart within PowerPoint and any change in figures can also be speedily done.
And about dashboards in presentations, I don’t think that is a good choice because the number of visualizations in a dashboard might distract an audience, and it is difficult to demonstrate its dynamic features on a presentation.”&lt;/p&gt;

&lt;p&gt;&lt;i&gt;I have had a better experience working with Power BI over tableau due to its user-friendly nature and a relatively easier visualisation capability.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Result of the survey were as follows-&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/tableau/1.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;
&lt;img src=&quot;/blog/tableau/2.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BUSINESS INTELLIGENCE TOOLS FROM A STUDENTS’ PERSPECTIVE 
Microsoft’s Power BI is a suite of data visualization and business analytic tools. It offers tools to easily analyse, transform and visualize data pipelines, including the ability to build reusable models. The software enables users to integrate their apps, to deliver reports along with real-time dashboards.
Tableau is a visualization tool that helps businesses transform their data into insights that can lead to action. The tool makes it easy to connect data in almost any format from almost any source. Interactive dashboard with visual analytics can be created with simple dragging and dropping, and data transformed in graphs, maps, charts, and other visualizations.&lt;/p&gt;

&lt;p&gt;Data Visualizations tools are necessary when it comes to creating a visual representation of analytics and sharing insights with other. 
And as Management students, we faced difficulty initially in choosing the best tool for making reports and dashboards, because both Power BI and Tableau were terrific and had a lot to offer. So, to make life simpler and easier, we’ll be comparing these two on a range of parameters-&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Cost&lt;/b&gt;
Tableau is expensive than Power BI when it comes to buying the complete version which usually is bought by the businesses and working professionals. Tableau’s annual price ranges around $1000 while Power BI’s annual price ranges around $100. Even though there is a great price difference in the full version, however if we compare only the free version Tableau has an upper hand. This is because many useful features were not included in Power BI’s free desktop version and this certainly impaired us during this project. We also had to face many hassles when it came to editing the shared files.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Data Analysis&lt;/b&gt;
Power BI should be the choice if someone wants to have an in- depth analysis of the data. It offers DAX (Data Analysis Expressions) which is a delight to work with. It is designed to work with tables and relational database, creating meaningful relationships between various data sources.
Tableau has in-built features like data blending and drill-down, which one can use to determine the variations, data patterns and for further data analysis.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Data Visualization&lt;/b&gt;
There is no one best tool when it comes to Data Visualization as both Power BI and Tableau has some really powerful visuals to offer. While talking about simplicity, surely Tableau creates fascinating dashboards through simple drag- and- drop, and complex calculations can also be made with the help of simple line of codes. It offers various types of visualizations such as Heat maps, Treemaps, Scatter Plots etc. One can also create ‘Word clouds’ and ‘Bubble charts’ in Tableau.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;On the other hand, Power BI has loads of visualisations which help in depth analysis. Power BI boasts of a wide variety of visualizations, such as R script visuals and Python visuals as well. These visuals can be created in Power BI Desktop and then published online.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Performance&lt;/b&gt;
The Clock Rate in Power BI is faster than that in Tableau, it loads data sets faster than the latter, plus it saves the files in a compressed manner and takes lower disk space. One can also publish their Power BI Desktop reports online and thus can have an easy access.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;User community&lt;/b&gt;
Both these tools offer great customer support, in terms of services and learning material. However, Tableau may have an upper edge in community support due to its huge user base and awareness in general.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;User Interface&lt;/b&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Tableau has an edge over Power BI when it comes to the user interface. It has an intuitive user interface along with versatile features. Its Tool Tip is more efficient than Power BI’s and provides elaborate details.
Power BI’s user interface is no less than that of Tableau’s but it can be a little intimidating for a novice.&lt;/p&gt;

&lt;p&gt;After comparing the 2 tools on various parameters and from our personal experience, we felt that Tableau is the better of the two when it comes to creating Dashboards and visualizations.&lt;/p&gt;

&lt;p&gt;Conclusion- 
Tableau remains the choice of BI Tool for students from non-technical background like us. However, the professional reports and perfect visualisations by Power BI would certainly add a feather in your cap in terms of both skillset and employability.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;&lt;u&gt;Authored by&lt;/u&gt;&lt;/b&gt;: Chaitanya Gupta and Ritik Garg&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;&lt;u&gt;Co-Authored by&lt;/u&gt;&lt;/b&gt;: Mr Shoury Anand and Mr Vikrant Sharma
&lt;img src=&quot;/blog/tableau/3.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;
&lt;img src=&quot;/blog/tableau/4.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;
&lt;img src=&quot;/blog/tableau/5.jpeg&quot; alt=&quot;Tableau versus Power Bi&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Initially, BI used to be the work of just IT professionals, who would supply the whole organisation with the reports for decision-making, however, now it is used at every stage of management by the managers (primarily due to the development in data science sphere and the rise of self- service BI tools).</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ML CHALLENGE - CHURN MODELLING</title><link href="http://localhost:4000/blog/ML-CHALLENGE-CHURN-MODELLING/" rel="alternate" type="text/html" title="ML CHALLENGE - CHURN MODELLING" /><published>2020-07-28T00:00:00+05:30</published><updated>2020-07-28T00:00:00+05:30</updated><id>http://localhost:4000/blog/ML-CHALLENGE-CHURN-MODELLING</id><content type="html" xml:base="http://localhost:4000/blog/ML-CHALLENGE-CHURN-MODELLING/">&lt;p&gt;&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/ML CHALLENGE – CHURN MODEL.jpg&quot; alt=&quot;ML Challenge&quot; /&gt;
The topics covered in the series were,&lt;/p&gt;

&lt;p&gt;All about Machine Learning
Random Forest
Logistic Regression
K-Nearest Neighbours
Support Vector Machines (SVMs)
Clustering
Association rule learning with Apriori
Deep Learning and AI
To provide hands on practice so that our followers can now learn while practicing we opened a Machine Learning Challenge based on Churn Modelling. An important area of operation of banks is Churn Analysis. Churn Modelling or Analysis is the evaluation of the customer loss rate in order to reduce it. Banks are intrigued to identify segments of such customer as the cost for associating with a new customer is usually higher than retaining the old one.&lt;/p&gt;

&lt;p&gt;For this challenge, a dataset was provided wherein, a Bank is witnessing some unusual churn rates and thus they want to predict whether a customer will leave the bank or not, based on the data provided.&lt;/p&gt;

&lt;p&gt;The winner of the Challenge, Soham Mukherjee, majorly performed following 4 steps to solve this Challenge-&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Exploratory Data Analysis&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data Pre-Processing&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data Modelling&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Boosting Techniques&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Exploratory Data Analysis (EDA)
Imagine your friends decide to go out for dinner to a restaurant you have never heard of before. Being a foodie, you will straight away find yourself perplexed with several questions. As a rule of thumb, you would go online to check for menu, reviews, and ratings. Furthermore, you will dig out details on the experience of people from the restaurant.&lt;/p&gt;

&lt;p&gt;Whatever research you will undertake before finally booking your table for the dinner is nothing but what Data Scientists in their language call “Exploratory Data Analysis”&lt;/p&gt;

&lt;p&gt;EDA refers to the analytical procedure of performing preliminary investigations on data in order to discover patterns, to detect abnormalities in data, to test hypothesis and to check assumptions with the support of insights on synopsis and graphical representations of data provided.&lt;/p&gt;

&lt;p&gt;In this case, our Winner performed multiple EDA techniques including checking for skewness, visualizing with help of histogram plots, and checking the relationship between variables. Further distribution of dependent variable was depicted using a pie chart.&lt;/p&gt;

&lt;p&gt;Data is always first available in human readable form due to which the data is labelled with words. However, Label Encoding is applied to convert the words into numeric values and make the data in machine-readable form. Therefore, the categorical variables (Geography, Gender) were first encoded with the help of following code-
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/1.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once data was verified for duplications, the participant checked the skewness of the variables and then further illustrated the frequency-variable chart for each variable as shown below-
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/2.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then, some detailed visualizations were prepared with help of Pair Plots. One of the most significant analysis is to check the relationship between the two variables. To accomplish that so that further decisions could be made, a heat map was plotted that illustrating the correlation between the variables-
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/3.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since Correlation of X with itself is always 1, we get a diagonal which divides data into two triangles.&lt;/p&gt;

&lt;p&gt;Now that Exploratory Data Analysis has been performed and required results fetched, by going into the intricacies of data with help of certain visualizations, the next step would be Data Pre-processing to make data fit for Machine Learning Modelling.&lt;/p&gt;

&lt;p&gt;Data Pre-processing
This stage is one of the most significant steps in Machine Learning and no data scientist can ever skip this step in his/her analysis.&lt;/p&gt;

&lt;p&gt;To define in simple words, Data Preprocessing is a technique to convert the raw data which has been gathered from numerous sources into cleaner and more meaningful pieces of information. Datasets are incredibly massive and usually contain unnecessary data as well. That vast amount of information is also heterogeneous by nature, which means that they don’t share the same structure and thus there is also a need to standardize the data first. Data Preprocessing technically deals with issues such as inconsistent data, missing values, insufficient data, imbalance data, etc.&lt;/p&gt;

&lt;p&gt;In the winner’s entry, special attention was given to this step and he removed the “Balance” variable as it would decrease accuracy. It was observed from the correlation matrix between all the variables that Balance variable can reduce the accuracy of model as it was less related to the target and other variables.  Further to handle the imbalance and inconsistent data following two techniques were performed-&lt;/p&gt;

&lt;p&gt;SMOTE for Handling Imbalanced Data- 
Data imbalance usually reflects an unequal distribution of classes within a dataset and a major issue associated is that there are very few examples of the minority class in data for a model to effectively learn and predict. Therefore, to overcome this issue, one way is to oversample the examples in the minority class. The same can be achieved by replicating examples from the minority class in the training dataset before fitting a model. This can balance the class distribution without providing any additional information to the model. An improvement over replicating examples from the minority class is to synthesize new examples from the minority class.&lt;/p&gt;

&lt;p&gt;For the same, the most widely used approach to synthesizing new examples is called the Synthetic Minority Oversampling Technique (SMOTE). SMOTE first selects a minority class instance at random and finds its k-nearest minority class neighbours. These synthetic training records are generated by randomly selecting one or more of the k-nearest neighbours for each example in the minority class. After the oversampling process, the data is reconstructed and several classification models can be applied for the processed data.
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/4.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Standardization of Inconsistent Data-
The purpose of standardising the dataset is to ensure that each of the variables contributes equally to the analysis without any bias. It becomes essential to do standardization before applying Machine Learning Models as variables are the core of the model and the analysis is pretty much sensitive regarding the variance of initial variables. That is if there are variables with large differences between the initial variables, the variable with larger range will dominate over the smaller ranges.&lt;/p&gt;

&lt;p&gt;For example, consider two variables, distance within city and distance between cities. One of the variables is measured in meters while the other in Kilometres. Now if the data is not standardised then distance of 500 meters would be considered as greater than distance of 5KM, which isn’t true. Therefore, transforming the data to comparable scales can prevent this problem.&lt;/p&gt;

&lt;p&gt;Mathematically, it is done by subtracting the mean and dividing by the standard deviation for each value of each variable.&lt;/p&gt;

&lt;p&gt;z = (value – mean)/ (standard deviation)&lt;/p&gt;

&lt;p&gt;All the variables will be transformed to the same scale once the standardization is completed. Finally, the data is divided into training and test sets for further steps.&lt;/p&gt;

&lt;p&gt;Data Modelling
This is a technique to analyse data and acquiring certain results from it. There are various data modelling techniques which can be used such as Random Forest Regression, Logistic Regression, K Nearest Neighbours etc. All these techniques can be used to fit in the dataset and then predict the required outcome. For this purpose, dataset is divided into two categories-&lt;/p&gt;

&lt;p&gt;Train set- For fitting the model.
Test set- To predict the required outcome.
The various data modelling techniques used in this case are-&lt;/p&gt;

&lt;p&gt;Logistic Regression
Random Forest Classifier
Decision Tree Classifier
Support Vector Machines
K nearest neighbours
Here is the code and confusion matrix for the Random Forest Classifier;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/5.png&quot; alt=&quot;ML Challenge&quot; /&gt;
Heatmap from the seaborn library has been used in the Confusion Matrix for better understanding-&lt;/p&gt;

&lt;p&gt;The confusion matrix shows that among all positive predictions, 84.4% of the predictions were true positives and it was correctly predicted that a customer will churn. Rest 15.6% were false positives which means that it was wrongly predicted that these customers will churn.&lt;/p&gt;

&lt;p&gt;Similarly, out of the negative predictions, 83.8% were true negatives while rest 16.2% were false negatives and they were wrongly predicted.
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/6.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Boosting Techniques-
Boosting techniques helps to convert weak learners into strong learners. The various boosting techniques used in the case are-&lt;/p&gt;

&lt;p&gt;Gradient Boosting Classifier
Ada boost Classifier
XG Boost
Here is a sample code for Gradient Boosting Classifier
&lt;img src=&quot;/blog/2020-07-28-ML CHALLENGE - CHURN MODEL/Winner_s Python code/7.png&quot; alt=&quot;ML Challenge&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The winner has used certain techniques to increase the accuracy of the model in the XG Boost technique-&lt;/p&gt;

&lt;p&gt;Random search has been used to find out the best parameters with the help of a Param grid. It searches among a given set of values to find out parameters with highest accuracy.
Stratified-K-Fold shuffles the data and then split the dataset into n-splits (specified number of splits) Then it uses each part as a test set.
When applying machine learning models, one usually does data pre-processing (as explained above), feature engineering, feature extraction and, feature selection. After this, he/ she selects the best algorithm and tuning of the parameters is done in order to obtain the best results.&lt;/p&gt;

&lt;p&gt;AutoML is a series of concepts and techniques used to automate these processes. It reduces the bias and errors that occur when a human being is designing the machine learning models. An organization can also reduce the cost of hiring many experts by applying AutoML in their data pipeline. The winner achieved the best accuracy with the help of Stacked Ensemble (Auto ML).&lt;/p&gt;

&lt;p&gt;We hope that you understood the framework used in order to solve this Challenge. Also, if the training time of model was increased from 5 minutes to probably a couple of hours, the model, the accuracy would have been around a remarkable 94-96%. In case of any queries, feel free to reach out to us.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">The topics covered in the series were,</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Data Visualization in R using ggplot2 (PART 2)</title><link href="http://localhost:4000/blog/Data-Visualization-in-R-Part2/" rel="alternate" type="text/html" title="Data Visualization in R using ggplot2 (PART 2)" /><published>2020-07-14T00:00:00+05:30</published><updated>2020-07-14T00:00:00+05:30</updated><id>http://localhost:4000/blog/Data-Visualization-in-R-Part2</id><content type="html" xml:base="http://localhost:4000/blog/Data-Visualization-in-R-Part2/">&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Featured.jpg&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
PLOT 11 - JITTER
&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot11 jitter.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
So, what’s a jitter? Jitter is a random value that is assigned to the variable so they don’t fall on top of each other.&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y= price,x=cut,color=cut))  + geom_jitter(size=1.2, alpha= 0.5)&lt;/p&gt;

&lt;p&gt;PLOT 12 - Box Plot&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y= price,x=cut,color=cut))  + geom_boxplot(size=1.2)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/2.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Box pot is a method to show data through their quartiles. The line in middle shows the median of the range. This gives us a hint about skewness and some other characteristics of the data.&lt;/p&gt;

&lt;p&gt;The dots out of the box represent outliers. It is a useful tool to quickly represent outliers.&lt;/p&gt;

&lt;p&gt;Here it can be observed that median of the premium quality cuts is higher, this sounds very reasonable because the premium quality does have high prices.&lt;/p&gt;

&lt;p&gt;PLOT 13 -  Histogram&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, color= “Blue”, fill= “White”)
&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/3.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The histogram represents the price of a diamond and the no. of diamonds in each price range. Binwidth here represents the width of each bar in the histograms. Please note that here colour represents the outline colour of the bar and fill represents the inside. A key take away from here is that almost more than 15000 diamonds lie in the range of 1000$-2000$. But remember, what we’ve tried now is setting i.e assigning one colour to the whole plot. Let’s try mapping -&lt;/p&gt;

&lt;p&gt;PLOT 14 -  Mapping Histogram&lt;/p&gt;

&lt;p&gt;Trying out mapping -&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, aes( fill= clarity))
&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/4-Plot15-Adding Color.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now we have the categorization based on clarity in each price range but the graph looks distorted and cluttered in. Let’s try to add a borderline to see a better view.&lt;/p&gt;

&lt;p&gt;PLOT 15 -  Adding Color&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, aes( fill= clarity),color=”Black”)&lt;/p&gt;

&lt;p&gt;The chart now looks good and is understandable. Let’s move on to explore facets now -&lt;/p&gt;

&lt;p&gt;Facets –&lt;/p&gt;

&lt;p&gt;Facets are used to create subdivision. For example - we want to see the price of each diamond concerning its cut independently. We will use facets to divide the diamonds according to their cuts. The code for creating distinct plots in one bar is – facet_grid(Rows~Columns).&lt;/p&gt;

&lt;p&gt;Replace rows for dividing charts in the form of rows and columns for the same. Let’s try it out -&lt;/p&gt;

&lt;p&gt;PLOT 16 -  Scatter Plot (Facet Version)&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(x = carat ,y=price)) + geom_point() + facet_grid(clarity~.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot-16-Scatter Plot.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
Here, we have a relation between price and carat of each category of clarity. Each category of clarity has a different scatter plot in the form of rows. We would like you to try the same thing in the column form of facet_grid. Code would appear something like this -&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(x = carat ,y=price)) + geom_point() + facet_grid(~Clarity)&lt;/p&gt;

&lt;p&gt;PLOT 17 -&lt;/p&gt;

&lt;p&gt;Now we’ll use both row and column&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(x = carat ,y=price)) + geom_point() + facet_grid(clarity~cut)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot 17.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The chart now is depicting relationship between carat and price on the basis of the quality of cut as well as clarity.Let’s refine our chart to make it visually appealing.&lt;/p&gt;

&lt;p&gt;PLOT 18 -  Refining Visualization&lt;/p&gt;

&lt;p&gt;Code -  ggplot(data=diamond, aes(x = carat ,y=price)) + geom_point(aes(color= cut),size=0.5,alpha=0.5) + facet_grid(clarity~cut)+geom_smooth()&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot 18 Refining viz.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ve reduced the size of the dots to make it easy for us to interpret. Smoother is added to identify trends, if there’s any. It can be seen from the chart that diamonds with I1 clarity and very good cut has almost equal price range as the premium cut. 
The cheapest diamonds are here seen to be of VVS2 category with fair cut, which makes sense.
Coordinates –&lt;/p&gt;

&lt;p&gt;Co-ordinates deal with adjusting or zooming in and out of your coordinates. Let’s zoom in to one of the previously made visualizations.&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, aes( fill= clarity),color=”Black”) + coord_cartesian(ylim = c(0,10000))&lt;/p&gt;

&lt;p&gt;PLOT 19 -&lt;/p&gt;

&lt;p&gt;The y-axis is zoomed in from (0-10,000), there is also another method of limiting your axes. You can similarly limit the x-axis as well. However, that method sometimes cuts your data and is not advisable. The code is given here, we would suggest you to try it yourself and identify this limitation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot 19.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, aes( fill= clarity),color=”Black”) + ylim(0,10000)&lt;/p&gt;

&lt;p&gt;PLOT 20 -&lt;/p&gt;

&lt;p&gt;Let’s now try zooming in both the axes -&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes( x = price)) + geom_histogram(binwidth = 1000, aes( fill= clarity),color=”Black”) +&lt;/p&gt;

&lt;p&gt;coord_cartesian(ylim = c(0,10000),xlim = c(0,10000))&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot 20.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ve successfully zoomed in both the axes, we believe that is all there is tell about axes. Let’s move on to themes.&lt;/p&gt;

&lt;p&gt;Themes –&lt;/p&gt;

&lt;p&gt;Theme is all about formatting your axes labels, adjusting and positioning legend. This should’ve been done to every chart but you can of course go and apply this to every chart.&lt;/p&gt;

&lt;p&gt;Let’s start with one of our most basic plots and start giving it the elements of themes.&lt;/p&gt;

&lt;p&gt;Starting with labels and titles -&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y=price,x=carat, color= cut)) + geom_point(alpha=0.5) + xlab(“Diamond carat”) + ylab(“Price”) + ggtitle(“Relationship between Price and Carat”)&lt;/p&gt;

&lt;p&gt;PLOT 21 -  Giving label&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot 21 giving labe_.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
The chart looks more descriptive, but there is still a lot of scope for improvement. Let’s try to change the font of labels.&lt;/p&gt;

&lt;p&gt;PLOT 22 -  Adjusting Label&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y=price,x=carat, color= cut)) + geom_point(alpha=0.5) + xlab(“Diamond carat”) + ylab(“Price”) +&lt;/p&gt;

&lt;p&gt;ggtitle(“Relationship between Price and Carat”) +&lt;/p&gt;

&lt;p&gt;theme(axis.title.x = element_text(color = “Dark Blue”, size = 30),&lt;/p&gt;

&lt;p&gt;axis.title.y = element_text(color = “Dark Blue”, size = 30),&lt;/p&gt;

&lt;p&gt;axis.text.x = element_text(size=20), axis.text.y = element_text(size=20))&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot22 Adjust label.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
Finally, Lets adjust labels and get done with it.&lt;/p&gt;

&lt;p&gt;PLOT 23 -  Final Touch&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y=price,x=carat, color= cut)) + geom_point(alpha=0.5) + xlab(“Diamond carat”) + ylab(“Price”) +  ggtitle(“Relationship between Price and Carat”) +&lt;/p&gt;

&lt;p&gt;theme(axis.title.x = element_text(color = “Dark Blue”, size = 20),axis.title.y = element_text(color = “Dark Blue”, size = 20),axis.text.x = element_text(size=20),axis.text.y = element_text(size=20),legend.title = element_text(size = 15),legend.text  = element_text(size = 15),legend.position = c(1,1),legend.justification = c(1,1), plot.title = element_text(color=”Dark Blue”,size=20,family = “Courier”))&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R (Part 2)-20200902T032404Z-001/Data Viz in R (Part 2)/Plot23 final touch.png&quot; alt=&quot;Data Visualisation in R&quot; /&gt;
Finally, we’re done with everything. I would suggest you to try things on your own and experiment things in order to get a better view yourself.&lt;/p&gt;

&lt;p&gt;Before proceeding towards visualization, it is highly suggested to be aware about all the elements of your data set and what is to be achieved from it.&lt;/p&gt;

&lt;p&gt;GGPLOT2, in it has even more fascinating and useful tools. We suggest our readers to explore more of it and learn more about it.&lt;/p&gt;

&lt;p&gt;The use of visualization techniques differ according to the purpose it serves but the underlying idea essentially remains the same. We hope that the basic idea and concept of visualization is clear to all of you. So, play around with these visualizations and do write to us in case of any queries. All the best!!!&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">PLOT 11 - JITTER So, what’s a jitter? Jitter is a random value that is assigned to the variable so they don’t fall on top of each other.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SPURIOUS CORRELATION - A CORRELATION WITH NO RELATION</title><link href="http://localhost:4000/blog/SPURIOUS-CORRELATION/" rel="alternate" type="text/html" title="SPURIOUS CORRELATION - A CORRELATION WITH NO RELATION" /><published>2020-06-30T00:00:00+05:30</published><updated>2020-06-30T00:00:00+05:30</updated><id>http://localhost:4000/blog/SPURIOUS-CORRELATION</id><content type="html" xml:base="http://localhost:4000/blog/SPURIOUS-CORRELATION/">&lt;p&gt;&lt;img src=&quot;/blog/2020-06-30- Spurious Correlation_ A Correlation with no Relation/sp.jpg&quot; alt=&quot;Spurious Correlation&quot; /&gt;
Correlation is used to test the relationship between 2 or more variables. For instance, Weight and height are correlated. A taller person would tend to have more weight than a shorter person. However, this may not be always true. Hence, we use the Correlation coefficient, which measures as to how strong/ weak, the relation is between the variables. The range of correlation coefficient varies between (-1,1). The higher the correlation coefficient, the higher is the positive correlation between the variables and vice versa.&lt;/p&gt;

&lt;p&gt;ROLE OF CORRELATION IN BUSINESS LIFE&lt;/p&gt;

&lt;p&gt;Simply, the importance of correlation in business decision- making means better weighing of factors that have an effect on its performance.&lt;/p&gt;

&lt;p&gt;For instance, given are some common business correlations -&lt;/p&gt;

&lt;p&gt;Allocating more resources on Research &amp;amp; Development (R&amp;amp;D) correlates with more innovation.
The hiring of employees with certain personality characteristics correlates with higher productivity.
Correlation is greatly used in predicting the future of a business direction using Regression. If marketers identify a correlation between consumer behavior and events and a particular type of product or service, then they can take advantage of the relationship to boost business.&lt;/p&gt;

&lt;p&gt;If measures are taken to correlate unknown factors with business performance, then it may lead to less uncertainty in the Business since these unknown factors may have a huge impact but sometimes, these factors are volatile, complex, unknown; hence it is difficult to take them into consideration.&lt;/p&gt;

&lt;p&gt;Correlation is greatly used in various business industries, one such being Marketing Analytics. Managers lookout for the following correlations -&lt;/p&gt;

&lt;p&gt;Correlations between on-page keyword use and rankings.
Correlation with the type of top-level domain (.com, .org, etc.) and rankings in Google search.
Relation of posts with images getting more shares across social media.&lt;/p&gt;

&lt;p&gt;Above is a snapshot of an article in The Washington Post about the relationship between the gender of hurricanes’ names and the number of deaths the hurricane causes.&lt;/p&gt;

&lt;p&gt;The article’s title is “Female-named hurricanes kill more than male hurricanes because people don’t respect them, study finds.” The author concluded that the number of hurricane-related deaths is caused due to the gender of the hurricane’s name. Now, there may be (read ‘is’) relation between these two variables, however, there is no reason to believe that due to specific naming of the hurricane, it leads to more deaths, since ‘people take feminine hurricane names less seriously than their male counterparts’ and hence, do not prepare themselves as they should.&lt;/p&gt;

&lt;p&gt;The author realized the mistake and hence, the title was changed.&lt;/p&gt;

&lt;p&gt;Correlation Vs Causation&lt;/p&gt;

&lt;p&gt;A basic fact with respect to both these terms is that - “Correlation doesn’t imply Causation”. Let’s understand what this statement means;&lt;/p&gt;

&lt;p&gt;Causation, also referred to as ‘ Cause and effect’, is just an extension of Correlation, which says that a change in one variable will cause a change in the value of another value.&lt;/p&gt;

&lt;p&gt;Hence, correlation and causation must not be mixed up.&lt;/p&gt;

&lt;p&gt;Since now we understand the difference between Correlation and Causation, Let’s move to Spurious Correlation.&lt;/p&gt;

&lt;p&gt;SPURIOUS CORRELATION is a relationship wherein two events or variables are associated or correlated with each other, but not causally related i.e. they have no relation or meaning between them. A spurious correlation is usually caused due to coincidence or a third factor that may be ignored at the time of examination, usually known as the ‘lurking variable’ or ‘confounding factor’.&lt;/p&gt;

&lt;p&gt;Whenever two events are related in the same direction, we say that there exists a correlation between them but most of the events are spuriously related as we do not find any cause of this relation.&lt;/p&gt;

&lt;p&gt;Given is an interesting example of spurious correlation, you may notice that the graph is perfectly correlated but there is no reason or cause between this way- apart- from each other events&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2020-06-30- Spurious Correlation_ A Correlation with no Relation/http_www.tylervigen.com_spurious-correlations.jpg&quot; alt=&quot;Spurious Correlation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Photo credit - http -//www.tylervigen.com&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/2020-06-30- Spurious Correlation_ A Correlation with no Relation/image-18.png&quot; alt=&quot;Spurious Correlation&quot; /&gt;
In the above example, Ageing is a confounding variable. The apparent association between living in old age homes and having Alzheimer’s is confounded by age.&lt;/p&gt;

&lt;p&gt;If a researcher doesn’t take age into consideration, then he/ she may draw incorrect conclusions about the correlation between living in an old age home and Alzheimer’s.&lt;/p&gt;

&lt;p&gt;Misrepresentation of Data to show correlation -&lt;/p&gt;

&lt;p&gt;Skewed Scales Manipulating Ranges to Align Data&lt;/p&gt;

&lt;p&gt;Even when Y axes measure the same category, changing the scales can alter the lines to suggest a correlation. These Y axes for a company’s yearly revenue from a particular country, differ in range and proportional increase.
&lt;img src=&quot;/blog/2020-06-30- Spurious Correlation_ A Correlation with no Relation/graph 1.png&quot; alt=&quot;Spurious Correlation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Eliminating the second axis shows how skewed this chart is. Such problems can arise during the analysis due to the misrepresentation of the data.
&lt;img src=&quot;/blog/2020-06-30- Spurious Correlation_ A Correlation with no Relation/graph 2.png&quot; alt=&quot;Spurious Correlation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;How to Spot Spurious Correlations -&lt;/p&gt;

&lt;p&gt;It is very important to spot a Spurious correlation. Some of the methods that can be used are as follows -&lt;/p&gt;

&lt;p&gt;•Using a null hypothesis and checking for a high p-value.&lt;/p&gt;

&lt;p&gt;•By ensuring there is a proper representative sample.&lt;/p&gt;

&lt;p&gt;•By having an adequate sample size.&lt;/p&gt;

&lt;p&gt;Well, the truth is that Spurious Correlations are everywhere, you will find it in the news, websites, blog posts, etc. Many times, these correlations may even be used to share fake news.&lt;/p&gt;

&lt;p&gt;So, what are you waiting for start googling up and find correlation between events that may amuse you and us!&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Correlation is used to test the relationship between 2 or more variables. For instance, Weight and height are correlated. A taller person would tend to have more weight than a shorter person. However, this may not be always true. Hence, we use the Correlation coefficient, which measures as to how strong/ weak, the relation is between the variables. The range of correlation coefficient varies between (-1,1). The higher the correlation coefficient, the higher is the positive correlation between the variables and vice versa.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Data Visualization in R using ggplot2 (PART 1)</title><link href="http://localhost:4000/blog/Data-Visualization-in-R-Part1/" rel="alternate" type="text/html" title="Data Visualization in R using ggplot2 (PART 1)" /><published>2020-06-16T00:00:00+05:30</published><updated>2020-06-16T00:00:00+05:30</updated><id>http://localhost:4000/blog/Data-Visualization-in-R-Part1</id><content type="html" xml:base="http://localhost:4000/blog/Data-Visualization-in-R-Part1/">&lt;p&gt;&lt;img src=&quot;/blog/Data Visualization in R Part1.jpg&quot; alt=&quot;Data Visualization&quot; /&gt;
&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/1.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;s this image speaks for itself, we think this gives enough clarity about the elements, which are essential in making the plot. Before we proceed further, we’d like to tell that the first 3 layers are the essentials to making a plot.Otherwise, we’ll have nothing but an empty graph. Now, as the old saying goes “learn by doing”. We’ll jump on to GGPLOT, start from the basics and explore all layers.&lt;/p&gt;

&lt;p&gt;Let’s Begin&lt;/p&gt;

&lt;p&gt;As you know before starting anything we need to make sure we’ve installed the package and activated it in our library. Please note the code for installing and activating the package&lt;/p&gt;

&lt;p&gt;Instal.package(“ggplot2”)&lt;/p&gt;

&lt;p&gt;library(ggplot2)&lt;/p&gt;

&lt;p&gt;As soon as the package is installed and the data set is imported, we will start working on the visualization part.&lt;/p&gt;

&lt;p&gt;Please do follow the codes and practice them simultaneously on R for a thorough and complete understanding of these beautiful visualizations.&lt;/p&gt;

&lt;p&gt;Dataset Link- https-//vincentarelbundock.github.io/Rdatasets/datasets.html&lt;/p&gt;

&lt;p&gt;PLOT 1 - Blank Graph&lt;/p&gt;

&lt;p&gt;The data taken here is life expectancy over the years in India since 1960.&lt;/p&gt;

&lt;p&gt;Code - ggplot(data= life_expectancy, aes(x=expectancy, y = Years))
&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot 1-blank graph.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, the plot is empty; this is because the minimum 3 elements i.e data, aesthetics, and geometries are not supplied in the code. The plot does contain data and aesthetics but before knowing what type of geometry is to be made, how can anything move forward?&lt;/p&gt;

&lt;p&gt;PLOT 2  - Scatter Plot&lt;/p&gt;

&lt;p&gt;Let’s make a simple point chart!&lt;/p&gt;

&lt;p&gt;ggplot(data=x, aes(x=years, y=Expectancy) + geom_point
 Now that we’ve added geometry the plot is finally formed with a scatter plot.
&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot2 scatter plot.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;MOVING FORWARD!&lt;/p&gt;

&lt;p&gt;But why would anyone go through all the trouble of making a visualization as simple as this, which can even be made in Excel? So let us look at some things which cannot be done in Excel. The data set we have taken has 6 columns. It is a data set of different categories of diamond according to their price, cut, clarity and some other elements.&lt;/p&gt;

&lt;p&gt;PLOT 3 -  Scatter Plot&lt;/p&gt;

&lt;p&gt;Preview of the data can be taken by&lt;/p&gt;

&lt;p&gt;Summary(diamonds)&lt;/p&gt;

&lt;p&gt;ggplot(data = x, aes(x=carat,y=price)) + geom_point()
&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot 3.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The plot is again a simple scatter plot with the relationship between price and carat. It can roughly be concluded that there exists a positive relationship between price and carat i.e as the carat increases price also increase. But again, this can also be done in excel, then why should we prefer R over excel? Let’s try to build over this only-&lt;/p&gt;

&lt;p&gt;PLOT 4-  Adding Colors and Size&lt;/p&gt;

&lt;p&gt;Now what If you want to work with more than 2 variables? let’s say we want to see the price, carat as well as the cut in each carat category and its price&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y=price,x=carat, color= cut)) + geom_point()&lt;/p&gt;

&lt;p&gt;Now,we have Price of diamonds with each carat along with the cut, so the color signifies the quality of cut in each category and its price.So, now we have a strong reason for choosing R over excel when it comes to visualizing big data&lt;/p&gt;

&lt;p&gt;Even 3 variables is not enough and we want to classify the data  on the basis of the size too.&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y=price,x=carat, color= cut,size=depth)) + geom_point()&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot4.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see the size of the points is adjusted according to the depth. The description is given in the legend. However, the plot looks horrible and hardly any interpretation can be made from it. It is just to give you an idea of how elements can be used.&lt;/p&gt;

&lt;p&gt;SIMPLIFYING CODING&lt;/p&gt;

&lt;p&gt;PLOT 5 -  Scatter Plot&lt;/p&gt;

&lt;p&gt;It gets tedious to add again and again the primitive part of the code and then to add layers when it is nothing but the same code?&lt;/p&gt;

&lt;p&gt;So ,we will store the code in an object to save us from the labor work and make things easier.&lt;/p&gt;

&lt;p&gt;Code - q &amp;lt;- ggplot(data=diamond, aes(y=price,x=carat, color= cut))&lt;/p&gt;

&lt;p&gt;We have saved the code in ‘q’ we don’t need to do anything but build layers on top  of the basic structure&lt;/p&gt;

&lt;p&gt;q + geom_point()&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot5.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It gave you the same result as plot 5 just with a lot of ease. You can develop a basic code store it an object just like we did and then explore your data set with convenience&lt;/p&gt;

&lt;p&gt;OVERRIDING AESTHETICS&lt;/p&gt;

&lt;p&gt;Now that we’ve stored our basic plot details such as data and aesthetics, what if we change our mind and now we want different variables in the x and y-axis. Don’t you think going back and changing the plot again and storing it would be a lot of work? If yes, then you can try a thing called overriding aesthetics, all you need to do is re-enter the new aesthetics in your geometry, which in our case is geom_point. This will override the previously entered aesthetics in our basic object i.e ‘q’ and give us a greater flexibility and encourage to explore even more. Let’s try it out!&lt;/p&gt;

&lt;p&gt;Code - q + geom_point(aes(x= depth, y = price)) + xlab(“Depth”)&lt;/p&gt;

&lt;p&gt;PLOT 6&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot6.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The object q already had a command of depicting the relationship between price and carat but the command in geom_point overrode the command present in the basic plot. Hence, we will display the relationship between depth and price. Now you must be wondering why we had to put a label on the x-axis. This is because one of the limitations of overriding aesthetics is that the label doesn’t get changed. Hence, in this case if it weren’t for me changing the label. The label would’ve been Price and Carat giving out a false representation of the data. This is why you have to be extremely careful while overriding aesthetics.&lt;/p&gt;

&lt;p&gt;MAPPING VS  SETTING&lt;/p&gt;

&lt;p&gt;Mapping is what we’ve been doing till now i.e assigning colorsto a level detail. That means colour here is signifying a bifurcation based on different characteristics of the data set. Plot 4 is a perfect representation of mapping. But what if we don’t want to add color to a level of detail, we just want to change the color from black to Red. That’s where the setting comes in. Assigning one color to the whole data set is called setting. Let’s try it out.&lt;/p&gt;

&lt;p&gt;PLOT 7 - SETTING&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot7.png&quot; alt=&quot;Data Visualization&quot; /&gt;
Code - q + ggplot(aes(x= depth, y = price), color= “Red”) + xlab(“Depth”)&lt;/p&gt;

&lt;p&gt;This is what setting is, assigning a whole new color to the data set. Please note, while setting a color we don’t use a prefix of “aes”. Using aes will cause a misunderstanding and lead to an error. Hence, be cautious with that. We think we don’t need to present mapping as it has already been presented in the former part. We’ve explored the aesthetics enough, now let’s move to on exploring geometries.&lt;/p&gt;

&lt;p&gt;PLOT 8 - Line Chart&lt;/p&gt;

&lt;p&gt;How about we make a line plot instead of a scatter plot&lt;/p&gt;

&lt;p&gt;Code - ggplot(data=diamond, aes(y= price,x=carat)) +  geom_smooth(fill=NA)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot8.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This chart is simply depicting the relationship with carat. However, this chart in itself is not that informative, so lets also add some more geometries in it.&lt;/p&gt;

&lt;p&gt;PLOT 9 - Line Plot + Scatter plot&lt;/p&gt;

&lt;p&gt;Code  - ggplot(data=diamond, aes(y= price,x=carat)) + geom_point() +  geom_smooth(fill=NA)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot9.png&quot; alt=&quot;Data Visualization&quot; /&gt;
This looks better. But, still it can be misleading as carat is not only deciding factor of price and looking only on this relationship independently can be misleading&lt;/p&gt;

&lt;p&gt;PLOT 10 - Mapping Clarity&lt;/p&gt;

&lt;p&gt;To solve the problem let’s add variable  ‘clarity’ to our plot
Code - ggplot(data=diamond, aes(y= price,x=carat,color=cut)) + geom_point() + geom_smooth(fill=NA)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/Data Viz in R Part 1-20200902T032430Z-001/Data Viz in R Part 1/plot10.png&quot; alt=&quot;Data Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We believe this gives a better picture of the prices as the data depicts the prices according to carat as well as clarity
So, we hope that you must have understood the concept of visualization using ggplot2 in R. Worry not, because we will be posting the second part soon covering even more insightful and amazing visualization techniques under ggplot2. Hit the like button, if you liked our post; and subscribe our blog to be connected to us, as many more wonderful topics are underway-)&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Accounting Analytics</title><link href="http://localhost:4000/blog/Accounting-Analytics/" rel="alternate" type="text/html" title="Accounting Analytics" /><published>2020-06-03T00:00:00+05:30</published><updated>2020-06-03T00:00:00+05:30</updated><id>http://localhost:4000/blog/Accounting-Analytics</id><content type="html" xml:base="http://localhost:4000/blog/Accounting-Analytics/">&lt;p&gt;&lt;img src=&quot;/blog/Accounting Analytics.jpg&quot; alt=&quot;Accounting Analytics&quot; /&gt;
But what exactly is ‘accounting analytics’?&lt;/p&gt;

&lt;p&gt;So, in layman’s language, Accounting analytics is the examination of big data using data science or data analytics tools to help answer accounting-related questions. We live in a world which is full of huge piles of online data. The Accountants, now a days, are required to have the skill of analyzing this huge data in the form of financial statements of a company. That is why, it is very important to be familiar with the concept of accounting analytics.&lt;/p&gt;

&lt;p&gt;Here, we would learn about how data science is practically applied in the field of accounting analytics.There are various functions which the accountants are now a days using which originate from analytics. For example- budgeting, planning, data management, auditing; these are some fields where accountants are used to dealing with data. In fact, most professionals have already mastered two types of analytics-&lt;/p&gt;

&lt;p&gt;Descriptive Analytics- By summarizing and interpreting raw data, accountants find answers to what has happened. For example, we see that usually, analysts use sums, averages, and percent changes to calculate sales results, inventory stock, cost per customer, average dollars spent, year-over-year change in sales, etc.&lt;/p&gt;

&lt;p&gt;Diagnostic Analytics- Accountants often deploy data analytics and data mining to discover why something happened. They also create variance reports to show differences between budgeted amounts and actual income or expenses, employ tools and software to look for patterns and problems in large data sets.&lt;/p&gt;

&lt;p&gt;These skills are great, but they’re not enough. As the automation takes over day-to-day tasks, accountants are increasingly being asked to act as data scientists. Accountants can use various tools of analytics such as- Excel, Tableau, Python, SQL, etc&lt;/p&gt;

&lt;p&gt;Let us now examine how 3 different types of accounting studies which may use analytics to modify their current work through examples.&lt;/p&gt;

&lt;p&gt;Managerial Accounting
Financial Accounting
Auditing
In this blog, we’re going to talk about three major aspects of accounting Analytics with respect to each type of accounting study mentioned above-&lt;/p&gt;

&lt;p&gt;1) Variance Analysis in Management Accounting&lt;/p&gt;

&lt;p&gt;2) The concept of SVA in Financial Accounting&lt;/p&gt;

&lt;p&gt;3) Trend Analysis in Auditing&lt;/p&gt;

&lt;p&gt;1) Variance Analysis in Managerial Accounting
&lt;img src=&quot;/blog/01.png&quot; alt=&quot;Accounting Analytics&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Managerial Accounting is a domain in which accountants prepare and use data for internal decision making process. We’d like to introduce you to a couple of ways in which advanced data analytics skills will help you make better internal decisions.&lt;/p&gt;

&lt;p&gt;Case study of the usage of analytics in Dominos&lt;/p&gt;

&lt;p&gt;Dominos India is a popular pizza restaurant chain in India. So, Dominos India implements standards to keep the costs of cheese in control with respect to every normal pizza size. They also measure how much cheese they start a day with, how much is prepared, and how much they end with so that they can see how much of it was used.&lt;/p&gt;

&lt;p&gt;When a customer buys a pizza, say a medium size, the employees at Dominos are supposed to use a standard amount of cheese.&lt;/p&gt;

&lt;p&gt;•If the employee doesn’t put enough cheese in the pizza, then customers may be dissatisfied.&lt;/p&gt;

&lt;p&gt;•On the other hand, if employees put too much cheese on the pizza, then the profit margin decreases and Dominos’ shareholders are dissatisfied.&lt;/p&gt;

&lt;p&gt;The company has to minimize the un-evenness.&lt;/p&gt;

&lt;p&gt;Solution&lt;/p&gt;

&lt;p&gt;Combining the point of sale data with inventory data and data about standard quantities of cheese can calculate variances to help identify if the employees are using the right amount of cheese.&lt;/p&gt;

&lt;p&gt;Variance information can be criticized for being too old to make a difference. However, if one knows how to systematically fetch data at regular intervals and combine data, perhaps from, say, a point of sale system, spreadsheets, and QuickBooks, then one can help set up a process to calculate and communicate variances on a daily basis.&lt;/p&gt;

&lt;p&gt;This is just the start. If you can gather the names of the employees who are responsible for serving the pizza, then you can also quantify the extent to which each employee is serving the right amount of cheese, helping managers to identify employees who may need additional training. All of this can be done with the help of Excel.&lt;/p&gt;

&lt;p&gt;2) The Concept of SVA in Financial Accounting
&lt;img src=&quot;/blog/02.png&quot; alt=&quot;Accounting Analytics&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Financial accounting is a domain of accounting that focuses on summarizing accounting transactions during a period of time for users external to an organization like investors, creditors, and regulators.&lt;/p&gt;

&lt;p&gt;Shareholder Value Analytics&lt;/p&gt;

&lt;p&gt;Shareholder value analysis (SVA) is one of several non-traditional metrics being used in business today. SVA determines the financial value of a company by looking at the returns it gives its stockholders and is based on the view that the objective of company directors is to maximize the wealth of company stockholders.&lt;/p&gt;

&lt;p&gt;How is shareholder value calculated?&lt;/p&gt;

&lt;p&gt;Shareholder value is calculated by dividing the estimated total net value of a company based on its present and future cash flows by the value of its shares of stock. The resulting figure indicates the company’s value to stockholders.&lt;/p&gt;

&lt;p&gt;The Formula for Shareholder Value Added Is-&lt;/p&gt;

&lt;p&gt;SVA=NOPAT−CC&lt;/p&gt;

&lt;p&gt;Where-&lt;/p&gt;

&lt;p&gt;NOPAT=Net operating profit after tax&lt;/p&gt;

&lt;p&gt;CC=Cost of capital​&lt;/p&gt;

&lt;p&gt;Why do the companies adopt SVA?&lt;/p&gt;

&lt;p&gt;The principle of shareholder value is that a company adds value for its stockholders only when equity returns exceed equity costs. Once the amount of value is calculated, targets for improvement can be set and shareholder value can be used as a measure for managing performance.&lt;/p&gt;

&lt;p&gt;Some of the value investors use SVA as a tool to judge the corporation’s profitability and management efficacy. This line of thinking often runs congruent with value-based management, which assumes that the foremost consideration of a corporation should be to maximize economic value for its shareholders.&lt;/p&gt;

&lt;p&gt;Shareholder value is created when a company’s profits exceed its costs. But there is more than one way to calculate this.&lt;/p&gt;

&lt;p&gt;3) Trend Analysis in Auditing
&lt;img src=&quot;/blog/03.png&quot; alt=&quot;Accounting Analytics&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Auditing is the domain within accounting which focuses on whether the control procedures are being followed and whether the reported financial statements are accurate.&lt;/p&gt;

&lt;p&gt;Case Study for the Application of Trend Analysis&lt;/p&gt;

&lt;p&gt;Comfort Ltd. is a major player in the shoe industry in India and worldwide. We are here to have an audit examining how the number of orders shipped impacts its sales.&lt;/p&gt;

&lt;p&gt;Let’s take a look at the following table showing the sales and number of orders shipped over five periods-&lt;/p&gt;

 	Period 1	Period 2	Period 3	Period 4	Period 5
&lt;p&gt;Sales	750,000	850,000	900,000	1,100,000	1,150,000
Change ($)	 	100,000	50,000	200,000	50,000
Change (%)	 	13%	6%	22%	5%&lt;/p&gt;

&lt;p&gt;Orders Shipped	700	1,200	1,600	2,300	3,000
Change (Count)	 	500	400	700	700
Change (%)	 	71%	33%	44%	30%
We can see that the information in the table shows solid performance over the five periods.  Sales and orders shipped increase during each period.&lt;/p&gt;

&lt;p&gt;So, can the auditors come to the conclusion that the company has faired well in the 5 periods?&lt;/p&gt;

&lt;p&gt;It’s hard to answer without digging a little deeper.  So, let’s take a look at the graph below which shows the sales and orders shipped information-&lt;/p&gt;

&lt;p&gt;The graph clearly shows the same increasing sales and orders shipped. However, upon a closer examination, we see that the sales do not increase at the same rate as orders shipped.  This shows us that the sales per order shipped are deteriorating.&lt;/p&gt;

&lt;p&gt;Does this decrease in sales per order shipped represent an alarming development over the five periods?  Not necessarily.  It can be the company’s intent to increase the orders shipped over time.  However, the company may want to investigate and take measures if the decreasing sales per order shipped numbers are not what the company wants or expects.&lt;/p&gt;

&lt;p&gt;This is just an example of how analytics can be used in auditing. There are many more applications being used in the corporate world.&lt;/p&gt;

&lt;p&gt;So, we hope that you must have understood the concept of accounting analytics and how analytics is widely used in the field of accounting. Hit the like button, if you liked our post; and subscribe our blog to be connected to us, as many more wonderful topics are underway-)&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">But what exactly is ‘accounting analytics’?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Modelling the 4P Mix</title><link href="http://localhost:4000/blog/Modelling-the-4P-Mix/" rel="alternate" type="text/html" title="Modelling the 4P Mix" /><published>2020-05-30T00:00:00+05:30</published><updated>2020-05-30T00:00:00+05:30</updated><id>http://localhost:4000/blog/Modelling-the-4P-Mix</id><content type="html" xml:base="http://localhost:4000/blog/Modelling-the-4P-Mix/">&lt;p&gt;&lt;img src=&quot;/blog/Modelling-the-4P-Mix/Modelling the 4P Mix.png&quot; alt=&quot;Modelling the 4P mix&quot; /&gt;
Marketing Mix Modelling is one of the best methods available to value and analyze various marketing tactics on sales and/or market share, and then forecast the future impact of the similar activities. This basically means quantifying the information available with the firm to analyze their impact on sales and market share.&lt;/p&gt;

&lt;p&gt;It uses both the internal information (Pricing, Quality Metrics, Inventory Levels, Spending on Print Media) as well as the external information (Macroeconomic Situation, Seasonality and Competitors). The key objective is to maximize the return on investment while minimizing the budget required for the same.&lt;/p&gt;

&lt;p&gt;Digital Transformation of various business processes have helped the collection of better data for modelling purposes. This is because in order to create an effective model, dynamic metrics, such as social media and customer engagement metrics, help. GRP (gross rating point) in TVs and Click-through rates in Online Advertising are some of the popular metrics often used to understand the impact of different promotion methods.&lt;/p&gt;

&lt;p&gt;We will be illustrating two important concepts of Marketing Mix Modelling- Linear Regression and Incremental Drivers.&lt;/p&gt;

&lt;p&gt;Using Linear Regression for Marketing Mix Modelling&lt;/p&gt;

&lt;p&gt;Marketing Mix Modelling uses the concept of Multiple Regression. The dependent variable can be Sales while the independent variables commonly used are Price, Advertising spends, Location, product discounts, and so on.&lt;/p&gt;

&lt;p&gt;Before diving into the exacts of Linear Regression, let’s understand an important component of Marketing Mix Modelling- Adstock. Adstock refers to the prolonged effect of advertising on consumers. This tool can help us quantify the impact of advertising.&lt;/p&gt;

&lt;p&gt;The basic Adstock model is as follows&lt;/p&gt;

&lt;p&gt;At = Tt + λAt-1&lt;/p&gt;

&lt;p&gt;Where, At is Adstock at time t, Tt is value of advertising variable at time t, and λ refers to the decay parameter. Thus, when t approaches infinity, the Adstock value approaches 0. We will use Adstock as a component of the Linear Regression equation that we will create.&lt;/p&gt;

&lt;p&gt;We will also include one component for seasonal trend in our equation, as well as one for the number of distribution points. To illustrate, imagine that there is a Clothing store which wants to know the relative importance of parts of its Marketing Mix. The store usually experiences higher sales in Summers than at other times of the year (Seasonal Trend). It has around 100 outlets across India (Number of Distribution Points) and advertises only through Online Media (for simplicity purposes).&lt;/p&gt;

&lt;p&gt;Here is an example of how the regression equation would look like-&lt;/p&gt;

&lt;p&gt;Sales = β0 + β1 (Seasonal Trend) + β2(Number of Distribution Points) + β3(Adstock of at time t)&lt;/p&gt;

&lt;p&gt;The betas generated from Regression analysis will help in quantifying the impact of each of the inputs. Basically, the beta depicts that one unit increase in the input value would increase the sales by Beta times those units, keeping the other inputs constant.&lt;/p&gt;

&lt;p&gt;Many other attributes can be added to this equation; even polynomial features can be used in order to show non-linear changes. Actual models are much more complicated than the one shown above.&lt;/p&gt;

&lt;p&gt;Distinguishing Base Sales from Incremental Sales&lt;/p&gt;

&lt;p&gt;Marketing Mix Modelling breaks down the Business metrics into two major contributors- Base Drivers and Incremental Drivers.&lt;/p&gt;

&lt;p&gt;Incremental Drivers refer to the business results generated by using marketing tactics different from those used in the normal course of business, while Base Drivers refer to the business results generated in the normal course of business, mainly due to the goodwill and brand equity developed over the years.&lt;/p&gt;

&lt;p&gt;Using the same example as before of a clothing store, the base drivers would be that of the number of outlets, its advertisement stock, and the average price level. A temporary reduction in the price would be an example of an incremental driver. The effect of this incremental driver can be separated and analysed to measure its impact.&lt;/p&gt;

&lt;p&gt;There are innumerable other concepts that can be applied to model the 4 Ps of Marketing than Linear Regression and Incremental Drivers, such as Budget Optimization or Deep-dive analysis. Usually these concepts are not used in isolation, but rather a collection of all these concepts is used. Marketing Mix Modelling has a much wider scope and shows us how Analytics can be useful for business.&lt;/p&gt;

&lt;p&gt;A customer’s journey of ‘from thinking to buying’ takes him/her through multiple touch points before deciding the final product to buy. As marketing is becoming more customer centric, the need of identification of right channels to target potential customers has become critical for companies. This helps companies to utilise their marketing funds in a better way and target the right customers in right places. So, we hope that you must have got the gist of the concept. Hit the like button, if you liked our post; and subscribe our blog to be connected to us, as many more wonderful topics are underway-)&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">Marketing Mix Modelling is one of the best methods available to value and analyze various marketing tactics on sales and/or market share, and then forecast the future impact of the similar activities. This basically means quantifying the information available with the firm to analyze their impact on sales and market share.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">K-Means Clustering</title><link href="http://localhost:4000/blog/K-Means-Clustering/" rel="alternate" type="text/html" title="K-Means Clustering" /><published>2020-05-20T00:00:00+05:30</published><updated>2020-05-20T00:00:00+05:30</updated><id>http://localhost:4000/blog/K-Means-Clustering</id><content type="html" xml:base="http://localhost:4000/blog/K-Means-Clustering/">&lt;p&gt;&lt;img src=&quot;/blog/k mean clustering/K Means Clustering.png&quot; alt=&quot;K-Means Clustering&quot; /&gt;
What is clustering?&lt;/p&gt;

&lt;p&gt;Clustering refers to grouping of data under different groups based on their properties, nature, attributes and relationship. One cluster contains all the objects of similar nature that are atleast in some way different from other clusters. Clustering analysis is an essential part of both supervised and unsupervised learning. Whether exclusive (an item belongs exclusively to one cluster) or overlapping (item belongs to a number of clusters), it is of wide importance to all Data Scientists.&lt;/p&gt;

&lt;p&gt;Categorizing news and articles into different groups like entertainment, sports, educational, political etc, spam filter (that automatically marks some mails as spam), classifying fake news, segmenting markets,  document analysis are some of the common real world applications of clustering algorithms.&lt;/p&gt;

&lt;p&gt;K means clustering&lt;/p&gt;

&lt;p&gt;K means clustering is one of the most popular clustering methods for unsupervised learning with wide application. It iteratively calculates the deviation from a point called centroid. Initially K number of so called centroids are chosen. A centroid is a data point (imaginary or real) at the center of a cluster. The initial centroids are randomly selected and then each data point is mapped to closest centroid. The second centroid is moved to mean of each cluster it defines. This is iteratively done till convergence or till the clusters become constant and centroid and composition of cluster no longer changes.
&lt;img src=&quot;/blog/k mean clustering/1_IXGsBrC9FnSHGJVw9lDhQA.png&quot; alt=&quot;K-Means Clustering&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let us understand the concept with a simple inbuilt dataset in R. Suppose, we have a list of Sepal length and Sepal width of various species of flowers and for their nomenclature it is required to put them into appropriate clusters. These species are unknown to us and we can’t simply put them anywhere. That is when clustering comes into the picture. To be more precise this is when K-Means Clustering comes in.&lt;/p&gt;

&lt;p&gt;Let us first start with loading the dataset. To avoid the trouble of downloading a dataset from the internet we have used a popular dataset inbuilt in R- IRIS. Let us look at it first. A brief overview of the data and detection of NAs will clean it for us to start.&lt;/p&gt;

&lt;p&gt;install.packages(“datasets”)&lt;/p&gt;

&lt;p&gt;library(datasets)&lt;/p&gt;

&lt;p&gt;str(iris)&lt;/p&gt;

&lt;p&gt;table(is.na(iris))&lt;/p&gt;

&lt;p&gt;This summarizes the dataset for us with 5 variables and 150 observations and to our utter delight no NAs. The dataset is now all set to work upon.&lt;/p&gt;

&lt;p&gt;We now divide our datasets into two parts– try and test. Although not essential but this little step helps us to appreciate the reliability of our models. We run it on train dataset and then introduce the test dataset to our model to check how well it works in unknown variables.&lt;/p&gt;

&lt;p&gt;parting&amp;lt;-sample(2, nrow(iris),replace=TRUE, prob=c(0.5,0.5))&lt;/p&gt;

&lt;p&gt;try&amp;lt;-iris[parting==1,]&lt;/p&gt;

&lt;p&gt;test&amp;lt;-iris[parting==2,]&lt;/p&gt;

&lt;p&gt;For K means this has to be then converted into a matrix. We select the columns which will form the basis for clustering. Though, we have used sepal length and sepal width you are free to try all possible combinations and write to us if you explore something more insightful.&lt;/p&gt;

&lt;p&gt;try.short&amp;lt;-try[,c(1,2)]&lt;/p&gt;

&lt;p&gt;try.matrix&amp;lt;-data.matrix(try.short)&lt;/p&gt;

&lt;p&gt;Now, since we are all equipped; let’s use K means.&lt;/p&gt;

&lt;p&gt;Syntax of K-means function in R…&lt;/p&gt;

&lt;p&gt;Object=kmeans(x, centers, iter.max=, nstart=)&lt;/p&gt;

&lt;p&gt;x =&amp;gt; Data matrix of data to be clustered&lt;br /&gt;
centers =&amp;gt; Number of clusters
iter.max =&amp;gt; Maximum iterations
nstart =&amp;gt; Number of centers to start with
A two line code will print a half page result. Big but significant.&lt;/p&gt;

&lt;p&gt;traincluster&amp;lt;-kmeans(try.matrix,3)&lt;/p&gt;

&lt;p&gt;traincluster&lt;/p&gt;

&lt;p&gt;traincluster$size&lt;/p&gt;

&lt;p&gt;traincluster$centers&lt;/p&gt;

&lt;p&gt;This way the additional arguments can be used for specific and detailed results.&lt;/p&gt;

&lt;p&gt;K means clustering have classified the data into three categories each possibly belonging to different species on the basis of their distance from mean sepal length and sepal width.&lt;/p&gt;

&lt;p&gt;Visualizing the clusters make the results more attractive and comprehensible.&lt;/p&gt;

&lt;p&gt;Install.packages(“ggplot2”)&lt;/p&gt;

&lt;p&gt;Library(ggplot2)&lt;/p&gt;

&lt;p&gt;traincluster$cluster&amp;lt;-as.character(traincluster$cluster)&lt;/p&gt;

&lt;p&gt;ggplot()+geom_point(data=try, mapping=aes(x=Sepal.Length,y=Sepal.Width, color=traincluster$cluster)) +geom_point(mapping=aes_string(x= traincluster$centers[,”Sepal.Length”], y=traincluster$centers[,”Sepal.Width”]), size=4,color=”red”)&lt;/p&gt;

&lt;p&gt;Within Cluster Sum of Squares&lt;/p&gt;

&lt;p&gt;The sum of the squared deviations from each observation and the cluster centroid is called as within- cluster sum of square. The within-cluster sum of squares is a measure of the variability of the observations within each cluster. In general, a cluster that has a small sum of squares is more compact than a cluster that has a large sum of squares. Clusters that have higher values exhibit greater variability of the observations within the cluster. As the number of observations increases, the sum of squares becomes larger. Therefore, the within-cluster sum of squares is often not directly comparable across clusters with different numbers of observations. To compare the within-cluster variability of different clusters, use the average distance from centroid instead.&lt;/p&gt;

&lt;p&gt;Determining the optimum number of clusters (Elbow method)&lt;/p&gt;

&lt;p&gt;wss&amp;lt;-(nrow(try.matrix)-1)*sum(apply(try.matrix,2,var))&lt;/p&gt;

&lt;p&gt;for(i in 2-15)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wss[i]&amp;lt;-sum(kmeans(try.matrix,centers=i)$withinss)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;a&amp;lt;-plot(1-15,wss,type=”b”,xlab=”Number of Clusters”,ylab=”Within Cluster Sum of Square”)&lt;/p&gt;

&lt;p&gt;Elbow method brings out the graphical representation of various combinations of within cluster sum of squares and number of clusters. So, usually a elbow like structure is formed. Considering that we require lower within cluster sum of squares, we use that number of cluster where bent of elbow is achieved. This shows a sharp decrease in within cluster sum of square with increasing a cluster and after this within cluster sum of squares tends to be constant. And this nearly constant within cluster sum of squares is often not useful because it arbitrarily increases the number of cluster and makes them all more homogeneous.&lt;/p&gt;

&lt;p&gt;So, in the above graph we see that WSS significantly reduces from  1st to 3rd cluster, so here optimum clusters are 3.&lt;/p&gt;

&lt;p&gt;Prediction and clustering&lt;/p&gt;

&lt;p&gt;If we are using clustering to predict about the some categorical variable with same level of factors as we have the clusters, we can check the accuracy using simple table or even the confusion matrix&lt;/p&gt;

&lt;p&gt;table(traincluster$cluster,try$species)&lt;/p&gt;

&lt;p&gt;This would give us the number of values that are correctly predicted and number of that are incorrect. Since, clustering is part of unsupervised learning , we do not test the model accuracy. But for the sake of developing a understanding and reliability of the model this hack could be used. This alongwith some other tools like confusion matrix is used while we use clustering as part of supervised learning environment.&lt;/p&gt;

&lt;p&gt;So, we hope that you have understood the concept through our blog. Do write your precious feedback and feel free to ask any related doubt.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">What is clustering?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Python Basics- Get them Intact</title><link href="http://localhost:4000/blog/Python-Basics/" rel="alternate" type="text/html" title="Python Basics- Get them Intact" /><published>2020-05-13T00:00:00+05:30</published><updated>2020-05-13T00:00:00+05:30</updated><id>http://localhost:4000/blog/Python-Basics</id><content type="html" xml:base="http://localhost:4000/blog/Python-Basics/">&lt;p&gt;We taught you how to set up the required python environment in the previous blog, if you haven’t read that, do give it a shot, else, you’ll have tough time setting it up on your own. We highly encourage you to not just read this blog but actually practice whatever we teach you by doing a hands on.&lt;/p&gt;

&lt;p&gt;Let’s get our basics done. This part might feel a little boring but trust me this is very important, in fact it is a mandate, can’t skip.&lt;/p&gt;

&lt;p&gt;Syntax Note- Python is a case sensitive language. Python takes indentation very seriously, indentation here represents a block of code. You’ll get loads of errors if you ignore the indentation.&lt;/p&gt;

&lt;p&gt;Import the libraries
The two libraries that we’ll be using would be numpy and pandas, how to use import them?&lt;/p&gt;

&lt;p&gt;Import them!&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Variable
You would need to store data, loads of it, quite frequently. You’ll be using variables for that.&lt;/p&gt;

&lt;p&gt;Assigning Value to a variable
Assign by hard coding
a=10
flag=true&lt;/p&gt;

&lt;p&gt;Input Data
How do we input data from the user? Use this syntax
input(“Enter data- “)&lt;/p&gt;

&lt;p&gt;Conditions
You’ll encounter multiple instances throughout your journey where you’ll have to make some decisions, decisions that are based on some conditions. Well, we can do this with python as well.&lt;/p&gt;

&lt;p&gt;Syntax-
if condition
Condition 1’s result
elif
Condition 2’s result
else
Default result&lt;/p&gt;

&lt;p&gt;Mangoes = 15
Bananas = 20
if Bananas &amp;gt; Mangoes-
print(“There are more Bananas than Mangoes“)
elif a == b-
print(“The no. of bananas and mangoes is equal“)
else -
print(“There are more Mangoes than Bananas“)&lt;/p&gt;

&lt;p&gt;Arrays
Heard of lists, call them arrays from now. An Array is basically a list of values referenced under single name.&lt;/p&gt;

&lt;p&gt;Syntax for assigning value- arrayName = [value1, value2, value3, …. , valuen]
So instead of using
firstPlayer = Player1
secondPlayer = Player2
thirdPlayer = Player3
Use
player = [Player1, Player2, Player3]&lt;/p&gt;

&lt;p&gt;Know length of an array, you can do this by typing len(arrayName)
So, len(player) will give 3 as output&lt;/p&gt;

&lt;p&gt;Loops
You would encounter situations wherein you’ll have to print lot of repetitive stuff like a count from 1 to 100 or print something hundred times. We use loops for that. You can learn more about and types of loops from the official python documentation.&lt;/p&gt;

&lt;p&gt;The below code prints counting from 0 to 99 in 4 lines of code.
i = 0
while i&amp;lt;100 -
print(i)
i += 1&lt;/p&gt;

&lt;p&gt;What if you had a list say, myList = [“First”, “Second”, “Third”] and you wanted to print all the elements of the list? Loops make this task easier. We use a for loop to demonstrate how this works&lt;/p&gt;

&lt;p&gt;for var in myList-
print(var)&lt;/p&gt;

&lt;p&gt;Output-
First
Second
Third&lt;/p&gt;

&lt;p&gt;What if you wanted to print something through a loop but add some conditions at the same time?&lt;/p&gt;

&lt;p&gt;myList = [“First”, “Second”, “Third”]
for var in myList-
if var!=”Second”-
print(var)&lt;/p&gt;

&lt;p&gt;Output-
First
Third&lt;/p&gt;

&lt;p&gt;You can learn more properties/functions of arrays, loops, conditions at https-//wiki.python.org/moin/BeginnersGuide/NonProgrammers, after all, we at Analytics Bay want to make you self-sufficient to survive in this world of Analytics.&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">We taught you how to set up the required python environment in the previous blog, if you haven’t read that, do give it a shot, else, you’ll have tough time setting it up on your own. We highly encourage you to not just read this blog but actually practice whatever we teach you by doing a hands on.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">MONTE CARLO SIMULATION</title><link href="http://localhost:4000/blog/MONTE-CARLO-SIMULATION/" rel="alternate" type="text/html" title="MONTE CARLO SIMULATION" /><published>2020-05-06T00:00:00+05:30</published><updated>2020-05-06T00:00:00+05:30</updated><id>http://localhost:4000/blog/MONTE-CARLO-SIMULATION</id><content type="html" xml:base="http://localhost:4000/blog/MONTE-CARLO-SIMULATION/">&lt;p&gt;&lt;img src=&quot;/blog/2020-05-06-MONTE-CARLO-SIMULATION/MONTE CARLO SIMULATION.jpg&quot; alt=&quot;K-Means Clustering&quot; /&gt;
It is pertinent to note that Monte Carlo Simulation provides a probabilistic estimate of the uncertainty in a model. It is never deterministic. However, given the uncertainty or risk ingrained in a system, it is a useful tool for approximation of realty.&lt;/p&gt;

&lt;p&gt;HISTORY
Monte Carlo simulation has been named after the city in Monaco ( which is famous for its casino) where games of chance (e.g., roulette) involve repetitive events with known probabilities. Mathematician Stanislaw Ulami developed the technique while working on nuclear weapon projects during the World war II era.&lt;/p&gt;

&lt;p&gt;APPLICATION
The method is used extensively in a wide variety of fields such as physical science, computational biology, statistics, artificial intelligence, and quantitative finance.&lt;/p&gt;

&lt;p&gt;It can be used in virtually any field such as- Statistics, Artificial Intelligence, finance, engineering, physical sciences, law etc.&lt;/p&gt;

&lt;p&gt;For example, it can be used in the word of business and finance. The MCM is used to price financial instruments and also plays a critical role in risk analysis. It also used to solve complex business problems relating to consultancy and forecasting future trends of business performance&lt;/p&gt;

&lt;p&gt;The above picture is a basic example of MC Simulation. It shows the net return of an index after 5 years given the mean and standard deviation.
We have prepared an MC Simulation, you can check it out-
https-//bit.ly/MCSimul&lt;/p&gt;

&lt;p&gt;EXAMPLE
Let’s consider calculating the probability of a particular sum (7) on the throw of 2unbiased dice. So, there are total 36 combinations of dice rolls-&lt;/p&gt;

&lt;p&gt;There are 6different ways that the dice cab sum to 7. Hence, the probability of rolling 7 = 0.167.&lt;/p&gt;

&lt;p&gt;Instead, we can throw the dice a hundred times and record how many times each outcome occurs. If the dice totals 7, 18 times (out of 100 rolls), we would conclude that the probability of getting the sum 7 is approximately 0.18 (18%).Better than rolling the dice a hundred times, we can easily use a computer to simulate rolling the dice n times. The output of 10,000 realizations-&lt;/p&gt;

&lt;p&gt;Two Dice
HOW ACCURATE ARE THE RESULTS
The accuracy of a Monte Carlo simulation is a function of the number of realizations. That is, the confidence bounds on the results can be readily computed based on the number of realizations. The example below shows the 5% and 95% confidence bounds on the value for each outcome (i.e., there is a 90% chance the the true value lies between the bounds)-&lt;/p&gt;</content><author><name>Shivansh Mehendiratta</name><email>connect@theanalyticsbay.com</email></author><summary type="html">It is pertinent to note that Monte Carlo Simulation provides a probabilistic estimate of the uncertainty in a model. It is never deterministic. However, given the uncertainty or risk ingrained in a system, it is a useful tool for approximation of realty.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bg.jpeg" /><media:content medium="image" url="http://localhost:4000/bg.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>